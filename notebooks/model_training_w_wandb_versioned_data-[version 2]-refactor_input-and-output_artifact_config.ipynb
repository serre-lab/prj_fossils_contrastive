{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd275f95",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Training with WandB Versioned Data `[version 2]`\n",
    "\n",
    "`model_training_w_wandb_versioned_data-[version 2]-refactor_input-and-output_artifact_config.ipynb`\n",
    "\n",
    "Author: Jacob A Rose  \n",
    "Created on: Wednesday May 12th, 2021\n",
    "\n",
    "-----\n",
    "Based on [this](https://colab.research.google.com/drive/1PRnwxttjPst6OmGiw9LDoVYRDApbcIjE) notebook (or see the original [report](https://wandb.ai/stacey/mendeleev/reports/DSViz-for-Image-Classification--VmlldzozNjE3NjA)), in which the [iNaturalist 2017](https://github.com/visipedia/inat_comp/tree/master/2017) dataset is re-sampled to several different versions, each with different #s of samples per class.\n",
    "* These are then used to train and validate a keras model, then log many image predictions to W&B so that the versioned input images are directly associated with each new log. \n",
    "* This allows a flexible and scalable opportunity for researchers and even untrained members of the public to easily interact with the data and interrogate model decisions.\n",
    "\n",
    "* Users can query subsets of the prediction results using basic logical queries w/ an effective autocomplete providing real-time suggestions for valid queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7253315c",
   "metadata": {},
   "source": [
    "----------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e652a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a1a7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f587dc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "import wandb\n",
    "from lightning_hydra_classifiers.train_basic import run_full_tuned_experiment\n",
    "from hydra.experimental import initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "from rich import print as pp\n",
    "os.environ['WANDB_CACHE_DIR'] = \"/media/data/jacob/wandb_cache\"\n",
    "os.makedirs(\"/media/data/jacob/wandb_cache\", exist_ok=True)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd371eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bdbb3c4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'trainer'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pytorch_lightning.Trainer'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'gpus'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'min_epochs'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'max_epochs'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'weights_summary'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'top'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'progress_bar_refresh_rate'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'profiler'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'simple'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'log_every_n_steps'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'terminate_on_nan'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'fast_dev_run'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'limit_train_batches'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'limit_val_batches'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'lightning_hydra_classifiers.models.resnet.ResNet'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'basename'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'resnet50'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'resnet50_512'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'optimizer'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'torch.optim.Adam'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Adam'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'lr'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.001</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'weight_decay'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'num_classes'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'input_shape'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'lr'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.001</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'head'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'avg_pool'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'flatten'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'fc'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'unfreeze'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'layer4'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'root_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/models/resnet50_512'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_path'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/models/resnet50_512/best_model.pkl'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'datamodule'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'contrastive_learning.data.pytorch.extant.ExtantLightningDataModule'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'basename'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Extant'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Extant_family_10_512'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'val_split'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'num_classes'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'image_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'channels'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'class_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'family'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'normalize'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'seed'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">98568764</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'predict_on_split'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'val'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'debug'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'num_workers'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'root_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'dataset_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/datasets/Extant_family_10_512'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'callbacks'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_checkpoint'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pytorch_lightning.callbacks.ModelCheckpoint'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'monitor'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'val_loss'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'save_top_k'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'save_last'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'mode'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'min'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'verbose'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'dirpath'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/models/resnet50_512/checkpoints/'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'filename'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'{epoch:02d}'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'early_stopping'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pytorch_lightning.callbacks.early_stopping.EarlyStopping'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'code_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'monitor'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'val/loss'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'patience'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'verbose'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'mode'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'min'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'log_per_class_metrics_to_wandb'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'lightning_hydra_classifiers.callbacks.wandb_callbacks.LogPerClassMetricsToWandb'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'module_data_monitor'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pl_bolts.callbacks.ModuleDataMonitor'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'logger'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'wandb'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pytorch_lightning.loggers.wandb.WandbLogger'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'entity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'jrose'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'image_classification'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'job_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'train'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'group'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stage_0'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'csv'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pytorch_lightning.loggers.csv_logs.CSVLogger'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'save_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/models/resnet50_512/results/logs'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'csv/'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'wandb'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'0'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'init'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'entity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'jrose'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'image_classification_train'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'job_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'batch_size_tune_init'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'group'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'tune'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'run_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'tags'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Extant_family_10_512'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'batch_size_tune'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'init'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'config'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'stage'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'batch_size_tune'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'stage_idx'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'0'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'results_file'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stage_idx_0/hparam_batch_size.yaml'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'results_path'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/models/resnet50_512/result</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">s/stage_idx_0/hparam_batch_size.yaml'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'auto_scale_batch_size'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'power'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'image_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>\n",
       "                <span style=\"font-weight: bold\">}</span>\n",
       "            <span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'init'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'entity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'jrose'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'image_classification_train'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'job_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'lr_tune_init'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'group'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'tune'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'run_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'tags'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Extant_family_10_512'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'resnet50_512'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'lr_tune'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'init'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'config'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'stage'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'lr_tune'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'stage_idx'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'results_file'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stage_idx_1/hparam_lr.yaml'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'results_path'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/models/resnet50_512/results/stage_idx_1/hparam_lr.yaml'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'tuner_kwargs'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'min_lr'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-08</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'max_lr'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'num_training'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'mode'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'exponential'</span>\n",
       "                    <span style=\"font-weight: bold\">}</span>\n",
       "                <span style=\"font-weight: bold\">}</span>\n",
       "            <span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'init'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'entity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'jrose'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'image_classification_train'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'job_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'train_supervised'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'group'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'train'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'run_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'tags'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'Extant_family_10_512'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'resnet50_512'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'train'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'supervised'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'init'</span>\n",
       "                <span style=\"font-weight: bold\">]</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'config'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'stage'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'train'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'stage_idx'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'results_file'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stage_idx_2/train_results.csv'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'results_path'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/models/resnet50_512/results/stage_idx_2/train_results.csv'</span>\n",
       "                <span style=\"font-weight: bold\">}</span>\n",
       "            <span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'3'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'init'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'entity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'jrose'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'image_classification_train'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'job_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'test_supervised'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'group'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'test'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'run_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'tags'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'Extant_family_10_512'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'resnet50_512'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'test'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'supervised'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'init'</span>\n",
       "                <span style=\"font-weight: bold\">]</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'config'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'stage'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'test'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'stage_idx'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'3'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'results_file'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stage_idx_3/test_results.csv'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'results_path'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/models/resnet50_512/results/stage_idx_3/test_results.csv'</span>\n",
       "                <span style=\"font-weight: bold\">}</span>\n",
       "            <span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'artifacts'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'root_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/artifacts'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_root_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/models/resnet50_512'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'input_dataset_artifact'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'basename'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Extant_family_10_512'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Extant_family_10_512'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'image_classification_datasets'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'entity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'jrose'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'version'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'latest'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'raw_data'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'artifact_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'InputArtifact'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'root_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/artifacts'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'uri'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'jrose/image_classification_datasets/Extant_family_10_512:latest'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'input_model_artifact'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'basename'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'resnet50_imagenet'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'resnet50_imagenet_init_model'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'image_classification_train'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'entity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'jrose'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'version'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'latest'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'artifact_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'InputModelArtifact'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model_stage'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'init'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'init_model'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'root_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/artifacts'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model_dir'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/models/resnet50_512/resnet50_imagenet/init'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model_path'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/models/resnet50_512/resnet50_imagenet/init/best_model.ckpt'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'datasets_seen'</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'uri'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'jrose/image_classification_train/resnet50_imagenet_init_model:latest'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'output_model_artifact'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'basename'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'resnet50_Extant_family_10_512'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'resnet50_Extant_family_10_512_train_model'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'image_classification_train'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'entity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'jrose'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'version'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'latest'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'artifact_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'OutputModelArtifact'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'resnet50_Extant_family_10_512_train_model'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model_stage'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'train'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'train_model'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'root_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/artifacts'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model_dir'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/models/resnet50_512/resnet50_Extant_family_10_512/train'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model_path'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/models/resnet50_512/resnet50_Extant_</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">family_10_512/train/best_model.ckpt'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'uri'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'jrose/image_classification_train/resnet50_Extant_family_10_512_train_model:latest'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'datasets_seen'</span>: <span style=\"font-weight: bold\">[]</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'tuner'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'trainer_kwargs'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'auto_scale_batch_size'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'power'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'auto_lr_find'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'tuner_kwargs'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'lr'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'min_lr'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-08</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'max_lr'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num_training'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'mode'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'exponential'</span><span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'root_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'results_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/models/resnet50_512/results'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'debug'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'print_config'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'disable_warnings'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'trainer'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'_target_'\u001b[0m: \u001b[32m'pytorch_lightning.Trainer'\u001b[0m,\n",
       "        \u001b[32m'gpus'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "        \u001b[32m'min_epochs'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "        \u001b[32m'max_epochs'\u001b[0m: \u001b[1;36m40\u001b[0m,\n",
       "        \u001b[32m'weights_summary'\u001b[0m: \u001b[32m'top'\u001b[0m,\n",
       "        \u001b[32m'progress_bar_refresh_rate'\u001b[0m: \u001b[1;36m10\u001b[0m,\n",
       "        \u001b[32m'profiler'\u001b[0m: \u001b[32m'simple'\u001b[0m,\n",
       "        \u001b[32m'log_every_n_steps'\u001b[0m: \u001b[1;36m50\u001b[0m,\n",
       "        \u001b[32m'terminate_on_nan'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[32m'fast_dev_run'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[32m'limit_train_batches'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "        \u001b[32m'limit_val_batches'\u001b[0m: \u001b[1;36m1.0\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'model'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'_target_'\u001b[0m: \u001b[32m'lightning_hydra_classifiers.models.resnet.ResNet'\u001b[0m,\n",
       "        \u001b[32m'basename'\u001b[0m: \u001b[32m'resnet50'\u001b[0m,\n",
       "        \u001b[32m'name'\u001b[0m: \u001b[32m'resnet50_512'\u001b[0m,\n",
       "        \u001b[32m'optimizer'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'_target_'\u001b[0m: \u001b[32m'torch.optim.Adam'\u001b[0m,\n",
       "            \u001b[32m'name'\u001b[0m: \u001b[32m'Adam'\u001b[0m,\n",
       "            \u001b[32m'lr'\u001b[0m: \u001b[1;36m0.001\u001b[0m,\n",
       "            \u001b[32m'weight_decay'\u001b[0m: \u001b[1;36m0.0\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'num_classes'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[32m'input_shape'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m512\u001b[0m, \u001b[1;36m512\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[32m'batch_size'\u001b[0m: \u001b[1;36m256\u001b[0m,\n",
       "        \u001b[32m'lr'\u001b[0m: \u001b[1;36m0.001\u001b[0m,\n",
       "        \u001b[32m'head'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'avg_pool'\u001b[0m, \u001b[32m'flatten'\u001b[0m, \u001b[32m'fc'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[32m'unfreeze'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'layer4'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[32m'root_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache'\u001b[0m,\n",
       "        \u001b[32m'model_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache/models/resnet50_512'\u001b[0m,\n",
       "        \u001b[32m'model_path'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache/models/resnet50_512/best_model.pkl'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'datamodule'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'_target_'\u001b[0m: \u001b[32m'contrastive_learning.data.pytorch.extant.ExtantLightningDataModule'\u001b[0m,\n",
       "        \u001b[32m'basename'\u001b[0m: \u001b[32m'Extant'\u001b[0m,\n",
       "        \u001b[32m'name'\u001b[0m: \u001b[32m'Extant_family_10_512'\u001b[0m,\n",
       "        \u001b[32m'batch_size'\u001b[0m: \u001b[1;36m256\u001b[0m,\n",
       "        \u001b[32m'val_split'\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "        \u001b[32m'num_classes'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[32m'image_size'\u001b[0m: \u001b[1;36m512\u001b[0m,\n",
       "        \u001b[32m'channels'\u001b[0m: \u001b[1;36m3\u001b[0m,\n",
       "        \u001b[32m'class_type'\u001b[0m: \u001b[32m'family'\u001b[0m,\n",
       "        \u001b[32m'normalize'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "        \u001b[32m'seed'\u001b[0m: \u001b[1;36m98568764\u001b[0m,\n",
       "        \u001b[32m'predict_on_split'\u001b[0m: \u001b[32m'val'\u001b[0m,\n",
       "        \u001b[32m'debug'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[32m'num_workers'\u001b[0m: \u001b[1;36m4\u001b[0m,\n",
       "        \u001b[32m'root_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache'\u001b[0m,\n",
       "        \u001b[32m'dataset_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache/datasets/Extant_family_10_512'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'callbacks'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'model_checkpoint'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'_target_'\u001b[0m: \u001b[32m'pytorch_lightning.callbacks.ModelCheckpoint'\u001b[0m,\n",
       "            \u001b[32m'monitor'\u001b[0m: \u001b[32m'val_loss'\u001b[0m,\n",
       "            \u001b[32m'save_top_k'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "            \u001b[32m'save_last'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'mode'\u001b[0m: \u001b[32m'min'\u001b[0m,\n",
       "            \u001b[32m'verbose'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "            \u001b[32m'dirpath'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache/models/resnet50_512/checkpoints/'\u001b[0m,\n",
       "            \u001b[32m'filename'\u001b[0m: \u001b[32m'{epoch:02d}'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'early_stopping'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'_target_'\u001b[0m: \u001b[32m'pytorch_lightning.callbacks.early_stopping.EarlyStopping'\u001b[0m,\n",
       "            \u001b[32m'code_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache'\u001b[0m,\n",
       "            \u001b[32m'monitor'\u001b[0m: \u001b[32m'val/loss'\u001b[0m,\n",
       "            \u001b[32m'patience'\u001b[0m: \u001b[1;36m5\u001b[0m,\n",
       "            \u001b[32m'verbose'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "            \u001b[32m'mode'\u001b[0m: \u001b[32m'min'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'log_per_class_metrics_to_wandb'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'_target_'\u001b[0m: \n",
       "\u001b[32m'lightning_hydra_classifiers.callbacks.wandb_callbacks.LogPerClassMetricsToWandb'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'module_data_monitor'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \u001b[32m'pl_bolts.callbacks.ModuleDataMonitor'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'logger'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'wandb'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'_target_'\u001b[0m: \u001b[32m'pytorch_lightning.loggers.wandb.WandbLogger'\u001b[0m,\n",
       "            \u001b[32m'entity'\u001b[0m: \u001b[32m'jrose'\u001b[0m,\n",
       "            \u001b[32m'project'\u001b[0m: \u001b[32m'image_classification'\u001b[0m,\n",
       "            \u001b[32m'job_type'\u001b[0m: \u001b[32m'train'\u001b[0m,\n",
       "            \u001b[32m'group'\u001b[0m: \u001b[32m'stage_0'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'csv'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'_target_'\u001b[0m: \u001b[32m'pytorch_lightning.loggers.csv_logs.CSVLogger'\u001b[0m,\n",
       "            \u001b[32m'save_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache/models/resnet50_512/results/logs'\u001b[0m,\n",
       "            \u001b[32m'name'\u001b[0m: \u001b[32m'csv/'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'wandb'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'0'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'init'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'entity'\u001b[0m: \u001b[32m'jrose'\u001b[0m,\n",
       "                \u001b[32m'project'\u001b[0m: \u001b[32m'image_classification_train'\u001b[0m,\n",
       "                \u001b[32m'job_type'\u001b[0m: \u001b[32m'batch_size_tune_init'\u001b[0m,\n",
       "                \u001b[32m'group'\u001b[0m: \u001b[32m'tune'\u001b[0m,\n",
       "                \u001b[32m'run_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache'\u001b[0m,\n",
       "                \u001b[32m'tags'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'Extant_family_10_512'\u001b[0m, \u001b[32m'batch_size_tune'\u001b[0m, \u001b[32m'init'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                \u001b[32m'config'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'stage'\u001b[0m: \u001b[32m'batch_size_tune'\u001b[0m,\n",
       "                    \u001b[32m'stage_idx'\u001b[0m: \u001b[32m'0'\u001b[0m,\n",
       "                    \u001b[32m'results_file'\u001b[0m: \u001b[32m'stage_idx_0/hparam_batch_size.yaml'\u001b[0m,\n",
       "                    \u001b[32m'results_path'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache/models/resnet50_512/result\u001b[0m\n",
       "\u001b[32ms/stage_idx_0/hparam_batch_size.yaml'\u001b[0m,\n",
       "                    \u001b[32m'auto_scale_batch_size'\u001b[0m: \u001b[32m'power'\u001b[0m,\n",
       "                    \u001b[32m'image_size'\u001b[0m: \u001b[1;36m512\u001b[0m\n",
       "                \u001b[1m}\u001b[0m\n",
       "            \u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'1'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'init'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'entity'\u001b[0m: \u001b[32m'jrose'\u001b[0m,\n",
       "                \u001b[32m'project'\u001b[0m: \u001b[32m'image_classification_train'\u001b[0m,\n",
       "                \u001b[32m'job_type'\u001b[0m: \u001b[32m'lr_tune_init'\u001b[0m,\n",
       "                \u001b[32m'group'\u001b[0m: \u001b[32m'tune'\u001b[0m,\n",
       "                \u001b[32m'run_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache'\u001b[0m,\n",
       "                \u001b[32m'tags'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'Extant_family_10_512'\u001b[0m, \u001b[32m'resnet50_512'\u001b[0m, \u001b[32m'lr_tune'\u001b[0m, \u001b[32m'init'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                \u001b[32m'config'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'stage'\u001b[0m: \u001b[32m'lr_tune'\u001b[0m,\n",
       "                    \u001b[32m'stage_idx'\u001b[0m: \u001b[32m'1'\u001b[0m,\n",
       "                    \u001b[32m'results_file'\u001b[0m: \u001b[32m'stage_idx_1/hparam_lr.yaml'\u001b[0m,\n",
       "                    \u001b[32m'results_path'\u001b[0m: \n",
       "\u001b[32m'/media/data/jacob/wandb_cache/models/resnet50_512/results/stage_idx_1/hparam_lr.yaml'\u001b[0m,\n",
       "                    \u001b[32m'tuner_kwargs'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                        \u001b[32m'min_lr'\u001b[0m: \u001b[1;36m1e-08\u001b[0m,\n",
       "                        \u001b[32m'max_lr'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "                        \u001b[32m'num_training'\u001b[0m: \u001b[1;36m100\u001b[0m,\n",
       "                        \u001b[32m'mode'\u001b[0m: \u001b[32m'exponential'\u001b[0m\n",
       "                    \u001b[1m}\u001b[0m\n",
       "                \u001b[1m}\u001b[0m\n",
       "            \u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'2'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'init'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'entity'\u001b[0m: \u001b[32m'jrose'\u001b[0m,\n",
       "                \u001b[32m'project'\u001b[0m: \u001b[32m'image_classification_train'\u001b[0m,\n",
       "                \u001b[32m'job_type'\u001b[0m: \u001b[32m'train_supervised'\u001b[0m,\n",
       "                \u001b[32m'group'\u001b[0m: \u001b[32m'train'\u001b[0m,\n",
       "                \u001b[32m'run_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache'\u001b[0m,\n",
       "                \u001b[32m'tags'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "                    \u001b[32m'Extant_family_10_512'\u001b[0m,\n",
       "                    \u001b[32m'resnet50_512'\u001b[0m,\n",
       "                    \u001b[32m'train'\u001b[0m,\n",
       "                    \u001b[32m'supervised'\u001b[0m,\n",
       "                    \u001b[32m'init'\u001b[0m\n",
       "                \u001b[1m]\u001b[0m,\n",
       "                \u001b[32m'config'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'stage'\u001b[0m: \u001b[32m'train'\u001b[0m,\n",
       "                    \u001b[32m'stage_idx'\u001b[0m: \u001b[32m'2'\u001b[0m,\n",
       "                    \u001b[32m'results_file'\u001b[0m: \u001b[32m'stage_idx_2/train_results.csv'\u001b[0m,\n",
       "                    \u001b[32m'results_path'\u001b[0m: \n",
       "\u001b[32m'/media/data/jacob/wandb_cache/models/resnet50_512/results/stage_idx_2/train_results.csv'\u001b[0m\n",
       "                \u001b[1m}\u001b[0m\n",
       "            \u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'3'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'init'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'entity'\u001b[0m: \u001b[32m'jrose'\u001b[0m,\n",
       "                \u001b[32m'project'\u001b[0m: \u001b[32m'image_classification_train'\u001b[0m,\n",
       "                \u001b[32m'job_type'\u001b[0m: \u001b[32m'test_supervised'\u001b[0m,\n",
       "                \u001b[32m'group'\u001b[0m: \u001b[32m'test'\u001b[0m,\n",
       "                \u001b[32m'run_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache'\u001b[0m,\n",
       "                \u001b[32m'tags'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "                    \u001b[32m'Extant_family_10_512'\u001b[0m,\n",
       "                    \u001b[32m'resnet50_512'\u001b[0m,\n",
       "                    \u001b[32m'test'\u001b[0m,\n",
       "                    \u001b[32m'supervised'\u001b[0m,\n",
       "                    \u001b[32m'init'\u001b[0m\n",
       "                \u001b[1m]\u001b[0m,\n",
       "                \u001b[32m'config'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'stage'\u001b[0m: \u001b[32m'test'\u001b[0m,\n",
       "                    \u001b[32m'stage_idx'\u001b[0m: \u001b[32m'3'\u001b[0m,\n",
       "                    \u001b[32m'results_file'\u001b[0m: \u001b[32m'stage_idx_3/test_results.csv'\u001b[0m,\n",
       "                    \u001b[32m'results_path'\u001b[0m: \n",
       "\u001b[32m'/media/data/jacob/wandb_cache/models/resnet50_512/results/stage_idx_3/test_results.csv'\u001b[0m\n",
       "                \u001b[1m}\u001b[0m\n",
       "            \u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'artifacts'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'root_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache/artifacts'\u001b[0m,\n",
       "        \u001b[32m'model_root_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache/models/resnet50_512'\u001b[0m,\n",
       "        \u001b[32m'input_dataset_artifact'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'basename'\u001b[0m: \u001b[32m'Extant_family_10_512'\u001b[0m,\n",
       "            \u001b[32m'name'\u001b[0m: \u001b[32m'Extant_family_10_512'\u001b[0m,\n",
       "            \u001b[32m'project'\u001b[0m: \u001b[32m'image_classification_datasets'\u001b[0m,\n",
       "            \u001b[32m'entity'\u001b[0m: \u001b[32m'jrose'\u001b[0m,\n",
       "            \u001b[32m'version'\u001b[0m: \u001b[32m'latest'\u001b[0m,\n",
       "            \u001b[32m'type'\u001b[0m: \u001b[32m'raw_data'\u001b[0m,\n",
       "            \u001b[32m'artifact_type'\u001b[0m: \u001b[32m'InputArtifact'\u001b[0m,\n",
       "            \u001b[32m'description'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[32m'root_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache/artifacts'\u001b[0m,\n",
       "            \u001b[32m'uri'\u001b[0m: \u001b[32m'jrose/image_classification_datasets/Extant_family_10_512:latest'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'input_model_artifact'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'basename'\u001b[0m: \u001b[32m'resnet50_imagenet'\u001b[0m,\n",
       "            \u001b[32m'name'\u001b[0m: \u001b[32m'resnet50_imagenet_init_model'\u001b[0m,\n",
       "            \u001b[32m'project'\u001b[0m: \u001b[32m'image_classification_train'\u001b[0m,\n",
       "            \u001b[32m'entity'\u001b[0m: \u001b[32m'jrose'\u001b[0m,\n",
       "            \u001b[32m'version'\u001b[0m: \u001b[32m'latest'\u001b[0m,\n",
       "            \u001b[32m'artifact_type'\u001b[0m: \u001b[32m'InputModelArtifact'\u001b[0m,\n",
       "            \u001b[32m'description'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[32m'model_stage'\u001b[0m: \u001b[32m'init'\u001b[0m,\n",
       "            \u001b[32m'type'\u001b[0m: \u001b[32m'init_model'\u001b[0m,\n",
       "            \u001b[32m'root_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache/artifacts'\u001b[0m,\n",
       "            \u001b[32m'model_dir'\u001b[0m: \n",
       "\u001b[32m'/media/data/jacob/wandb_cache/models/resnet50_512/resnet50_imagenet/init'\u001b[0m,\n",
       "            \u001b[32m'model_path'\u001b[0m: \n",
       "\u001b[32m'/media/data/jacob/wandb_cache/models/resnet50_512/resnet50_imagenet/init/best_model.ckpt'\u001b[0m,\n",
       "            \u001b[32m'datasets_seen'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[32m'uri'\u001b[0m: \u001b[32m'jrose/image_classification_train/resnet50_imagenet_init_model:latest'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'output_model_artifact'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'basename'\u001b[0m: \u001b[32m'resnet50_Extant_family_10_512'\u001b[0m,\n",
       "            \u001b[32m'name'\u001b[0m: \u001b[32m'resnet50_Extant_family_10_512_train_model'\u001b[0m,\n",
       "            \u001b[32m'project'\u001b[0m: \u001b[32m'image_classification_train'\u001b[0m,\n",
       "            \u001b[32m'entity'\u001b[0m: \u001b[32m'jrose'\u001b[0m,\n",
       "            \u001b[32m'version'\u001b[0m: \u001b[32m'latest'\u001b[0m,\n",
       "            \u001b[32m'artifact_type'\u001b[0m: \u001b[32m'OutputModelArtifact'\u001b[0m,\n",
       "            \u001b[32m'description'\u001b[0m: \u001b[32m'resnet50_Extant_family_10_512_train_model'\u001b[0m,\n",
       "            \u001b[32m'model_stage'\u001b[0m: \u001b[32m'train'\u001b[0m,\n",
       "            \u001b[32m'type'\u001b[0m: \u001b[32m'train_model'\u001b[0m,\n",
       "            \u001b[32m'root_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache/artifacts'\u001b[0m,\n",
       "            \u001b[32m'model_dir'\u001b[0m: \n",
       "\u001b[32m'/media/data/jacob/wandb_cache/models/resnet50_512/resnet50_Extant_family_10_512/train'\u001b[0m,\n",
       "            \u001b[32m'model_path'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache/models/resnet50_512/resnet50_Extant_\u001b[0m\n",
       "\u001b[32mfamily_10_512/train/best_model.ckpt'\u001b[0m,\n",
       "            \u001b[32m'uri'\u001b[0m: \n",
       "\u001b[32m'jrose/image_classification_train/resnet50_Extant_family_10_512_train_model:latest'\u001b[0m,\n",
       "            \u001b[32m'datasets_seen'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'tuner'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'trainer_kwargs'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'auto_scale_batch_size'\u001b[0m: \u001b[32m'power'\u001b[0m, \u001b[32m'auto_lr_find'\u001b[0m: \u001b[3;92mTrue\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'tuner_kwargs'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'batch_size'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[32m'lr'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'min_lr'\u001b[0m: \u001b[1;36m1e-08\u001b[0m, \u001b[32m'max_lr'\u001b[0m: \u001b[1;36m1\u001b[0m, \u001b[32m'num_training'\u001b[0m: \u001b[1;36m100\u001b[0m, \u001b[32m'mode'\u001b[0m: \u001b[32m'exponential'\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'root_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache'\u001b[0m,\n",
       "    \u001b[32m'results_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache/models/resnet50_512/results'\u001b[0m,\n",
       "    \u001b[32m'debug'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "    \u001b[32m'print_config'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[32m'disable_warnings'\u001b[0m: \u001b[3;92mTrue\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config_dir = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs\"\n",
    "overrides = []\n",
    "\n",
    "os.chdir(config_dir)\n",
    "# with initialize(config_path=\".\", job_name=\"test_app\"):   # \"../configs\", job_name=\"test_app\"):\n",
    "with initialize_config_dir(config_dir=config_dir, job_name=\"test_app\"):\n",
    "#     config = compose(config_name=\"PNAS_config\", overrides=overrides)\n",
    "    config = compose(config_name=\"Extant_config\", overrides=overrides)\n",
    "    pp(OmegaConf.to_container(config, resolve=True))\n",
    "    OmegaConf.set_struct(config, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e85609e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "603e21c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Initiating run_full_tuned_experiment\n",
      "Current Time = 13:13:06\n",
      "[FOUND] Previously completed trial. Results located in file:\n",
      "`/media/data/jacob/wandb_cache/models/resnet50_512/results/stage_idx_0/hparam_batch_size.yaml`\n",
      "[LOADING] Previous results + avoiding repetition of tuning procedure.\n",
      "Proceeding with batch_size = 512\n",
      "Model hparams =\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'optimized_hparam_key'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'resnet50'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num_classes'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'input_shape'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'optimizer'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.optim.Adam'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Adam'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'lr'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.001</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'weight_decay'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">}</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'seed'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'lr'</span>: \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.001</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'optimized_hparam_key'\u001b[0m: \u001b[32m'batch_size'\u001b[0m, \u001b[32m'model_name'\u001b[0m: \u001b[32m'resnet50'\u001b[0m, \u001b[32m'num_classes'\u001b[0m: \u001b[1;36m19\u001b[0m, \n",
       "\u001b[32m'input_shape'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m512\u001b[0m, \u001b[1;36m512\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'batch_size'\u001b[0m: \u001b[1;36m512\u001b[0m, \u001b[32m'optimizer'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \n",
       "\u001b[32m'torch.optim.Adam'\u001b[0m, \u001b[32m'name'\u001b[0m: \u001b[32m'Adam'\u001b[0m, \u001b[32m'lr'\u001b[0m: \u001b[1;36m0.001\u001b[0m, \u001b[32m'weight_decay'\u001b[0m: \u001b[1;36m0.0\u001b[0m\u001b[1m}\u001b[0m, \u001b[32m'seed'\u001b[0m: \u001b[3;35mNone\u001b[0m, \u001b[32m'lr'\u001b[0m: \n",
       "\u001b[1;36m0.001\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FOUND] Previously completed trial. Results located in file:\n",
      "`/media/data/jacob/wandb_cache/models/resnet50_512/results/stage_idx_1/hparam_lr.yaml`\n",
      "[LOADING] Previous results + avoiding repetition of tuning procedure.\n",
      "Proceeding with learning rate, lr = 0.000630957344480193\n",
      "Model hparams =\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'optimized_hparam_key'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'lr'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'lr'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000630957344480193</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'input_shape'</span>: \n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'image_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'optimized_hparam_key'\u001b[0m: \u001b[32m'lr'\u001b[0m, \u001b[32m'lr'\u001b[0m: \u001b[1;36m0.000630957344480193\u001b[0m, \u001b[32m'batch_size'\u001b[0m: \u001b[1;36m512\u001b[0m, \u001b[32m'input_shape'\u001b[0m: \n",
       "\u001b[1m[\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m512\u001b[0m, \u001b[1;36m512\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'image_size'\u001b[0m: \u001b[1;36m512\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjrose\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.26<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">whole-snowball-65</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/jrose/image_classification_train\" target=\"_blank\">https://wandb.ai/jrose/image_classification_train</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/jrose/image_classification_train/runs/oxhelyx4\" target=\"_blank\">https://wandb.ai/jrose/image_classification_train/runs/oxhelyx4</a><br/>\n",
       "                Run data is saved locally in <code>/media/data/jacob/wandb_cache/wandb/run-20210527_131307-oxhelyx4</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact Extant_family_10_512:latest, 16735.02MB. 26184 files... Done. 0:0:0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact resnet50_imagenet_init_model:latest, 90.18MB. 1 files... Done. 0:0:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048 524288\n",
      "model.load_model failed. Attempting model.load_from_checkpoint\n",
      "2048 100352\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/utils/train_basic_utils.py\u001b[0m in \u001b[0;36muse_model_artifact\u001b[0;34m(model, artifact_config, model_config, run_or_api)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# RunTimeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/models/resnet.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".ckpt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1052\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([19, 2048]) from checkpoint, the shape in current model is torch.Size([179, 2048]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([179]).\n\tsize mismatch for classifier.2.weight: copying a param with shape torch.Size([19, 2048]) from checkpoint, the shape in current model is torch.Size([179, 2048]).\n\tsize mismatch for classifier.2.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([179]).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e64848ea14cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Current Time =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mrun_full_tuned_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/train_basic.py\u001b[0m in \u001b[0;36mrun_full_tuned_experiment\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m### train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     trainer, datamodule, model, config = run_train(config=config,\n\u001b[0m\u001b[1;32m     77\u001b[0m                                                    pipeline_stage=\"2\")\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m## Part III: Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/train_basic.py\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m(config, pipeline_stage)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;31m# assert (num_classes == 19) | (num_classes == 179)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m     model, model_artifact = use_model_artifact(model=None,\n\u001b[0m\u001b[1;32m    354\u001b[0m                                                \u001b[0martifact_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0martifacts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_model_artifact\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                                                \u001b[0mmodel_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/utils/train_basic_utils.py\u001b[0m in \u001b[0;36muse_model_artifact\u001b[0;34m(model, artifact_config, model_config, run_or_api)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# RunTimeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.load_model failed. Attempting model.load_from_checkpoint'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#config.artifacts.output_model_artifact.model_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/pytorch_lightning/core/saving.py\u001b[0m in \u001b[0;36mload_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHECKPOINT_HYPER_PARAMS_KEY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_model_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/pytorch_lightning/core/saving.py\u001b[0m in \u001b[0;36m_load_model_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;31m# load the state_dict on the model automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'state_dict'"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "print('#'*20)\n",
    "print(f'Initiating run_full_tuned_experiment')\n",
    "\n",
    "start_time = datetime.now()\n",
    "current_time = start_time.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)\n",
    "\n",
    "run_full_tuned_experiment(config=config)\n",
    "\n",
    "end_time = datetime.now()\n",
    "total_duration = end_time - start_time\n",
    "\n",
    "current_time = start_time.strftime(\"%H:%M:%S\")\n",
    "total_time = total_duration.strftime(\"%H:%M:%S\")\n",
    "\n",
    "print(\"Current Time =\", current_time)\n",
    "print(f'[FINISHED ALL STAGES] - Total run duration: {total_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6681499",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88249767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(3, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(3, 2)\n",
    "\n",
    "        # initialize weights and biases\n",
    "        self.lin1.weight = nn.Parameter(torch.arange(-4.0, 5.0).view(3, 3))\n",
    "        self.lin1.bias = nn.Parameter(torch.zeros(1,3))\n",
    "        self.lin2.weight = nn.Parameter(torch.arange(-3.0, 3.0).view(2, 3))\n",
    "        self.lin2.bias = nn.Parameter(torch.ones(1,2))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.lin2(self.relu(self.lin1(input)))\n",
    "\n",
    "\n",
    "model = ToyModel()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817aef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(2, 3)\n",
    "baseline = torch.zeros(2, 3)\n",
    "\n",
    "ig = IntegratedGradients(model)\n",
    "attributions, delta = ig.attribute(input, baseline, target=0, return_convergence_delta=True)\n",
    "print('IG Attributions:', attributions)\n",
    "print('Convergence Delta:', delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0d6068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e60fa3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "921a9eca",
   "metadata": {
    "tags": []
   },
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dae60c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### experiment functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87952565",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 389\n",
      "Global seed set to 389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "TITAN X (Pascal)\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "Using device: cuda\n",
      "TITAN X (Pascal)\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pytorch_lightning as pl\n",
    "# import torchvision\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# import matplotlib.pyplot as plt\n",
    "# from contrastive_learning.data.pytorch.tensor import tensor_nbytes\n",
    "# from contrastive_learning.data.pytorch.pnas import PNASLightningDataModule\n",
    "# from contrastive_learning.data.pytorch.extant import ExtantLightningDataModule\n",
    "# from contrastive_learning.data.pytorch.common import DataStageError, colorbar, LeavesLightningDataModule\n",
    "# from contrastive_learning.data.pytorch.datamodules import get_datamodule, fetch_datamodule_from_dataset_artifact\n",
    "\n",
    "# from contrastive_learning.data.pytorch.utils.file_utils  import ensure_dir_exists\n",
    "\n",
    "# from lightning_hydra_classifiers.callbacks.wandb_callbacks import LogPerClassMetricsToWandb, WandbClassificationCallback\n",
    "# from lightning_hydra_classifiers.models.resnet import ResNet\n",
    "# from lightning_hydra_classifiers.utils.train_basic_utils import build_and_log_model_to_artifact, use_model_artifact, log_model_artifact, run_test_model\n",
    "\n",
    "\n",
    "# import lightning_hydra_classifiers\n",
    "# from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "# from pl_bolts.callbacks import ModuleDataMonitor, BatchGradientVerificationCallback\n",
    "\n",
    "# import inspect\n",
    "# pl.trainer.seed_everything(seed=389)\n",
    "\n",
    "# from stuf import stuf\n",
    "# from box import Box\n",
    "# from rich import print as pp\n",
    "\n",
    "# from lightning_hydra_classifiers.utils.train_basic_utils import * # build_and_log_model_to_artifact\n",
    "# from hydra.experimental import compose, initialize, initialize_config_dir\n",
    "# from omegaconf import OmegaConf\n",
    "# import os\n",
    "# from rich import print as pp\n",
    "# from typing import Union\n",
    "# from box import Box\n",
    "# import yaml\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print('Using device:', device)\n",
    "# if device.type == 'cuda':\n",
    "#     print(torch.cuda.get_device_name(0))\n",
    "#     print('Memory Usage:')\n",
    "#     print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "#     print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "# def efficient_zero_grad(model):\n",
    "#     \"\"\"\n",
    "#     [source] https://ai.plainenglish.io/best-performance-tuning-practices-for-pytorch-3ef06329d5fe\n",
    "#     \"\"\"\n",
    "#     for param in model.parameters():\n",
    "#         param.grad = None\n",
    "\n",
    "# def run_batch_size_tuner(config: Box,\n",
    "#                          pipeline_stage: Union[str,int]=\"0\"):\n",
    "#     \"\"\"\n",
    "#     Batch size tuner\n",
    "#     \"\"\"\n",
    "#     pipeline_stage = str(pipeline_stage)\n",
    "#     stage_config = config.wandb[pipeline_stage]\n",
    "    \n",
    "#     if not config.tuner.trainer_kwargs.auto_scale_batch_size:\n",
    "#         print(f'config.trainer.auto_scale_batch_size is set to False, Skipping run_batch_size_tuner().')\n",
    "#         print(f'Proceeding with batch size = {config.model.batch_size}')\n",
    "#         return None, None, config, None\n",
    "#     if os.path.isfile(stage_config.init.config.results_path):\n",
    "        \n",
    "#         best_hparams = OmegaConf.load(stage_config.init.config.results_path)\n",
    "        \n",
    "#         best_bsz = best_hparams['batch_size']\n",
    "#         config.datamodule.batch_size = best_bsz\n",
    "#         assert config.model.batch_size == best_bsz\n",
    "        \n",
    "#         print(f'[FOUND] Previously completed trial. Results located in file:\\n`{stage_config.init.config.results_path}`')\n",
    "#         print(f'[LOADING] Previous results + avoiding repetition of tuning procedure.')\n",
    "#         print(f'Proceeding with batch_size = {config.model.batch_size}')\n",
    "#         print('Model hparams =')\n",
    "#         pp(best_hparams)\n",
    "        \n",
    "#         return None, None, config, best_hparams\n",
    "        \n",
    "# #         with open(stage_config.init.config.results_path, 'r') as f:\n",
    "# #             best_hparams = yaml.safe_load(f)\n",
    "# #         best_bsz = best_hparams['batch_size']\n",
    "# #         config.datamodule.batch_size = best_bsz\n",
    "# #         assert config.model.batch_size == best_bsz\n",
    "    \n",
    "#     os.environ[\"WANDB_PROJECT\"] = stage_config.init.project\n",
    "#     run = wandb.init(entity=stage_config.init.entity,\n",
    "#                      project=stage_config.init.project,\n",
    "#                      job_type=stage_config.init.job_type,\n",
    "#                      group=stage_config.init.group,\n",
    "#                      dir=stage_config.init.run_dir,\n",
    "#                      config=stage_config.init.config)\n",
    "\n",
    "#     datamodule, data_artifact = fetch_datamodule_from_dataset_artifact(dataset_config=config.datamodule,\n",
    "#                                                                        artifact_config=config.artifacts.input_dataset_artifact,\n",
    "#                                                                        run_or_api=run)\n",
    "#     config.model.num_classes = len(datamodule.classes)\n",
    "#     # assert (num_classes == 19) | (num_classes == 179)\n",
    "#     ########################################\n",
    "#     ########################################\n",
    "# #     model, model_artifact = build_and_log_model_to_artifact(model_config=config.model,\n",
    "# #                                         artifact_config=config.artifacts.input_model_artifact,\n",
    "# #                                         run_or_api=run)\n",
    "\n",
    "#     model, model_artifact = use_model_artifact(artifact_config=config.artifacts.input_model_artifact,\n",
    "#                                                model_config=config.model,\n",
    "#                                                run_or_api=run)\n",
    "    \n",
    "    \n",
    "#     trainer = configure_trainer(config)\n",
    "\n",
    "    \n",
    "#     bsz_tuner = trainer.tune(model, datamodule=datamodule) #, **config.tuner.tuner_kwargs.batch_size)\n",
    "#     best_bsz = model.hparams.batch_size\n",
    "        \n",
    "    \n",
    "# #     ensure_dir_exists(os.path.dirname(stage_config.init.config.results_path))\n",
    "# #     best_hparams = DictConfig({\"optimized_hparam_key\": \"batch_size\",\n",
    "# #                                   \"lr\":best_lr,\n",
    "# #                                   \"batch_size\":config.model.batch_size,\n",
    "# #                                   \"input_shape\": model.hparams.input_shape,\n",
    "# #                                   \"image_size\":config.datamodule.image_size})\n",
    "#     best_hparams = OmegaConf.merge({\"optimized_hparam_key\": \"batch_size\"},\n",
    "#                                        DictConfig(model.hparams))\n",
    "\n",
    "#     results_dir = os.path.dirname(stage_config.init.config.results_path)\n",
    "#     ensure_dir_exists(results_dir)\n",
    "#     OmegaConf.save(best_hparams, stage_config.init.config.results_path, resolve=True)\n",
    "    \n",
    "#     print(f'Saved best batch_size value == {best_bsz} (along w/ batch_size, image_size) to file located at: {stage_config.init.config.results_path}')\n",
    "#     print(f'File contents expected to contain: \\n{best_hparams}')\n",
    "# #     with open(stage_config.init.config.results_path, 'w') as fp:\n",
    "# #         yaml.dump({\"optimized_hparam_key\": \"batch_size\",\n",
    "# #                    **dict(model.hparams)}, fp)\n",
    "            \n",
    "#     config.datamodule.batch_size = best_bsz\n",
    "#     assert config.model.batch_size == best_bsz\n",
    "        \n",
    "#     run.summary['best_batch_size'] = config.model.batch_size\n",
    "#     run.summary['image_size'] = config.datamodule.image_size\n",
    "\n",
    "#     run.finish()\n",
    "    \n",
    "#     print(f'FINISHED: `run_batch_size_tuner(config, pipeline_stage={pipeline_stage})`')\n",
    "#     print(f'Proceeding with batch size = {config.model.batch_size}')\n",
    "    \n",
    "    \n",
    "#     return datamodule, model, config, best_hparams\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def run_lr_tuner(config: Box,\n",
    "#                  pipeline_stage: Union[str,int]=\"0\"):\n",
    "#     \"\"\"\n",
    "#     Learning rate tuner\n",
    "#     \"\"\"\n",
    "#     pipeline_stage = str(pipeline_stage)\n",
    "#     stage_config = config.wandb[pipeline_stage]\n",
    "\n",
    "#     if not config.tuner.trainer_kwargs.auto_lr_find:\n",
    "#         print(f'config.trainer.auto_lr_find is set to False, Skipping `run_lr_tuner(config, pipeline_stage={pipeline_stage})`')\n",
    "#         print(f'Proceeding with:\\n')\n",
    "#         print(f'Learning rate = {config.model.lr:.3e}')\n",
    "#         print(f'Batch size = {config.model.batch_size}')\n",
    "        \n",
    "#         return config.model.lr, None, None, None, config\n",
    "\n",
    "#     if os.path.isfile(stage_config.init.config.results_path):\n",
    "# #         with open(stage_config.init.config.results_path, 'r') as f:\n",
    "# #             best_hparams = yaml.safe_load(f)\n",
    "            \n",
    "#         best_hparams = OmegaConf.load(stage_config.init.config.results_path)\n",
    "            \n",
    "#         best_lr = best_hparams['lr']\n",
    "#         config.model.optimizer.lr = best_lr\n",
    "#         assert config.model.lr == best_lr\n",
    "\n",
    "#         print(f'[FOUND] Previously completed trial. Results located in file:\\n`{stage_config.init.config.results_path}`')\n",
    "#         print(f'[LOADING] Previous results + avoiding repetition of tuning procedure.')\n",
    "#         print(f'Proceeding with learning rate, lr = {config.model.optimizer.lr}')\n",
    "#         print('Model hparams =')\n",
    "#         pp(best_hparams)\n",
    "#         return config.model.lr, None, None, None, config\n",
    "    \n",
    "    \n",
    "#     os.environ[\"WANDB_PROJECT\"] = stage_config.init.project\n",
    "#     run = wandb.init(entity=stage_config.init.entity,\n",
    "#                      project=stage_config.init.project,\n",
    "#                      job_type=stage_config.init.job_type,\n",
    "#                      group=stage_config.init.group,\n",
    "#                      dir=stage_config.init.run_dir,\n",
    "#                      config=stage_config.init.config)\n",
    "\n",
    "#     datamodule, data_artifact = fetch_datamodule_from_dataset_artifact(dataset_config=config.datamodule,\n",
    "#                                                                        artifact_config=config.artifacts.input_dataset_artifact,\n",
    "#                                                                        run_or_api=run)\n",
    "#     config.model.num_classes = len(datamodule.classes)\n",
    "#     # assert (num_classes == 19) | (num_classes == 179)\n",
    "#     ########################################\n",
    "#     ########################################\n",
    "# #     model, model_artifact = build_and_log_model_to_artifact(model_config=config.model,\n",
    "# #                                         artifact_config=config.artifacts.input_model_artifact,\n",
    "# #                                         run_or_api=run)\n",
    "\n",
    "#     model, model_artifact = use_model_artifact(artifact_config=config.artifacts.input_model_artifact,\n",
    "#                                                model_config=config.model,\n",
    "#                                                run_or_api=run)\n",
    "\n",
    "\n",
    "#     trainer = configure_trainer(config)\n",
    "    \n",
    "#     try:\n",
    "#         model.hparams = OmegaConf.to_container(model.hparams, resolve=True)\n",
    "#         print('Continuing with model.hparams:', model.hparams)\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         print('conversion from Omegaconf failed', model.hparams)\n",
    "#         print('continuing')    \n",
    "    \n",
    "#     lr_tuner = trainer.tuner.lr_find(model, datamodule, **config.tuner.tuner_kwargs.lr)\n",
    "\n",
    "#     # TODO: pickle lr_tuner object\n",
    "#     lr_tuner_results = lr_tuner.results\n",
    "#     best_lr = lr_tuner.suggestion()\n",
    "    \n",
    "#     suggestion = {\"lr\": best_lr,\n",
    "#                   \"loss\":lr_tuner_results['loss'][lr_tuner._optimal_idx]}\n",
    "\n",
    "#     model.hparams.lr = suggestion[\"lr\"]\n",
    "#     config.model.optimizer.lr = model.hparams.lr\n",
    "#     config.model.lr = model.hparams.lr\n",
    "#     run.config.update(config)\n",
    "    \n",
    "        \n",
    "#     best_hparams = DictConfig({\"optimized_hparam_key\": \"lr\",\n",
    "#                                   \"lr\":best_lr,\n",
    "#                                   \"batch_size\":config.model.batch_size,\n",
    "#                                   \"input_shape\": model.hparams.input_shape,\n",
    "#                                   \"image_size\":config.datamodule.image_size})\n",
    "    \n",
    "#     results_dir = os.path.dirname(stage_config.init.config.results_path)\n",
    "#     ensure_dir_exists(results_dir)\n",
    "#     OmegaConf.save(best_hparams, stage_config.init.config.results_path, resolve=True)\n",
    "#     print(f'Saved best lr value (along w/ batch_size, image_size) to file located at: {stage_config.init.config.results_path}')\n",
    "#     print(f'File contents expected to contain: \\n{OmegaConf.to_yaml(best_hparams)}')\n",
    "    \n",
    "        \n",
    "#     fig = lr_tuner.plot(suggest=True)\n",
    "#     plot_fname = 'lr_tuner_results_loss-vs-lr.png'\n",
    "#     plot_path = os.path.join(results_dir, plot_fname)\n",
    "#     plt.suptitle(f\"Suggested lr={best_lr} |\\n| Searched {lr_tuner.num_training} lr values $\\in$ [{lr_tuner.lr_min},{lr_tuner.lr_max}] |\\n| bsz = {config.model.batch_size}\")\n",
    "#     plt.savefig(plot_path)\n",
    "#     run.summary['results_plot'] = wandb.Image(fig, caption=plot_fname)\n",
    "    \n",
    "    \n",
    "#     run.summary['best/loss'] = suggestion[\"loss\"]\n",
    "#     run.summary['best/lr'] = suggestion[\"lr\"]\n",
    "#     run.summary['batch_size'] = config.model.batch_size\n",
    "#     run.summary['image_size'] = config.datamodule.image_size\n",
    "#     run.summary['results'] = OmegaConf.to_container(best_hparams, resolve=True)\n",
    "\n",
    "#     run.finish()\n",
    "    \n",
    "    \n",
    "#     print(f'FINISHED: `run_lr_tuner(config, pipeline_stage={pipeline_stage})`')\n",
    "#     print(f'Proceeding with:\\n')\n",
    "#     print(f'Learning rate = {config.model.lr:.3e}')\n",
    "#     print(f'Batch size = {config.model.batch_size}')\n",
    "    \n",
    "    \n",
    "#     return suggestion, lr_tuner_results, datamodule, model, config\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def run_train(config: Box,\n",
    "#               pipeline_stage: Union[str,int]=\"0\"):\n",
    "#     \"\"\"\n",
    "    \n",
    "#     \"\"\"\n",
    "    \n",
    "    \n",
    "#     pipeline_stage = str(pipeline_stage)\n",
    "#     stage_config = config.wandb[pipeline_stage]\n",
    "    \n",
    "#     os.environ[\"WANDB_PROJECT\"] = stage_config.init.project\n",
    "#     run = wandb.init(entity=stage_config.init.entity,\n",
    "#                      project=stage_config.init.project,\n",
    "#                      job_type=stage_config.init.job_type,\n",
    "#                      group=stage_config.init.group,\n",
    "#                      dir=stage_config.init.run_dir,\n",
    "#                      config=config)\n",
    "\n",
    "#     datamodule, data_artifact = fetch_datamodule_from_dataset_artifact(dataset_config=config.datamodule,\n",
    "#                                                                        artifact_config=config.artifacts.input_dataset_artifact,\n",
    "#                                                                        run_or_api=run)\n",
    "#     config.model.num_classes = len(datamodule.classes)\n",
    "#     # assert (num_classes == 19) | (num_classes == 179)\n",
    "\n",
    "#     model, model_artifact = use_model_artifact(model=None,\n",
    "#                                                artifact_config=config.artifacts.input_model_artifact,\n",
    "#                                                model_config=config.model,\n",
    "#                                                run_or_api=run)\n",
    "\n",
    "\n",
    "#     trainer = configure_trainer(config, log_gpu_memory=True)\n",
    "\n",
    "#     trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "#     best_model_ckpt = trainer.callbacks[-1].best_model_path\n",
    "\n",
    "#     trainer.model = trainer.model.load_from_checkpoint(best_model_ckpt)\n",
    "#     trainer.save_checkpoint(config.artifacts.output_model_artifact.model_path)\n",
    "#     wandb.save(config.artifacts.output_model_artifact.model_path)\n",
    "\n",
    "#     model_artifact = log_model_artifact(artifact_config=config.artifacts.output_model_artifact,\n",
    "#                                         model_config=config.model,\n",
    "#                                         run_or_api=run,\n",
    "#                                         finalize=False)\n",
    "\n",
    "#     csv_logger = trainer.logger.experiment[1]\n",
    "#     model_artifact.add_dir(csv_logger.log_dir)\n",
    "#     run.log_artifact(model_artifact)\n",
    "    \n",
    "#     run.finish()\n",
    "#     print(f'Finished train!')\n",
    "    \n",
    "#     return trainer, datamodule, model, config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc71168",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee7df614",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config_dir = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs\"\n",
    "\n",
    "# overrides = []#'+trainer.fast_dev_run=true',\n",
    "#              'trainer.auto_scale_batch_size=false',\n",
    "#              'trainer.auto_lr_find=false', #true',\n",
    "#             \"trainer.limit_train_batches=10\",\n",
    "#             \"trainer.limit_val_batches=5\",\n",
    "#             \"trainer.max_epochs=2\",\n",
    "#              \"model.optimizer.lr=float(2.75e-3)\",\n",
    "#              \"tuner.tuner_kwargs.lr.num_training=20\", # \"${datamodule.num_classes}\",\n",
    "# #              \"model.optimizer.lr=float(3.3e-3)\",\n",
    "#              'datamodule.batch_size=256']\n",
    "#              'datamodule=pnas_datamodule.yaml',\n",
    "#              'model=pnas_resnet_model.yaml']\n",
    "#              'datamodule=extant_datamodule.yaml',\n",
    "#              'model=extant_resnet_model.yaml']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f88bc87",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TODO: finish `def run_lr_tuner()`\n",
    "\n",
    "* figure out how to reliably restore at beginning or end of arbitrary pipeline stage. e.g. Checkpoints with output optimized summary for:\n",
    "    - bsz_tune\n",
    "    - lr_tune\n",
    "    - train\n",
    "    - etc\n",
    "    \n",
    "* Add lr_tune and auto_batch_size configs to config.search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3293c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### bsz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e2f718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FOUND] Previously completed trial. Results located in file:\n",
      "`/media/data/jacob/wandb_cache/models/resnet50_512/results/stage_idx_0/hparam_batch_size.yaml`\n",
      "[LOADING] Previous results + avoiding repetition of tuning procedure.\n",
      "Proceeding with batch_size = 512\n",
      "Model hparams =\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'optimized_hparam_key'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'resnet50'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num_classes'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'input_shape'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'optimizer'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.optim.Adam'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Adam'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'lr'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.001</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'weight_decay'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">}</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'seed'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'lr'</span>: \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.001</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'optimized_hparam_key'\u001b[0m: \u001b[32m'batch_size'\u001b[0m, \u001b[32m'model_name'\u001b[0m: \u001b[32m'resnet50'\u001b[0m, \u001b[32m'num_classes'\u001b[0m: \u001b[1;36m19\u001b[0m, \n",
       "\u001b[32m'input_shape'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m512\u001b[0m, \u001b[1;36m512\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'batch_size'\u001b[0m: \u001b[1;36m512\u001b[0m, \u001b[32m'optimizer'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \n",
       "\u001b[32m'torch.optim.Adam'\u001b[0m, \u001b[32m'name'\u001b[0m: \u001b[32m'Adam'\u001b[0m, \u001b[32m'lr'\u001b[0m: \u001b[1;36m0.001\u001b[0m, \u001b[32m'weight_decay'\u001b[0m: \u001b[1;36m0.0\u001b[0m\u001b[1m}\u001b[0m, \u001b[32m'seed'\u001b[0m: \u001b[3;35mNone\u001b[0m, \u001b[32m'lr'\u001b[0m: \n",
       "\u001b[1;36m0.001\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datamodule, model, config, best_hparams = run_batch_size_tuner(config=config, pipeline_stage=\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b47feb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from omegaconf import DictConfig, ListConfig, OmegaConf\n",
    "\n",
    "# import omegaconf\n",
    "# from omegaconf import DictConfig, ListConfig\n",
    "# # omegaconf.DictConfig\n",
    "\n",
    "# isinstance(config, DictConfig)\n",
    "\n",
    "# import yaml\n",
    "# results = {\"batch_size\":  883,\n",
    "#            \"input_shape\": [3, 224, 224],\n",
    "#            \"lr\":          0.00275,\n",
    "#            \"model_name\":  \"resnet50\",\n",
    "#            \"num_classes\": 19,\n",
    "#            \"optimizer\":   {'_target_': 'torch.optim.Adam', 'name': 'Adam', 'lr': 0.00275, 'weight_decay': 0.0}\n",
    "#           }\n",
    "\n",
    "# with open(config.wandb[\"0\"].init.config.results_path, 'w') as f:\n",
    "#     f.write(yaml.dump(results))\n",
    "# print(config.wandb[\"0\"].init.config.results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599affbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1e1793d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FOUND] Previously completed trial. Results located in file:\n",
      "`/media/data/jacob/wandb_cache/models/resnet50_512/results/stage_idx_1/hparam_lr.yaml`\n",
      "[LOADING] Previous results + avoiding repetition of tuning procedure.\n",
      "Proceeding with learning rate, lr = 0.000630957344480193\n",
      "Model hparams =\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'optimized_hparam_key'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'lr'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'lr'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000630957344480193</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'input_shape'</span>: \n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'image_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'optimized_hparam_key'\u001b[0m: \u001b[32m'lr'\u001b[0m, \u001b[32m'lr'\u001b[0m: \u001b[1;36m0.000630957344480193\u001b[0m, \u001b[32m'batch_size'\u001b[0m: \u001b[1;36m512\u001b[0m, \u001b[32m'input_shape'\u001b[0m: \n",
       "\u001b[1m[\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m512\u001b[0m, \u001b[1;36m512\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'image_size'\u001b[0m: \u001b[1;36m512\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "suggestion, results, datamodule, model, config = run_lr_tuner(config=config, pipeline_stage=\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271269d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20b19850",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjrose\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.26<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">earnest-bush-53</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/jrose/image_classification_train\" target=\"_blank\">https://wandb.ai/jrose/image_classification_train</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/jrose/image_classification_train/runs/ofrpdbjx\" target=\"_blank\">https://wandb.ai/jrose/image_classification_train/runs/ofrpdbjx</a><br/>\n",
       "                Run data is saved locally in <code>/media/data/jacob/wandb_cache/wandb/run-20210527_103656-ofrpdbjx</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact PNAS_family_100_512:latest, 788.44MB. 5314 files... Done. 0:0:0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-470e26c64a50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trainer, datamodule, model, config = run_train(config=config,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                                pipeline_stage=\"2\")\n",
      "\u001b[0;32m<ipython-input-1-12b453c01f2f>\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m(config, pipeline_stage)\u001b[0m\n\u001b[1;32m    307\u001b[0m                      config=config)\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     datamodule, data_artifact = fetch_datamodule_from_dataset_artifact(dataset_config=config.datamodule,\n\u001b[0m\u001b[1;32m    310\u001b[0m                                                                        \u001b[0martifact_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0martifacts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dataset_artifact\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                                                                        run_or_api=run)\n",
      "\u001b[0;32m/media/data/jacob/GitHub/prj_fossils_contrastive/contrastive_learning/data/pytorch/datamodules.py\u001b[0m in \u001b[0;36mfetch_datamodule_from_dataset_artifact\u001b[0;34m(dataset_config, artifact_config, run_or_api)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mdatamodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_datamodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mdatamodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0mdatamodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m########################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_prepared_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/jacob/GitHub/prj_fossils_contrastive/contrastive_learning/data/pytorch/common.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, stage, train_transform, eval_transform, target_transform)\u001b[0m\n\u001b[1;32m    241\u001b[0m               ):\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'fit'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             self.init_dataset_stage(stage='fit',\n\u001b[0m\u001b[1;32m    244\u001b[0m                                     \u001b[0mtrain_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                                     eval_transform=eval_transform)\n",
      "\u001b[0;32m/media/data/jacob/GitHub/prj_fossils_contrastive/contrastive_learning/data/pytorch/common.py\u001b[0m in \u001b[0;36minit_dataset_stage\u001b[0;34m(self, stage, train_transform, eval_transform)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'fit'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/jacob/GitHub/prj_fossils_contrastive/contrastive_learning/data/pytorch/common.py\u001b[0m in \u001b[0;36mget_dataset_split\u001b[0;34m(self, split)\u001b[0m\n\u001b[1;32m    304\u001b[0m                                                    return_paths=self.return_paths)\n\u001b[1;32m    305\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_split\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                 train_dataset, val_dataset = TrainValSplitDataset.train_val_split(train_dataset,\n\u001b[0m\u001b[1;32m    307\u001b[0m                                                                                   \u001b[0mval_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                                                                                   seed=self.seed)\n",
      "\u001b[0;32m/media/data/jacob/GitHub/prj_fossils_contrastive/contrastive_learning/data/pytorch/common.py\u001b[0m in \u001b[0;36mtrain_val_split\u001b[0;34m(cls, full_dataset, val_split, seed)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_from_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m         \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_from_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/jacob/GitHub/prj_fossils_contrastive/contrastive_learning/data/pytorch/common.py\u001b[0m in \u001b[0;36mselect_from_dataset\u001b[0;34m(cls, dataset, indices)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mselect_from_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0mupgraded_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mupgraded_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/jacob/GitHub/prj_fossils_contrastive/contrastive_learning/data/pytorch/common.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mnew_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0mold_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/jacob/GitHub/prj_fossils_contrastive/contrastive_learning/data/pytorch/common.py\u001b[0m in \u001b[0;36mfrom_dataset\u001b[0;34m(cls, dataset)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0mnew_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     ):\n\u001b[0;32m--> 226\u001b[0;31m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS if is_valid_file is None else None,\n\u001b[0m\u001b[1;32m    227\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    107\u001b[0m                                             target_transform=target_transform)\n\u001b[1;32m    108\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Found 0 files in subfolders of: {}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfnames\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollowlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/jrose3/envs/sequoia/lib/python3.8/os.py\u001b[0m in \u001b[0;36mwalk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[1;32m    360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                     \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscandir_it\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer, datamodule, model, config = run_train(config=config,\n",
    "                                               pipeline_stage=\"2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1ec1c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part III: Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a107b622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjrose\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.26<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">elated-universe-64</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/jrose/image_classification_train\" target=\"_blank\">https://wandb.ai/jrose/image_classification_train</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/jrose/image_classification_train/runs/xw05qa3i\" target=\"_blank\">https://wandb.ai/jrose/image_classification_train/runs/xw05qa3i</a><br/>\n",
       "                Run data is saved locally in <code>/media/data/jacob/wandb_cache/wandb/run-20210527_123622-xw05qa3i</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact PNAS_family_100_512:latest, 788.44MB. 5314 files... Done. 0:0:0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact resnet50_PNAS_family_100_512_train_model:latest, 204.72MB. 3 files... Done. 0:0:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048 524288\n",
      "model.load_model failed. Attempting model.load_from_checkpoint\n",
      "2048 524288\n",
      "2048 524288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "returning the test set for prediction~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]\n",
      "/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: Your predict_dataloader has `shuffle=True`, it is best practice to turn this off for validation and test dataloaders.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, predict dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4c1978ec064a0e89c0b7341885b521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Predicting', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60533daa7e8d415cb3d379f81227258c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epoch->', max=83.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch->', max=32.0, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2008<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 179.26MB of 179.26MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/media/data/jacob/wandb_cache/wandb/run-20210527_123622-xw05qa3i/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/media/data/jacob/wandb_cache/wandb/run-20210527_123622-xw05qa3i/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 2657 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">elated-universe-64</strong>: <a href=\"https://wandb.ai/jrose/image_classification_train/runs/xw05qa3i\" target=\"_blank\">https://wandb.ai/jrose/image_classification_train/runs/xw05qa3i</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'test_results' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-eb8703450774>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfix_catalog_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"PNAS\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m prediction_artifact, test_results = run_test_model(config,\n\u001b[0m\u001b[1;32m      4\u001b[0m                                                    \u001b[0mfix_catalog_number\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfix_catalog_number\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                    \u001b[0mpredictions_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#False,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/utils/train_basic_utils.py\u001b[0m in \u001b[0;36mrun_test_model\u001b[0;34m(config, fix_catalog_number, predictions_only, pipeline_stage)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprediction_artifact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'test_results' referenced before assignment"
     ]
    }
   ],
   "source": [
    "fix_catalog_number = \"PNAS\" in config.datamodule.basename\n",
    "\n",
    "prediction_artifact, test_results = run_test_model(config,\n",
    "                                                   fix_catalog_number=fix_catalog_number,\n",
    "                                                   predictions_only=True, #False,\n",
    "                                                   pipeline_stage=\"3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96aaf2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82206788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efb1154c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part IV: Finished main pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d83abe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37579a29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Above is the train() block\n",
    "\n",
    "\n",
    "\n",
    "# pipeline_stage = \"2\"\n",
    "# pipeline_stage = str(pipeline_stage)\n",
    "# stage_config = config.wandb[pipeline_stage]\n",
    "\n",
    "# # pp(OmegaConf.to_container(config.artifacts, resolve=True))\n",
    "\n",
    "# from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "# for i in config.values():\n",
    "#     print(type(i))\n",
    "\n",
    "# with open('temp.yaml', 'w') as fp:\n",
    "#     fp.write(OmegaConf.to_yaml(config, resolve=True))\n",
    "\n",
    "# os.getcwd()\n",
    "\n",
    "# OmegaConf.save(config, 'temp.yaml', resolve=True)\n",
    "\n",
    "# temp = OmegaConf.load('temp.yaml')\n",
    "# #(config, 'temp.yaml', resolve=True)\n",
    "\n",
    "# best_hparams = OmegaConf.merge({\"optimized_hparam_key\": \"batch_size\"},\n",
    "#                                   DictConfig(model.hparams))\n",
    "# OmegaConf.to_yaml(best_hparams)\n",
    "\n",
    "# print(config.model.input_shape)\n",
    "# config.model.input_shape = OmegaConf.to_container(config.model.input_shape, resolve=True)\n",
    "# print(config.model.input_shape)\n",
    "\n",
    "# model.hparams.input_shape = config.model.input_shape\n",
    "\n",
    "# model.hparams\n",
    "\n",
    "# # if False:  # os.path.isfile(stage_config.init.config.results_path):\n",
    "# #     if os.path.isfile(stage_config.init.config.results_path):\n",
    "# #         best_hparams = OmegaConf.load(best_hparams, stage_config.init.config.results_path)\n",
    "            \n",
    "# #         best_lr = best_hparams['lr']\n",
    "# #     with open(stage_config.init.config.results_path, 'r') as f:\n",
    "# #         results = yaml.safe_load(f)\n",
    "# #     print('results =', results)\n",
    "# #     print(f'[FOUND] Results from previously completed training trial, located in file: {stage_config.init.config.results_path}')\n",
    "# #     print(f'Loading previous results + avoiding repetition of training procedure.')\n",
    "        \n",
    "# #     pass\n",
    "\n",
    "\n",
    "# wandb.restore(best_model_ckpt)\n",
    "# model.load_from_checkpoint(best_model_ckpt)\n",
    "# wandb.save(config.artifacts.output_model_artifact)\n",
    "# vars(trainer.callbacks[-1])\n",
    "\n",
    "# os.listdir('/media/data/jacob/wandb_cache/wandb/run-20210526_202605-1wya93da/files/')\n",
    "# os.listdir('.')#/media/data/jacob/wandb_cache/wandb/run-20210526_202605-1wya93da/files/')\n",
    "\n",
    "# wandb.save(config.artifacts.output_model_artifact)\n",
    "# wandb.restore(best_model_ckpt)\n",
    "# model.load_from_checkpoint(best_model_ckpt)\n",
    "\n",
    "# artifact = log_model_artifact(model,\n",
    "#                               artifact_config=config.artifacts.output_model_artifact,\n",
    "#                               model_config=config.model,\n",
    "#                               run_or_api=run)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6421007a",
   "metadata": {},
   "source": [
    "## Consolidations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eacd95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b1d0c40",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### bsz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55bee100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_full_tuned_experiment(config: DictConfig):\n",
    "\n",
    "    datamodule, model, config, best_hparams = run_batch_size_tuner(config=config, pipeline_stage=\"0\")\n",
    "\n",
    "    #### lr\n",
    "\n",
    "    suggestion, results, datamodule, model, config = run_lr_tuner(config=config, pipeline_stage=\"1\")\n",
    "\n",
    "    ### train\n",
    "\n",
    "    trainer, datamodule, model, config = run_train(config=config,\n",
    "                                                   pipeline_stage=\"2\")\n",
    "    ## Part III: Test\n",
    "\n",
    "    fix_catalog_number = \"PNAS\" in config.datamodule.basename\n",
    "    prediction_artifact, test_results = run_test_model(config,\n",
    "                                                       fix_catalog_number=fix_catalog_number,\n",
    "                                                       predictions_only=False,\n",
    "                                                       pipeline_stage=\"3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f4171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4e2f32c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## To Do Wednesday May 26th, 2021:\n",
    "\n",
    "* wrap the fit() function\n",
    "* wrap the eval() function\n",
    "- [x] complete end-to-end train_basic.py cmdline experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a46242",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# config.wandb.model_artifact.output_model_dir\n",
    "\n",
    "\n",
    "# config.wandb.output_artifacts = [{\n",
    "#                                   os.path.join(*[config.wandb.artifacts.root_dir,\n",
    "#                                                  \"datasets\",\n",
    "#                                                  config.wandb.input_artifact.name \\\n",
    "#                                                  + f':{config.wandb.input_artifact.version}'\n",
    "#                                                  ])\n",
    "# }] \n",
    "# config.wandb.lr_tune_artifact.output_result_dir\n",
    "\n",
    "\n",
    "\n",
    "# config.wandb.input_artifact.root_dir = \n",
    "\n",
    "\n",
    "# config.wandb.model_artifact.root_dir = os.path.join(*[config.wandb.artifacts.root_dir,\n",
    "#                                                      \"models\",\n",
    "#                                                      config.wandb.model_artifact.basename \\\n",
    "#                                                  + f':{config.wandb.model_artifact.version}'\n",
    "#                                                     ])\n",
    "# config.wandb.model_artifact.init_model_dir = os.path.join(*[config.wandb.model_artifact.root_dir,\n",
    "#                                                             'init'\n",
    "#                                                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8cfdfd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Friday, May 21st 2021\n",
    "\n",
    "* TODO: Perform bootstrapping and/or k-val split for this ~50-100 sample learning rate search\n",
    "\n",
    "* Idea: What other hyperparameters can we constrain/further optimize with these wide->narrow sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d1ac763",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Pick point based on plot, or get suggestion\n",
    "# new_lr = lr_finder.suggestion()\n",
    "\n",
    "# print(f'Suggested starting learning rate: {new_lr:.2e}')\n",
    "\n",
    "# model.hparams.lr = new_lr\n",
    "# config.model.optimizer.init_lr = new_lr\n",
    "\n",
    "# fig = lr_finder.plot(suggest=True)\n",
    "# fig.show()\n",
    "\n",
    "# %%prun\n",
    "# trainer = configure_trainer(config, log_gpu_memory=True) # limit_train_batches=2, limit_val_batches=2)\n",
    "\n",
    "# trainer.fit(model, datamodule=datamodule)\n",
    "# best_ckpt = trainer.callbacks[-1].best_model_path\n",
    "\n",
    "\n",
    "# log_model_checkpoint_2_artifact(model, \n",
    "#                                 ckpt_path: str,\n",
    "#                                 artifact_config=config.wandb.output_artifacts[0],\n",
    "#                                 run=run)\n",
    "\n",
    "\n",
    "# fix_catalog_number = \"PNAS\" in config.dataset.name\n",
    "# trainer = configure_trainer(config) #, limit_test_batches=2)\n",
    "\n",
    "# test_results = test_model(trainer,\n",
    "#                           model,\n",
    "#                           output_model_artifact,\n",
    "#                           datamodule,\n",
    "#                           config,\n",
    "#                           fix_catalog_number=fix_catalog_number)\n",
    "\n",
    "\n",
    "\n",
    "# wandb.finish()\n",
    "# print(trainer.current_epoch, test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c296b136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def log_model_checkpoint_2_artifact(model, \n",
    "#                                     ckpt_path: str,\n",
    "#                                     artifact_config,\n",
    "#                                     run=None):\n",
    "    \n",
    "#     run = run or wandb.run\n",
    "#     os.makedirs(os.path.dirname(artifact_config.output_model_path), exist_ok=True)\n",
    "    \n",
    "#     model = model.load_from_checkpoint(ckpt_path)\n",
    "#     model.save_model(artifact_config.output_model_path)\n",
    "\n",
    "#     output_model_artifact = wandb.Artifact(\n",
    "#                                     artifact_config.output_name,\n",
    "#                                     type=artifact_config.output_type,\n",
    "#                                     description=artifact_config.description,\n",
    "#                                     metadata=dict(**artifact_config,\n",
    "#                                                   **config.model)\n",
    "#                                     )\n",
    "#     output_model_artifact.add_dir(artifact_config.output_model_dir)\n",
    "#     run.log_artifact(output_model_artifact)\n",
    "\n",
    "#     print(\"Output Model Artifact Checkpoint path:\\n\", \n",
    "#           artifact_config.output_model_path)  \n",
    "# log_model_checkpoint_2_artifact(model, \n",
    "#                                 ckpt_path: str,\n",
    "#                                 artifact_config=config.wandb.output_artifacts[0],\n",
    "#                                 run=run)\n",
    "\n",
    "    \n",
    "    \n",
    "#     model = model.load_from_checkpoint(ckpt_path)\n",
    "\n",
    "#     os.makedirs(os.path.dirname(config.wandb.model_artifact.output_model_path), exist_ok=True)\n",
    "#     model.save_model(config.wandb.model_artifact.output_model_path)\n",
    "\n",
    "#     output_model_artifact = wandb.Artifact(\n",
    "#                                     config.wandb.model_artifact.output_name,\n",
    "#                                     type=config.wandb.model_artifact.output_type,\n",
    "#                                     description=config.wandb.model_artifact.description,\n",
    "#                                     metadata=dict(**config.wandb.model_artifact,\n",
    "#                                                   **config.model)\n",
    "#                                     )\n",
    "#     output_model_artifact.add_dir(config.wandb.model_artifact.output_model_dir)\n",
    "#     run.log_artifact(output_model_artifact)\n",
    "#     run.finish()\n",
    "\n",
    "#     print(\"BEST MODEL:\\n\", config.wandb.model_artifact.output_model_path)\n",
    "\n",
    "#     wandb.run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3af4437",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "befd2eaa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2sivdqs3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 26579<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.18MB of 0.18MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/wandb/run-20210521_020303-2sivdqs3/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/wandb/run-20210521_020303-2sivdqs3/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>val/batch_idx</td><td>16</td></tr><tr><td>val/acc/top1</td><td>0.91525</td></tr><tr><td>val/acc/top3</td><td>0.97363</td></tr><tr><td>val/precision/top1</td><td>0.90252</td></tr><tr><td>val/recall/top1</td><td>0.89489</td></tr><tr><td>_runtime</td><td>472</td></tr><tr><td>_timestamp</td><td>1621577455</td></tr><tr><td>_step</td><td>250</td></tr><tr><td>epoch</td><td>2</td></tr><tr><td>train/batch_idx</td><td>34</td></tr><tr><td>train_loss_step</td><td>0.099</td></tr><tr><td>gpu_id: 0/memory.used (MB)</td><td>565.0</td></tr><tr><td>gpu_id: 1/memory.used (MB)</td><td>3053.0</td></tr><tr><td>gpu_id: 2/memory.used (MB)</td><td>1439.0</td></tr><tr><td>gpu_id: 3/memory.used (MB)</td><td>7581.0</td></tr><tr><td>gpu_id: 4/memory.used (MB)</td><td>7587.0</td></tr><tr><td>gpu_id: 5/memory.used (MB)</td><td>7581.0</td></tr><tr><td>gpu_id: 6/memory.used (MB)</td><td>7135.0</td></tr><tr><td>gpu_id: 7/memory.used (MB)</td><td>8852.0</td></tr><tr><td>trainer/global_step</td><td>149</td></tr><tr><td>train/acc/top1</td><td>0.90772</td></tr><tr><td>train/acc/top3</td><td>0.98588</td></tr><tr><td>train/precision/top1</td><td>0.91047</td></tr><tr><td>train/recall/top1</td><td>0.89165</td></tr><tr><td>train_loss_epoch</td><td>0.29329</td></tr><tr><td>val_loss_step/epoch_0</td><td>1.10232</td></tr><tr><td>val_loss_epoch</td><td>0.35539</td></tr><tr><td>val_loss_step/epoch_1</td><td>1.22443</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>val/batch_idx</td><td></td></tr><tr><td>val/acc/top1</td><td></td></tr><tr><td>val/acc/top3</td><td></td></tr><tr><td>val/precision/top1</td><td></td></tr><tr><td>val/recall/top1</td><td></td></tr><tr><td>_runtime</td><td></td></tr><tr><td>_timestamp</td><td></td></tr><tr><td>_step</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train/batch_idx</td><td></td></tr><tr><td>train_loss_step</td><td></td></tr><tr><td>gpu_id: 0/memory.used (MB)</td><td></td></tr><tr><td>gpu_id: 1/memory.used (MB)</td><td></td></tr><tr><td>gpu_id: 2/memory.used (MB)</td><td></td></tr><tr><td>gpu_id: 3/memory.used (MB)</td><td></td></tr><tr><td>gpu_id: 4/memory.used (MB)</td><td></td></tr><tr><td>gpu_id: 5/memory.used (MB)</td><td></td></tr><tr><td>gpu_id: 6/memory.used (MB)</td><td></td></tr><tr><td>gpu_id: 7/memory.used (MB)</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>train/acc/top1</td><td></td></tr><tr><td>train/acc/top3</td><td></td></tr><tr><td>train/precision/top1</td><td></td></tr><tr><td>train/recall/top1</td><td></td></tr><tr><td>train_loss_epoch</td><td></td></tr><tr><td>val_loss_step/epoch_0</td><td></td></tr><tr><td>val_loss_epoch</td><td></td></tr><tr><td>val_loss_step/epoch_1</td><td></td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">PNAS_family_100_1024-timm-resnet50</strong>: <a href=\"https://wandb.ai/jrose/image_classification/runs/2sivdqs3\" target=\"_blank\">https://wandb.ai/jrose/image_classification/runs/2sivdqs3</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:2sivdqs3). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.26<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">dandy-eon-140</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/jrose/image_classification\" target=\"_blank\">https://wandb.ai/jrose/image_classification</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/jrose/image_classification/runs/13l71er9\" target=\"_blank\">https://wandb.ai/jrose/image_classification/runs/13l71er9</a><br/>\n",
       "                Run data is saved locally in <code>/media/data/jacob/wandb_cache/wandb/run-20210521_021055-13l71er9</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact resnet50_output:latest, 90.18MB. 1 files... Done. 0:0:0\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]\n",
      "/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70be06ad53fa4ca69836b9e774299508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test/acc/top1': 0.6468373537063599,\n",
      " 'test/acc/top3': 0.8588102459907532,\n",
      " 'test/precision/top1': 0.650568425655365,\n",
      " 'test/recall/top1': 0.5781211256980896,\n",
      " 'test_loss': 1.350639820098877}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/val'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2b461b0bb758>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfigure_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, limit_test_batches=2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m test_results = test_model(trainer,\n\u001b[0m\u001b[1;32m      8\u001b[0m                           \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                           \u001b[0moutput_model_artifact\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-688f26f5d9f7>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(trainer, model, model_artifact, datamodule, config, fix_catalog_number, predictions_only)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mtest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mprediction_artifact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfix_catalog_number\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfix_catalog_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-688f26f5d9f7>\u001b[0m in \u001b[0;36mlog_predictions\u001b[0;34m(trainer, model, datamodule, fix_catalog_number)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlog_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfix_catalog_number\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mdatamodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"predict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_prepared_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/jacob/GitHub/prj_fossils_contrastive/contrastive_learning/data/pytorch/common.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, stage, train_transform, eval_transform, target_transform)\u001b[0m\n\u001b[1;32m    244\u001b[0m                                     eval_transform=eval_transform)\n\u001b[1;32m    245\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'predict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# or stage is None:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m             self.init_dataset_stage(stage='predict',\n\u001b[0m\u001b[1;32m    247\u001b[0m                                     eval_transform=eval_transform)\n\u001b[1;32m    248\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/jacob/GitHub/prj_fossils_contrastive/contrastive_learning/data/pytorch/common.py\u001b[0m in \u001b[0;36minit_dataset_stage\u001b[0;34m(self, stage, train_transform, eval_transform)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mreturn_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/jacob/GitHub/prj_fossils_contrastive/contrastive_learning/data/pytorch/common.py\u001b[0m in \u001b[0;36mget_dataset_split\u001b[0;34m(self, split)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"predict\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             predict_dataset = self.DatasetConstructor(self.name,\n\u001b[0m\u001b[1;32m    315\u001b[0m                                                    \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                                                    \u001b[0mdataset_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/jacob/GitHub/prj_fossils_contrastive/contrastive_learning/data/pytorch/pnas.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, split, dataset_dir, return_paths, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                  \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                  ) -> None:\n\u001b[0;32m---> 67\u001b[0;31m         super().__init__(name,\n\u001b[0m\u001b[1;32m     68\u001b[0m                          \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                          \u001b[0mdataset_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/jacob/GitHub/prj_fossils_contrastive/contrastive_learning/data/pytorch/common.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, split, dataset_dir, return_paths, data_df, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         super().__init__(root=self.split_dir,\n\u001b[0m\u001b[1;32m     87\u001b[0m                          **kwargs)\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     ):\n\u001b[0;32m--> 226\u001b[0;31m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS if is_valid_file is None else None,\n\u001b[0m\u001b[1;32m    227\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    106\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[1;32m    107\u001b[0m                                             target_transform=target_transform)\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[0;34m(self, dir)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mNo\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0msubdirectory\u001b[0m \u001b[0mof\u001b[0m \u001b[0manother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/val'"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "##########################################\n",
    "\n",
    "fix_catalog_number = \"PNAS\" in config.dataset.name\n",
    "trainer = configure_trainer(config) #, limit_test_batches=2)\n",
    "\n",
    "test_results = test_model(trainer,\n",
    "                          model,\n",
    "                          output_model_artifact,\n",
    "                          datamodule,\n",
    "                          config,\n",
    "                          fix_catalog_number=fix_catalog_number)\n",
    "\n",
    "\n",
    "\n",
    "wandb.finish()\n",
    "print(trainer.current_epoch, test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528cfc51",
   "metadata": {
    "tags": []
   },
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90ba448",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b485843e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Left to do 5-16-2021\n",
    "\n",
    "1. trainer.fit ***\n",
    "2. trainer.test ***\n",
    "\n",
    "3. Refactor ValLog Callback to log PyTorch Lightning predictions + per-class scores *****\n",
    "\n",
    "4. Add data tests for WandB downloaded data \\*\\*\\*\n",
    "\n",
    "    4a. Implement Data Drift framework!!!\n",
    "    \n",
    "5. Add Salient Map interpretability callback \\*\\*\\*\n",
    "    * https://github.com/MisaOgura/flashtorch\n",
    "    \n",
    "priority: \\*, \\*\\*,\\*\\*\\*, ****, *****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d062d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "\n",
    "def test_pytorch_gpu_empty_cache(num_GB_2_gpu=10):\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()\n",
    "    tensorList = []\n",
    "    for x in range(10):\n",
    "        tensorList.append(torch.randn(int(10e6), num_GB_2_gpu).cuda())   # reduce the size of tensor if you are getting OOM\n",
    "        \n",
    "        gpu_usage()\n",
    "\n",
    "    print(\"GPU Usage after allocating a bunch of Tensors\")\n",
    "    gpu_usage()\n",
    "    del tensorList\n",
    "    print(\"GPU Usage after deleting the Tensors\")\n",
    "    gpu_usage()\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    torch.cuda.empty_cache()\n",
    "    gpu_usage()\n",
    "    \n",
    "    \n",
    "    \n",
    "test_pytorch_gpu_empty_cache(12)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1110169d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c9ab91f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ff29e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aeb3622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# from tqdm.notebook import trange, tqdm\n",
    "# def get_labels_from_filepath(path: str, fix_catalog_number: bool = False):\n",
    "#     \"\"\"\n",
    "    \n",
    "#     \"\"\"\n",
    "#     family, genus, species, collection, catalog_number = Path(path).stem.split(\"_\", maxsplit=4)\n",
    "#     if fix_catalog_number:\n",
    "#         catalog_number = '_'.join([collection, catalog_number])\n",
    "#     return {\"family\":family,\n",
    "#             \"genus\":genus,\n",
    "#             \"species\":species,\n",
    "#             \"collection\":collection,\n",
    "#             \"catalog_number\":catalog_number}\n",
    "\n",
    "# # construct a table containing our dataset\n",
    "# def register_raw_dataset(dataset: torch.utils.data.Dataset,\n",
    "#                          artifact,\n",
    "#                          subset: str,\n",
    "#                          fix_catalog_number: bool=False):\n",
    "#     print(f\"Registering {len(dataset.samples)} samples in {subset} subset\")\n",
    "#     table = wandb.Table(['image',\n",
    "#                          'label', \n",
    "#                          \"family\",\n",
    "#                          \"genus\",\n",
    "#                          \"species\",\n",
    "#                          \"collection\",\n",
    "#                          \"catalog_number\"])\n",
    "#     for sample in tqdm(dataset.samples):\n",
    "#         path, label = sample        \n",
    "#         metadata = get_labels_from_filepath(path, fix_catalog_number=fix_catalog_number)\n",
    "#         rel_path = f\"{subset}/{metadata['family']}/{Path(path).name}\"\n",
    "        \n",
    "#         artifact.add_file(path, rel_path)\n",
    "        \n",
    "#         table.add_data(wandb.Image(path), label, *list(metadata.values()))\n",
    "        \n",
    "        \n",
    "#     if hasattr(dataset, 'image_size'):\n",
    "#         if isinstance(dataset.image_size, int):\n",
    "#             artifact.metadata['image_size'] = dataset.image_size\n",
    "        \n",
    "        \n",
    "#     artifact.metadata['num_samples'] = len(dataset.samples)\n",
    "#     artifact.add(table, f'tables/{subset}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d577ccdb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2: Download data split artifact for input to training run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f955e528",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. Access previously logged data artifact from Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91af39d8",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69166a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "TITAN X (Pascal)\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'trainer'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pytorch_lightning.Trainer'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'gpus'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'min_epochs'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'max_epochs'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'weights_summary'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'top'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'progress_bar_refresh_rate'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'profiler'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'simple'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'log_every_n_steps'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'terminate_on_nan'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'auto_scale_batch_size'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'auto_lr_find'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'auto_lr_num_samples'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'lightning_hydra_classifiers.models.resnet.ResNet'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'resnet50'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'num_classes'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'input_shape'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">224</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">224</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'optimizer'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'torch.optim.Adam'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Adam'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'lr'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.001</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'weight_decay'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'head'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'avg_pool'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'flatten'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'fc'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'unfreeze'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'layer4'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'root_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/models/resnet50'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_path'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/models/resnet50/best_model.pkl'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'datamodule'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'contrastive_learning.data.pytorch.extant.ExtantLightningDataModule'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Extant_family_10_512'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'val_split'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'image_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'channels'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'class_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'family'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'normalize'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'seed'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">98568764</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'predict_on_split'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'val'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'debug'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'num_workers'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'root_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'dataset_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/datasets/Extant_family_10_512'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'callbacks'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_checkpoint'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pytorch_lightning.callbacks.ModelCheckpoint'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'monitor'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'val_loss'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'save_top_k'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'save_last'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'mode'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'min'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'verbose'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'dirpath'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'checkpoints/'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'filename'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'{epoch:02d}'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'early_stopping'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pytorch_lightning.callbacks.early_stopping.EarlyStopping'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'monitor'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'val_loss'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'patience'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'verbose'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'mode'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'min'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'log_per_class_metrics_to_wandb'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'lightning_hydra_classifiers.callbacks.wandb_callbacks.LogPerClassMetricsToWandb'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'module_data_monitor'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pl_bolts.callbacks.ModuleDataMonitor'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'logger'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'csv'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pytorch_lightning.loggers.csv_logs.CSVLogger'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'save_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'csv/'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'wandb'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pytorch_lightning.loggers.wandb.WandbLogger'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'entity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'jrose'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'image_classification'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'job_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'train'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'group'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stage_0'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'wandb'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'0'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'init'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'entity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'jrose'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'image_classification_train'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'job_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'lr_tune_init'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'group'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'tune'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'run_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'tags'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'Extant_family_10_512'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'resnet50'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'lr_tune'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'init'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'auto_lr_num_samples=100'</span>\n",
       "                <span style=\"font-weight: bold\">]</span>\n",
       "            <span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'init'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'entity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'jrose'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'image_classification_train'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'job_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'batch_size_tune_init'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'group'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'tune'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'run_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'tags'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Extant_family_10_512'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'batch_size_tune'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'init'</span><span style=\"font-weight: bold\">]</span>\n",
       "            <span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'init'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'entity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'jrose'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'image_classification_train'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'job_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'train_supervised'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'group'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'run_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'tags'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Extant_family_10_512'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'resnet50'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'train'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'supervised'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'init'</span><span style=\"font-weight: bold\">]</span>\n",
       "            <span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'3'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'init'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'entity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'jrose'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'image_classification_train'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'job_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'finetune_supervised_init'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'group'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'run_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'tags'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'Extant_family_10_512'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'resnet50'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'finetune'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'supervised'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'init'</span>\n",
       "                <span style=\"font-weight: bold\">]</span>\n",
       "            <span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'artifacts'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'input_dataset_artifact'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'basename'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Extant_family_10_512'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Extant_family_10_512'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'image_classification_datasets'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'entity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'jrose'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'version'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'latest'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'raw_data'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'artifact_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'InputArtifact'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'root_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/artifacts'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'uri'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'jrose/image_classification_datasets/Extant_family_10_512:latest'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'input_model_artifact'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'basename'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'resnet50_imagenet'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'resnet50_imagenet_init_model'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'image_classification_datasets'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'entity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'jrose'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'version'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'latest'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'artifact_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'InputModelArtifact'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'root_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/artifacts'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model_stage'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'init'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'init_model'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/artifacts/models'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model_path'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/artifacts/models/init/best_model.ckpt'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'datasets_seen'</span>: <span style=\"font-weight: bold\">[]</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'output_model_artifact'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'basename'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'resnet50_Extant_family_10_512'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'resnet50_Extant_family_10_512_train_model'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'image_classification_datasets'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'entity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'jrose'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'version'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'latest'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'artifact_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'OutputModelArtifact'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model_stage'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'train'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'train_model'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'root_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/artifacts'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/artifacts/models'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model_path'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache/artifacts/models/train/best_model.ckpt'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'datasets_seen'</span>: <span style=\"font-weight: bold\">[]</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'root_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/media/data/jacob/wandb_cache'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'debug'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'print_config'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'disable_warnings'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'trainer'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'_target_'\u001b[0m: \u001b[32m'pytorch_lightning.Trainer'\u001b[0m,\n",
       "        \u001b[32m'gpus'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "        \u001b[32m'min_epochs'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "        \u001b[32m'max_epochs'\u001b[0m: \u001b[1;36m40\u001b[0m,\n",
       "        \u001b[32m'weights_summary'\u001b[0m: \u001b[32m'top'\u001b[0m,\n",
       "        \u001b[32m'progress_bar_refresh_rate'\u001b[0m: \u001b[1;36m10\u001b[0m,\n",
       "        \u001b[32m'profiler'\u001b[0m: \u001b[32m'simple'\u001b[0m,\n",
       "        \u001b[32m'log_every_n_steps'\u001b[0m: \u001b[1;36m50\u001b[0m,\n",
       "        \u001b[32m'terminate_on_nan'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[32m'auto_scale_batch_size'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[32m'auto_lr_find'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[32m'auto_lr_num_samples'\u001b[0m: \u001b[1;36m100\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'model'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'_target_'\u001b[0m: \u001b[32m'lightning_hydra_classifiers.models.resnet.ResNet'\u001b[0m,\n",
       "        \u001b[32m'name'\u001b[0m: \u001b[32m'resnet50'\u001b[0m,\n",
       "        \u001b[32m'num_classes'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[32m'input_shape'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m224\u001b[0m, \u001b[1;36m224\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[32m'optimizer'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'_target_'\u001b[0m: \u001b[32m'torch.optim.Adam'\u001b[0m,\n",
       "            \u001b[32m'name'\u001b[0m: \u001b[32m'Adam'\u001b[0m,\n",
       "            \u001b[32m'lr'\u001b[0m: \u001b[1;36m0.001\u001b[0m,\n",
       "            \u001b[32m'weight_decay'\u001b[0m: \u001b[1;36m0.0\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'head'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'avg_pool'\u001b[0m, \u001b[32m'flatten'\u001b[0m, \u001b[32m'fc'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[32m'unfreeze'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'layer4'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[32m'root_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache'\u001b[0m,\n",
       "        \u001b[32m'model_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache/models/resnet50'\u001b[0m,\n",
       "        \u001b[32m'model_path'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache/models/resnet50/best_model.pkl'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'datamodule'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'_target_'\u001b[0m: \u001b[32m'contrastive_learning.data.pytorch.extant.ExtantLightningDataModule'\u001b[0m,\n",
       "        \u001b[32m'name'\u001b[0m: \u001b[32m'Extant_family_10_512'\u001b[0m,\n",
       "        \u001b[32m'batch_size'\u001b[0m: \u001b[1;36m64\u001b[0m,\n",
       "        \u001b[32m'val_split'\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "        \u001b[32m'image_size'\u001b[0m: \u001b[1;36m512\u001b[0m,\n",
       "        \u001b[32m'channels'\u001b[0m: \u001b[1;36m3\u001b[0m,\n",
       "        \u001b[32m'class_type'\u001b[0m: \u001b[32m'family'\u001b[0m,\n",
       "        \u001b[32m'normalize'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "        \u001b[32m'seed'\u001b[0m: \u001b[1;36m98568764\u001b[0m,\n",
       "        \u001b[32m'predict_on_split'\u001b[0m: \u001b[32m'val'\u001b[0m,\n",
       "        \u001b[32m'debug'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[32m'num_workers'\u001b[0m: \u001b[1;36m4\u001b[0m,\n",
       "        \u001b[32m'root_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache'\u001b[0m,\n",
       "        \u001b[32m'dataset_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache/datasets/Extant_family_10_512'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'callbacks'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'model_checkpoint'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'_target_'\u001b[0m: \u001b[32m'pytorch_lightning.callbacks.ModelCheckpoint'\u001b[0m,\n",
       "            \u001b[32m'monitor'\u001b[0m: \u001b[32m'val_loss'\u001b[0m,\n",
       "            \u001b[32m'save_top_k'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "            \u001b[32m'save_last'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'mode'\u001b[0m: \u001b[32m'min'\u001b[0m,\n",
       "            \u001b[32m'verbose'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "            \u001b[32m'dirpath'\u001b[0m: \u001b[32m'checkpoints/'\u001b[0m,\n",
       "            \u001b[32m'filename'\u001b[0m: \u001b[32m'{epoch:02d}'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'early_stopping'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'_target_'\u001b[0m: \u001b[32m'pytorch_lightning.callbacks.early_stopping.EarlyStopping'\u001b[0m,\n",
       "            \u001b[32m'monitor'\u001b[0m: \u001b[32m'val_loss'\u001b[0m,\n",
       "            \u001b[32m'patience'\u001b[0m: \u001b[1;36m3\u001b[0m,\n",
       "            \u001b[32m'verbose'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "            \u001b[32m'mode'\u001b[0m: \u001b[32m'min'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'log_per_class_metrics_to_wandb'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'_target_'\u001b[0m: \n",
       "\u001b[32m'lightning_hydra_classifiers.callbacks.wandb_callbacks.LogPerClassMetricsToWandb'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'module_data_monitor'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \u001b[32m'pl_bolts.callbacks.ModuleDataMonitor'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'logger'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'csv'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'_target_'\u001b[0m: \u001b[32m'pytorch_lightning.loggers.csv_logs.CSVLogger'\u001b[0m,\n",
       "            \u001b[32m'save_dir'\u001b[0m: \u001b[32m'.'\u001b[0m,\n",
       "            \u001b[32m'name'\u001b[0m: \u001b[32m'csv/'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'wandb'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'_target_'\u001b[0m: \u001b[32m'pytorch_lightning.loggers.wandb.WandbLogger'\u001b[0m,\n",
       "            \u001b[32m'entity'\u001b[0m: \u001b[32m'jrose'\u001b[0m,\n",
       "            \u001b[32m'project'\u001b[0m: \u001b[32m'image_classification'\u001b[0m,\n",
       "            \u001b[32m'job_type'\u001b[0m: \u001b[32m'train'\u001b[0m,\n",
       "            \u001b[32m'group'\u001b[0m: \u001b[32m'stage_0'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'wandb'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'0'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'init'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'entity'\u001b[0m: \u001b[32m'jrose'\u001b[0m,\n",
       "                \u001b[32m'project'\u001b[0m: \u001b[32m'image_classification_train'\u001b[0m,\n",
       "                \u001b[32m'job_type'\u001b[0m: \u001b[32m'lr_tune_init'\u001b[0m,\n",
       "                \u001b[32m'group'\u001b[0m: \u001b[32m'tune'\u001b[0m,\n",
       "                \u001b[32m'run_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache'\u001b[0m,\n",
       "                \u001b[32m'tags'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "                    \u001b[32m'Extant_family_10_512'\u001b[0m,\n",
       "                    \u001b[32m'resnet50'\u001b[0m,\n",
       "                    \u001b[32m'lr_tune'\u001b[0m,\n",
       "                    \u001b[32m'init'\u001b[0m,\n",
       "                    \u001b[32m'\u001b[0m\u001b[32mauto_lr_num_samples\u001b[0m\u001b[32m=\u001b[0m\u001b[32m100\u001b[0m\u001b[32m'\u001b[0m\n",
       "                \u001b[1m]\u001b[0m\n",
       "            \u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'1'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'init'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'entity'\u001b[0m: \u001b[32m'jrose'\u001b[0m,\n",
       "                \u001b[32m'project'\u001b[0m: \u001b[32m'image_classification_train'\u001b[0m,\n",
       "                \u001b[32m'job_type'\u001b[0m: \u001b[32m'batch_size_tune_init'\u001b[0m,\n",
       "                \u001b[32m'group'\u001b[0m: \u001b[32m'tune'\u001b[0m,\n",
       "                \u001b[32m'run_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache'\u001b[0m,\n",
       "                \u001b[32m'tags'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'Extant_family_10_512'\u001b[0m, \u001b[32m'batch_size_tune'\u001b[0m, \u001b[32m'init'\u001b[0m\u001b[1m]\u001b[0m\n",
       "            \u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'2'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'init'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'entity'\u001b[0m: \u001b[32m'jrose'\u001b[0m,\n",
       "                \u001b[32m'project'\u001b[0m: \u001b[32m'image_classification_train'\u001b[0m,\n",
       "                \u001b[32m'job_type'\u001b[0m: \u001b[32m'train_supervised'\u001b[0m,\n",
       "                \u001b[32m'group'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[32m'run_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache'\u001b[0m,\n",
       "                \u001b[32m'tags'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'Extant_family_10_512'\u001b[0m, \u001b[32m'resnet50'\u001b[0m, \u001b[32m'train'\u001b[0m, \u001b[32m'supervised'\u001b[0m, \u001b[32m'init'\u001b[0m\u001b[1m]\u001b[0m\n",
       "            \u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'3'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'init'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'entity'\u001b[0m: \u001b[32m'jrose'\u001b[0m,\n",
       "                \u001b[32m'project'\u001b[0m: \u001b[32m'image_classification_train'\u001b[0m,\n",
       "                \u001b[32m'job_type'\u001b[0m: \u001b[32m'finetune_supervised_init'\u001b[0m,\n",
       "                \u001b[32m'group'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[32m'run_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache'\u001b[0m,\n",
       "                \u001b[32m'tags'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "                    \u001b[32m'Extant_family_10_512'\u001b[0m,\n",
       "                    \u001b[32m'resnet50'\u001b[0m,\n",
       "                    \u001b[32m'finetune'\u001b[0m,\n",
       "                    \u001b[32m'supervised'\u001b[0m,\n",
       "                    \u001b[32m'init'\u001b[0m\n",
       "                \u001b[1m]\u001b[0m\n",
       "            \u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'artifacts'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'input_dataset_artifact'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'basename'\u001b[0m: \u001b[32m'Extant_family_10_512'\u001b[0m,\n",
       "            \u001b[32m'name'\u001b[0m: \u001b[32m'Extant_family_10_512'\u001b[0m,\n",
       "            \u001b[32m'project'\u001b[0m: \u001b[32m'image_classification_datasets'\u001b[0m,\n",
       "            \u001b[32m'entity'\u001b[0m: \u001b[32m'jrose'\u001b[0m,\n",
       "            \u001b[32m'version'\u001b[0m: \u001b[32m'latest'\u001b[0m,\n",
       "            \u001b[32m'type'\u001b[0m: \u001b[32m'raw_data'\u001b[0m,\n",
       "            \u001b[32m'artifact_type'\u001b[0m: \u001b[32m'InputArtifact'\u001b[0m,\n",
       "            \u001b[32m'description'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[32m'root_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache/artifacts'\u001b[0m,\n",
       "            \u001b[32m'uri'\u001b[0m: \u001b[32m'jrose/image_classification_datasets/Extant_family_10_512:latest'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'input_model_artifact'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'basename'\u001b[0m: \u001b[32m'resnet50_imagenet'\u001b[0m,\n",
       "            \u001b[32m'name'\u001b[0m: \u001b[32m'resnet50_imagenet_init_model'\u001b[0m,\n",
       "            \u001b[32m'project'\u001b[0m: \u001b[32m'image_classification_datasets'\u001b[0m,\n",
       "            \u001b[32m'entity'\u001b[0m: \u001b[32m'jrose'\u001b[0m,\n",
       "            \u001b[32m'version'\u001b[0m: \u001b[32m'latest'\u001b[0m,\n",
       "            \u001b[32m'artifact_type'\u001b[0m: \u001b[32m'InputModelArtifact'\u001b[0m,\n",
       "            \u001b[32m'description'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[32m'root_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache/artifacts'\u001b[0m,\n",
       "            \u001b[32m'model_stage'\u001b[0m: \u001b[32m'init'\u001b[0m,\n",
       "            \u001b[32m'type'\u001b[0m: \u001b[32m'init_model'\u001b[0m,\n",
       "            \u001b[32m'model_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache/artifacts/models'\u001b[0m,\n",
       "            \u001b[32m'model_path'\u001b[0m: \n",
       "\u001b[32m'/media/data/jacob/wandb_cache/artifacts/models/init/best_model.ckpt'\u001b[0m,\n",
       "            \u001b[32m'datasets_seen'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'output_model_artifact'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'basename'\u001b[0m: \u001b[32m'resnet50_Extant_family_10_512'\u001b[0m,\n",
       "            \u001b[32m'name'\u001b[0m: \u001b[32m'resnet50_Extant_family_10_512_train_model'\u001b[0m,\n",
       "            \u001b[32m'project'\u001b[0m: \u001b[32m'image_classification_datasets'\u001b[0m,\n",
       "            \u001b[32m'entity'\u001b[0m: \u001b[32m'jrose'\u001b[0m,\n",
       "            \u001b[32m'version'\u001b[0m: \u001b[32m'latest'\u001b[0m,\n",
       "            \u001b[32m'artifact_type'\u001b[0m: \u001b[32m'OutputModelArtifact'\u001b[0m,\n",
       "            \u001b[32m'description'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[32m'model_stage'\u001b[0m: \u001b[32m'train'\u001b[0m,\n",
       "            \u001b[32m'type'\u001b[0m: \u001b[32m'train_model'\u001b[0m,\n",
       "            \u001b[32m'root_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache/artifacts'\u001b[0m,\n",
       "            \u001b[32m'model_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache/artifacts/models'\u001b[0m,\n",
       "            \u001b[32m'model_path'\u001b[0m: \n",
       "\u001b[32m'/media/data/jacob/wandb_cache/artifacts/models/train/best_model.ckpt'\u001b[0m,\n",
       "            \u001b[32m'datasets_seen'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'root_dir'\u001b[0m: \u001b[32m'/media/data/jacob/wandb_cache'\u001b[0m,\n",
       "    \u001b[32m'debug'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "    \u001b[32m'print_config'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[32m'disable_warnings'\u001b[0m: \u001b[3;92mTrue\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "TITAN X (Pascal)\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# from pathlib import Path\n",
    "# import os\n",
    "# import wandb\n",
    "# os.environ['WANDB_CACHE_DIR'] = \"/media/data/jacob/wandb_cache\"\n",
    "# os.makedirs(\"/media/data/jacob/wandb_cache\", exist_ok=True)\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"2\"\n",
    "# import numpy as np\n",
    "# import pytorch_lightning as pl\n",
    "# import torchvision\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# import matplotlib.pyplot as plt\n",
    "# from contrastive_learning.data.pytorch.tensor import tensor_nbytes\n",
    "# from contrastive_learning.data.pytorch.pnas import PNASLightningDataModule\n",
    "# from contrastive_learning.data.pytorch.extant import ExtantLightningDataModule\n",
    "# from contrastive_learning.data.pytorch.common import DataStageError, colorbar, LeavesLightningDataModule\n",
    "# from contrastive_learning.data.pytorch.datamodules import get_datamodule, fetch_datamodule_from_dataset_artifact\n",
    "\n",
    "# from lightning_hydra_classifiers.callbacks.wandb_callbacks import LogPerClassMetricsToWandb, WandbClassificationCallback\n",
    "# from lightning_hydra_classifiers.models.resnet import ResNet\n",
    "# from lightning_hydra_classifiers.utils.train_basic_utils import build_and_log_model_to_artifact\n",
    "\n",
    "\n",
    "# import lightning_hydra_classifiers\n",
    "# from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "# from pl_bolts.callbacks import ModuleDataMonitor, BatchGradientVerificationCallback\n",
    "\n",
    "# import inspect\n",
    "# pl.trainer.seed_everything(seed=389)\n",
    "\n",
    "# from stuf import stuf\n",
    "# from box import Box\n",
    "# from rich import print as pp\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print('Using device:', device)\n",
    "# print()\n",
    "\n",
    "# #Additional Info when using cuda\n",
    "# if device.type == 'cuda':\n",
    "#     print(torch.cuda.get_device_name(0))\n",
    "#     print('Memory Usage:')\n",
    "#     print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "#     print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# def efficient_zero_grad(model):\n",
    "#     \"\"\"\n",
    "#     [source] https://ai.plainenglish.io/best-performance-tuning-practices-for-pytorch-3ef06329d5fe\n",
    "#     \"\"\"\n",
    "#     for param in model.parameters():\n",
    "#         param.grad = None\n",
    "\n",
    "# # image_size = 512\n",
    "# # class_type = 'family'\n",
    "# # thresholds = [100]\n",
    "# # seed = 894\n",
    "# # config = Box()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff4f97e",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Dict Config commented out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dae5c0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config.datasets = [{\n",
    "#                   \"name\": f\"PNAS_{class_type}_{thresholds[0]}_{image_size}\",\n",
    "#                   \"batch_size\":512,\n",
    "#                   \"val_split\":None, # TODO specify split explicitly in wandb report\n",
    "#                   \"num_workers\":4,\n",
    "#                   \"image_size\":image_size,\n",
    "#                   \"channels\":3,\n",
    "#                   \"class_type\":class_type,\n",
    "#                   \"debug\":False,\n",
    "#                   \"normalize\":True,\n",
    "#                   \"seed\":seed,\n",
    "#                   \"dataset_dir\":None,\n",
    "#                   \"predict_on_split\":\"val\",\n",
    "#                   },\n",
    "#     {\n",
    "#                   \"name\":f\"Extant_{class_type}_{extant_threshold}_{image_size}\",  # f\"PNAS_{label_type}_{pnas_threshold}_{image_size}\"\n",
    "#                   \"batch_size\":32,\n",
    "#                   \"val_split\":None, # TODO specify split explicitly in wandb report\n",
    "#                   \"num_workers\":4,\n",
    "#                   \"image_size\":image_size,\n",
    "#                   \"channels\":3,\n",
    "#                   \"class_type\":class_type,\n",
    "#                   \"debug\":False,\n",
    "#                   \"normalize\":True,\n",
    "#                   \"seed\":seed,\n",
    "#                   \"dataset_dir\":None,\n",
    "#                   \"predict_on_split\":\"val\",\n",
    "#                   }]\n",
    "\n",
    "# config.model = {\n",
    "#                 \"name\":\"resnet50\",\n",
    "#                 \"num_classes\":None,\n",
    "#                 \"input_shape\": (config.dataset.channels, config.dataset.image_size, config.dataset.image_size),\n",
    "#                 \"optimizer\":{'name':\"Adam\",\n",
    "#                              'lr':0.01},\n",
    "#                 \"head\":[\"avg_pool\",\"flatten\",\"fc\"],\n",
    "#                 \"unfreeze\":[\"layer4\"],\n",
    "#                 \"checkpoint_dir\":None\n",
    "#                 }\n",
    "\n",
    "\n",
    "\n",
    "# config.wandb = {\n",
    "#                 \"init\":\n",
    "#                        {},\n",
    "#                 \"artifacts\":\n",
    "#                        {\"root_dir\":None},\n",
    "#                 \"input_artifacts\":\n",
    "#                        [],\n",
    "#                 \"output_artifacts\":\n",
    "#                        []\n",
    "#                 }\n",
    "\n",
    "\n",
    "# config.wandb.init = {\n",
    "#                      \"entity\":\"jrose\",\n",
    "#                      \"project\":\"image_classification_datasets\",\n",
    "# #                      \"job_type\":'create-dataset',\n",
    "#                      \"group\":None,\n",
    "#                      \"run_dir\":os.environ['WANDB_CACHE_DIR'],\n",
    "#                      \"tags\":[d.name for d in config.datasets]\n",
    "# }\n",
    "\n",
    "# config.wandb.artifacts.root_dir = str(Path(config.wandb.init.run_dir, 'artifacts'))\n",
    "# config.wandb.input_artifacts.append(\n",
    "#                                     {\n",
    "#                                     \"tags\":[\"input_dataset\"],\n",
    "#                                     \"entity\":\"jrose\",\n",
    "#                                     \"project\":\"image_classification_datasets\",\n",
    "#                                     \"name\": config.datasets[0].name,\n",
    "#                                     \"version\": \"v6\",\n",
    "#                                     \"type\": \"raw_data\",\n",
    "#                                     \"uri\":None,\n",
    "#                                     \"root_dir\":str(Path(config.wandb.artifacts.root_dir,\n",
    "#                                                         \"inputs\", \"dataset\",\n",
    "#                                                         config.datasets[0].name))\n",
    "#                                     },\n",
    "#                                     {\n",
    "#                                     \"tags\":[\"input_model\"],\n",
    "#                                     \"basename\": config.model.name,\n",
    "#                                     \"name\": config.model.name + \"_init\"\n",
    "#                                     \"version\":\"latest\",\n",
    "#                                     \"type\":\"imagenet-init-model\",\n",
    "#                                     \"description\":f\"imagenet-pretrained ResNet50 base model -> avg_pool -> randomly initiated 1x fc output layer w/ size=num_classes\",\n",
    "#                                     \"root_dir\":None \n",
    "#                                     }\n",
    "#                                     )\n",
    "        \n",
    "# config.wandb.output_artifacts.append({\n",
    "#                                     \"root_dir\":str(Path(config.wandb.artifacts.root_dir,\n",
    "#                                                         \"outputs\", \"model\",\n",
    "#                                                         config.datasets[0].name)),\n",
    "#                                       \"output_model_dir\": str(Path(config.wandb.model_artifact.root_dir,\n",
    "#                                                           f\"{config.dataset.name}_trained\")),\n",
    "#                                     output_model_path = os.path.join(config.output_artifacts.output_model_dir, \"best_model.pt\")\n",
    "#                                     output_name = config.output_artifacts.basename + \"_output\"\n",
    "\n",
    "\n",
    "# config.dataset.dataset_dir = config.wandb.input_artifact.root_dir\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a26c7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataclass Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cda1730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, asdict, astuple, InitVar\n",
    "from typing import Tuple, List, Dict, Any, Type, Union\n",
    "from enum import Enum\n",
    "from pprint import pprint as pp\n",
    "\n",
    "class ArtifactType(Enum):\n",
    "    InputArtifact = \"in\"\n",
    "    OutputArtifact = \"out\"\n",
    "    \n",
    "# TBD: Integrate this Enum if it seems worthwhile\n",
    "# class JobType(Enum):\n",
    "#     create_dataset = \"create_dataset\"\n",
    "#     train = \"train\"\n",
    "#     test = \"test\"\n",
    "    \n",
    "class ModelStage(Enum):\n",
    "    random_init = \"rand_init\"\n",
    "    imagenet_init = \"init\"\n",
    "    train = \"train\"\n",
    "    feature_extractor = \"feature extractor\"\n",
    "    finetune = \"finetune\"\n",
    "    continual = \"continual\"\n",
    "\n",
    "import typing\n",
    "    \n",
    "    \n",
    "class TypeEnum(Enum):\n",
    "#     typing.Tuple = tuple\n",
    "    List = list\n",
    "    Dict = dict\n",
    "    Any = (lambda x: x)\n",
    "    \n",
    "    @classmethod\n",
    "    def is_builtin(cls, key: Union[str,Type]):\n",
    "#         if str(key).split('.')[-1] in cls.__members__:\n",
    "        try:\n",
    "            cls.parse(key)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, key: Union[str,Type]):\n",
    "        parsed_key = str(key).split('.')[-1].split('[')[0]\n",
    "        return cls.__members__[parsed_key]\n",
    "\n",
    "    \n",
    "    \n",
    "@dataclass\n",
    "class BaseConfig:\n",
    "\n",
    "    def __post_init__(self):\n",
    "        for k,v in self.__dict__.items():\n",
    "            if isinstance(v, inspect.Parameter):\n",
    "                self.__dict__.update({k:v.default})\n",
    "    \n",
    "    def asdict(self):\n",
    "        return asdict(self)\n",
    "    \n",
    "    def astuple(self):\n",
    "        return astuple(self)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, config_dict: Dict[str,Any]):\n",
    "        input_kwargs = config_dict # .asdict()\n",
    "        sig = inspect.signature(cls) # SimpleExperimentConfig)\n",
    "\n",
    "        signature_kwargs = dict(sig.parameters)\n",
    "\n",
    "        for key in list(signature_kwargs.keys()):\n",
    "            if key in input_kwargs:\n",
    "                type_init = signature_kwargs[key].annotation\n",
    "                if hasattr(input_kwargs[key], \"asdict\"):\n",
    "                    input_kwargs[key] = input_kwargs[key].asdict()\n",
    "                if TypeEnum.is_builtin(type_init):\n",
    "                    type_init = TypeEnum.parse(type_init)\n",
    "#                 if isinstance(input_kwargs[key], ArtifactType):\n",
    "                if isinstance(input_kwargs[key], str):\n",
    "                    if hasattr(ArtifactType, input_kwargs[key]):\n",
    "                        signature_kwargs[key] = getattr(ArtifactType, input_kwargs[key])\n",
    "                elif hasattr(type_init,'from_dict'):\n",
    "                    signature_kwargs[key] = type_init.from_dict(input_kwargs[key])\n",
    "                elif isinstance(input_kwargs[key], dict):\n",
    "                    signature_kwargs[key] = type_init(**input_kwargs[key])\n",
    "                elif input_kwargs[key] == None:\n",
    "                    signature_kwargs[key] = input_kwargs[key]\n",
    "                else:\n",
    "                    signature_kwargs[key] = type_init(input_kwargs[key])\n",
    "        return cls(**signature_kwargs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OptimizerConfig(BaseConfig):\n",
    "    name: str = \"Adam\"\n",
    "    lr: float = 0.01\n",
    "    weight_decay: float = 0.0\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig(BaseConfig):\n",
    "\n",
    "    name: str = \"resnet50\"\n",
    "    num_classes: int = 0\n",
    "    input_shape: tuple = (3,None,None)\n",
    "    optimizer: OptimizerConfig = field(default_factory=OptimizerConfig)\n",
    "    head: list = None # field(default_factory=lambda : [\"avg_pool\",\"flatten\",\"fc\"])\n",
    "    unfreeze: list = (\"layer4\",) # field(default_factory=lambda : [\"layer4\"])\n",
    "    root_dir: str = None #field(default=None)\n",
    "    model_dir: str = field(default=None) #, init=False)\n",
    "    model_path: str = field(default=None) #, init=False)\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        super().__post_init__()\n",
    "        if self.root_dir is None or not isinstance(self.root_dir, str):\n",
    "            self.root_dir = os.path.join(os.environ[\"WANDB_CACHE_DIR\"],'artifacts')\n",
    "        \n",
    "        if self.head is None:\n",
    "            self.head = [\"avg_pool\",\"flatten\",\"fc\"]\n",
    "            \n",
    "\n",
    "\n",
    "        self.model_dir = os.path.join(self.root_dir, 'models', self.name) #, self.model_stage.value)\n",
    "        self.model_path = os.path.join(self.model_dir, \"best_model.ckpt\")\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "@dataclass\n",
    "class DatasetConfig(BaseConfig):\n",
    "\n",
    "    name: str = \"Extant_family_10_512\" # field(default=\"Extant_family_10_512\")\n",
    "    batch_size: int = 64\n",
    "    val_split: float = field(default=0.0) # TODO specify split explicitly in wandb report\n",
    "    image_size: int = 512\n",
    "    channels: int = 3\n",
    "    class_type: str = \"family\"\n",
    "    normalize: bool = True\n",
    "    seed: int = 253\n",
    "    predict_on_split: str = field(default=\"val\")\n",
    "    debug: bool = False\n",
    "    num_workers: int = 4\n",
    "    root_dir: str = None\n",
    "#     dataset_dir: str = field(init=False) #default=None)\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        super().__post_init__()\n",
    "        if not isinstance(self.name, str) and not self.name == \"None\":\n",
    "            self.name = str(self.name)\n",
    "        if self.root_dir is None or not isinstance(self.root_dir, str):\n",
    "            self.root_dir = os.path.join(os.environ[\"WANDB_CACHE_DIR\"],'artifacts')\n",
    "        self.dataset_dir = os.path.join(self.root_dir, 'datasets', self.name)\n",
    "\n",
    "\n",
    "# opt_cfg = OptimizerConfig(lr=1e-4)\n",
    "# model_cfg = ModelConfig(optimizer=opt_cfg, unfreeze=\"layer3\")#[\"layer3\", \"layer4\"])\n",
    "\n",
    "        \n",
    "@dataclass\n",
    "class ArtifactConfig(BaseConfig):\n",
    "    basename: str = None\n",
    "    name: str = None\n",
    "    project: str = None\n",
    "    entity: str = None\n",
    "    version: str = 'latest'\n",
    "#     uri: str = field(default=\"\", init=False)\n",
    "#     model_type: str = field(default=None)#\"imagenet-init-model\")\n",
    "    artifact_type: ArtifactType = None #field(default=None)\n",
    "    description: str = None #field(default=None)\n",
    "    root_dir: str = None # field(default=None)\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        super().__post_init__()\n",
    "        if self.root_dir is None:\n",
    "            self.root_dir = os.path.join(os.environ[\"WANDB_CACHE_DIR\"],'artifacts')#, 'models', self.basename)\n",
    "        if self.basename is None and self.name is not None:\n",
    "            self.basename = self.name.split(\"_\", maxsplit=1)\n",
    "        elif self.name is None and self.basename is not None:\n",
    "            self.name = self.basename\n",
    "        else:\n",
    "            self.name = f\"{type(self)}_default\"\n",
    "            self.basename = f\"{type(self)}\"\n",
    "\n",
    "        self.uri = self.get_uri()\n",
    "        \n",
    "            \n",
    "    def get_uri(self,\n",
    "                name: str = None,\n",
    "                project: str = None,\n",
    "                entity: str = None,\n",
    "                version: str = 'latest'):\n",
    "        name = name or self.name\n",
    "        project = project or self.project\n",
    "        entity = entity or self.entity\n",
    "        version = version or self.version\n",
    "        \n",
    "        uri = name + f\":{version}\"\n",
    "        if project:\n",
    "            uri = \"/\".join([project, uri])\n",
    "            if entity:\n",
    "                uri = \"/\".join([entity, uri])\n",
    "        return uri\n",
    "\n",
    "@dataclass\n",
    "class ModelArtifactConfig(ArtifactConfig):\n",
    "\n",
    "    model_type: str = field(default=None)#\"imagenet-init-model\")\n",
    "    model_stage: ModelStage = field(default=ModelStage.imagenet_init)\n",
    "    model_dir: str = field(default=None) #, init=False)\n",
    "    model_path: str = field(default=None) #, init=False)\n",
    "    datasets_seen: list = None #field(default_factory=list)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        super().__post_init__()\n",
    "        self.name = self.basename + f\"_{self.model_stage.value}\"\n",
    "        if self.datasets_seen is None:\n",
    "            self.datasets_seen = []\n",
    "        if len(self.datasets_seen)>0:\n",
    "            self.name = \"_\".join([self.name, *self.datasets_seen])\n",
    "        \n",
    "        self.model_dir = os.path.join(self.root_dir, 'models', self.basename, self.model_stage.value)\n",
    "        self.model_path = os.path.join(self.model_dir, \"best_model.ckpt\")\n",
    "        self.model_type = \"_\".join([*self.datasets_seen, f\"{self.model_stage.value}_model\"])\n",
    "\n",
    "        \n",
    "@dataclass\n",
    "class InputModelArtifactConfig(ModelArtifactConfig):\n",
    "\n",
    "    model_stage: ModelStage = field(default=ModelStage.imagenet_init)\n",
    "    artifact_type: ArtifactType = field(default=ArtifactType.InputArtifact)\n",
    "        \n",
    "@dataclass\n",
    "class OutputModelArtifactConfig(ModelArtifactConfig):\n",
    "\n",
    "    model_stage: ModelStage = field(default=ModelStage.train)\n",
    "    artifact_type: ArtifactType = field(default=ArtifactType.OutputArtifact)\n",
    "\n",
    "\n",
    "\n",
    "# input_model_artifact_cfg = InputModelArtifactConfig('resnet50')\n",
    "# output_model_artifact_cfg = OutputModelArtifactConfig('resnet50')\n",
    "# pp(input_model_artifact_cfg, output_model_artifact_cfg)\n",
    "\n",
    "@dataclass\n",
    "class DatasetArtifactConfig(ArtifactConfig):\n",
    "    \n",
    "    dataset_dir: str = field(default=None)\n",
    "        \n",
    "    def __post_init__(self):        \n",
    "        \n",
    "        if self.root_dir is None:\n",
    "            self.root_dir = os.path.join(os.environ[\"WANDB_CACHE_DIR\"],'artifacts', 'datasets')\n",
    "        super().__post_init__()\n",
    "        self.dataset_dir = os.path.join(self.root_dir, self.name)\n",
    "        \n",
    "        \n",
    "@dataclass\n",
    "class InputDatasetArtifactConfig(ArtifactConfig):\n",
    "    artifact_type: ArtifactType = field(default=ArtifactType.InputArtifact)\n",
    "        \n",
    "        \n",
    "# Currently no specific use for OutputDatasetArtifactConfig since  most typical training pipelines must input data far more often than they output it.,\n",
    "# but the definition is included for completeness + conceptual symmetry with Model In/Out artifacts + futureproofing.\n",
    "        \n",
    "@dataclass\n",
    "class OutputDatasetArtifactConfig(ArtifactConfig):\n",
    "    artifact_type: ArtifactType = field(default=ArtifactType.OutputArtifact)\n",
    "\n",
    "\n",
    "# input_dataset_artifact_cfg = InputDatasetArtifactConfig(basename=\"PNAS\")\n",
    "# pp(input_dataset_artifact_cfg)\n",
    "\n",
    "@dataclass\n",
    "class WandBInitConfig:\n",
    "\n",
    "    entity: str = \"jrose\"\n",
    "    project: str = \"image_classification\"\n",
    "    job_type: str = \"train\"\n",
    "    group: str = \"stage_0\"\n",
    "    run_dir: str = os.environ['WANDB_CACHE_DIR']\n",
    "#     tags: set = None\n",
    "\n",
    "        \n",
    "#     def __post_init__(self):\n",
    "#         if self.tags is None:\n",
    "#             self.tags = set()\n",
    "#         self.tags = set() if self.tags is None else set(self.tags)\n",
    "\n",
    "#     @property\n",
    "#     def tags(self):\n",
    "#         return self._tags\n",
    "#     @tags.setter\n",
    "#     def tags(self, new_tags: set):\n",
    "#         self._tags = self.tags.union(set(new_tags))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainerConfig(BaseConfig):\n",
    "    gpus: int = 1\n",
    "    max_epochs: int = 40\n",
    "    log_every_n_steps: int = 50\n",
    "    weights_summary: str = \"top\"\n",
    "    profiler: str = \"simple\"\n",
    "    auto_scale_batch_size: str = field(default=False) #\"power\", # \"binsearch\" # None\n",
    "    auto_lr_find: bool = field(default=False)\n",
    "    auto_lr_num_samples: int = 100\n",
    "\n",
    "# init_cfg = WandBInitConfig()\n",
    "# trainer_cfg = TrainerConfig()\n",
    "# pp(init_cfg,trainer_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d768552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "@dataclass\n",
    "class SimpleExperimentConfig(BaseConfig):\n",
    "    \n",
    "    wandb_init: WandBInitConfig\n",
    "    trainer: TrainerConfig\n",
    "    \n",
    "    dataset: DatasetConfig\n",
    "    model: ModelConfig\n",
    "    input_dataset_artifact: InputDatasetArtifactConfig\n",
    "    input_model_artifact: InputModelArtifactConfig\n",
    "    output_model_artifact: OutputModelArtifactConfig\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \n",
    "        self.model.input_shape = (self.dataset.channels, self.dataset.image_size, self.dataset.image_size)        \n",
    "#         self.wandb_init.tags.union({self.model.name, self.dataset.name})\n",
    "\n",
    "#     def asdict(self):\n",
    "#         return asdict(self)\n",
    "    \n",
    "#     def astuple(self):\n",
    "#         return astuple(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7c149f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#     @classmethod\n",
    "#     def from_dict(cls, data_dict: Dict[str,Any]):\n",
    "\n",
    "#         sig = inspect.signature(cls)#SimpleExperimentConfig)\n",
    "#         kwargs = {}\n",
    "\n",
    "#         for key, param in sig.parameters.items():\n",
    "#     #     for k, v in data_dict.items():\n",
    "#             if sig.parameters[key].annotation != inspect._empty:\n",
    "#                 kwargs[key] = param.annotation(**data_dict[key])\n",
    "#                 print(f'Added {key} to kwargs by converting them to a {param} first')\n",
    "\n",
    "#         return cls(**kwargs)\n",
    "#     print(k, f'type(v): {type(v)}', v.annotation,'\\n')\n",
    "\n",
    "# model_cfg_factory = sig.parameters['model'].annotation\n",
    "# model_cfg_factory(**experiment_cfg_2.model)\n",
    "\n",
    "# inspect.signature(param.annotation)\n",
    "\n",
    "# test = SimpleExperimentConfig.from_dict(exp_dict_cfg)\n",
    "# pp(test)\n",
    "\n",
    "# dir(experiment_cfg_2)\n",
    "# experiment_cfg_2.__dataclass_fields__.keys()\n",
    "# experiment_cfg_2.wandb_init.keys()\n",
    "# # experiment_cfg_2.__dataclass_fields__.keys()\n",
    "# SimpleExperimentConfig.__dataclass_fields__.keys()\n",
    "# for k,v in SimpleExperimentConfig.__dataclass_fields__.items():\n",
    "#     print(k, type(v), '\\n')\n",
    "# dir(v)\n",
    "# v.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a729208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as pp\n",
    "\n",
    "wandb_init_cfg = WandBInitConfig()\n",
    "trainer_cfg = TrainerConfig()\n",
    "dataset_cfg = DatasetConfig(name=\"Extant_family_10_512\",\n",
    "                            batch_size = 64,\n",
    "                            val_split = 0.0,\n",
    "                            image_size = 512,\n",
    "                            channels = 3,\n",
    "                            class_type = \"family\",\n",
    "                            normalize = True,\n",
    "                            seed = 253,\n",
    "                            predict_on_split = \"val\",\n",
    "                            debug = False,\n",
    "                            num_workers = 4,\n",
    "                            root_dir = None)\n",
    "\n",
    "opt_cfg = OptimizerConfig(name=\"Adam\", lr=1e-4)\n",
    "model_cfg = ModelConfig(optimizer=opt_cfg, unfreeze=[\"layer4\"])#[\"layer3\", \"layer4\"])\n",
    "\n",
    "\n",
    "input_dataset_artifact_cfg = InputDatasetArtifactConfig(basename=\"PNAS\")\n",
    "input_model_artifact_cfg = InputModelArtifactConfig(basename='resnet50')\n",
    "output_model_artifact_cfg = OutputModelArtifactConfig(basename='resnet50')\n",
    "\n",
    "experiment_cfg = SimpleExperimentConfig(wandb_init=wandb_init_cfg,\n",
    "                                        trainer=trainer_cfg,\n",
    "                                        dataset=dataset_cfg,\n",
    "                                        model=model_cfg,\n",
    "                                        input_dataset_artifact=input_dataset_artifact_cfg,\n",
    "                                        input_model_artifact=input_model_artifact_cfg,\n",
    "                                        output_model_artifact=output_model_artifact_cfg)\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "conf1 = OmegaConf.create(experiment_cfg.asdict())\n",
    "\n",
    "# pp(conf1)\n",
    "pp(experiment_cfg)\n",
    "\n",
    "deserialized = OmegaConf.create(OmegaConf.to_yaml(conf1))\n",
    "\n",
    "final_conf = SimpleExperimentConfig.from_dict(deserialized)\n",
    "pp(final_conf)\n",
    "\n",
    "# pp(experiment_cfg.asdict())\n",
    "\n",
    "# recon = SimpleExperimentConfig.from_dict(experiment_cfg.asdict())\n",
    "\n",
    "# pp(recon.model)\n",
    "\n",
    "# pp(experiment_cfg.model)\n",
    "\n",
    "# tags = set()\n",
    "# tags = tags.union({\"pnas\"})\n",
    "# print(tags)\n",
    "# tags = tags.union({\"extant\"})\n",
    "# print(tags)\n",
    "\n",
    "print(f'All equal: {experiment_cfg==recon}')\n",
    "print(f'All equal as_dict: {experiment_cfg.asdict()==recon.asdict()}')\n",
    "\n",
    "\n",
    "for k,v in experiment_cfg.asdict().items():\n",
    "    print(k)\n",
    "    print(f'Reconstructed:')\n",
    "    pp(recon.asdict()[k])\n",
    "    print('Original:')\n",
    "    pp(v)\n",
    "    \n",
    "    print(f'Equal: {v==recon.asdict()[k]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "810217e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class ModelArtifactConfig(ArtifactConfig):\n",
    "\n",
    "#     basename: str = 'resnet50'\n",
    "#     name: str = field(default=None)\n",
    "#     project: str = field(default=None)\n",
    "#     entity: str = field(default=None)\n",
    "#     version: str = 'latest'\n",
    "#     uri: str = field(default=\"\", init=False)\n",
    "#     model_type: str = field(default=None)#\"imagenet-init-model\")\n",
    "#     model_stage: ModelStage = field(default=ModelStage.imagenet_init)\n",
    "#     model_dir: str = field(init=False)\n",
    "#     model_path: str = field(init=False)\n",
    "#     datasets_seen: List = field(default_factory=list)\n",
    "#     artifact_type: ArtifactType = field(default=None)#ArtifactType.InputArtifact\n",
    "#     description: str = field(default=None)\n",
    "#     root_dir: str = field(default=None)\n",
    "\n",
    "#     def __post_init__(self):\n",
    "#         self.name = self.basename + f\"_{self.model_stage.value}\"\n",
    "#         if len(self.datasets_seen)>0:\n",
    "#             self.name = \"_\".join([self.name, *self.datasets_seen])\n",
    "#         super().__post_init__()\n",
    "#         self.model_dir = os.path.join(self.root_dir, self.model_stage.value)\n",
    "#         self.model_path = os.path.join(self.model_dir, \"best_model.ckpt\")\n",
    "\n",
    "#         self.model_type = \"_\".join([*self.datasets_seen, f\"{self.model_stage.value}_model\"])\n",
    "\n",
    "# @dataclass\n",
    "# class ModelArtifactConfig:\n",
    "        \n",
    "#     basename: str = 'resnet50'\n",
    "    \n",
    "#     name: str = field(default=None)\n",
    "#     project: str = field(default=None)\n",
    "#     entity: str = field(default=None)\n",
    "#     version: str = 'latest'\n",
    "#     uri: str = field(default=\"\", init=False)\n",
    "        \n",
    "#     model_type: str = field(default=None)#\"imagenet-init-model\")\n",
    "#     artifact_type: ArtifactType = field(default=None)#ArtifactType.InputArtifact\n",
    "#     description: str = field(default=None)\n",
    "#     root_dir: str = field(default=None)\n",
    "        \n",
    "        \n",
    "#     def __post_init__(self):\n",
    "#         if self.root_dir is None:\n",
    "#             self.root_dir = os.path.join(os.environ[\"WANDB_CACHE_DIR\"],'artifacts', 'models', self.basename)\n",
    "\n",
    "#         if self.name is None:\n",
    "#             self.name = self.basename\n",
    "\n",
    "#         self.uri = self.name + f\":{self.version}\"\n",
    "#         if self.project:\n",
    "#             self.uri = \"/\".join([self.project, self.uri])\n",
    "#             if self.entity:\n",
    "#                 self.uri = \"/\".join([self.entity, self.uri])\n",
    "\n",
    "# model_artifact_cfg = ModelArtifactConfig(entity=\"jrose\", project=\"image_classification\")\n",
    "# model_artifact_cfg\n",
    "\n",
    "# @dataclass\n",
    "# class InputModelArtifactConfig(ModelArtifactConfig):\n",
    "\n",
    "#     name: str = field(init=False)\n",
    "#     model_type: str = field(default=\"imagenet-init-model\")\n",
    "#     model_dir: str = field(init=False)\n",
    "#     artifact_type: ArtifactType = field(default=ArtifactType.InputArtifact)\n",
    "        \n",
    "#     def __post_init__(self):\n",
    "#         self.name = self.basename + \"_init\"\n",
    "        \n",
    "#         super().__post_init__()\n",
    "#         self.model_dir = os.path.join(self.root_dir, \"init\") #\"models\", self.basename + \"_init\")\n",
    "        \n",
    "        \n",
    "# @dataclass\n",
    "# class OutputModelArtifactConfig(ModelArtifactConfig):\n",
    "\n",
    "#     name: str = field(init=False)\n",
    "#     model_type: str = field(default=\"PNAS-trained-model\")\n",
    "#     model_dir: str = field(init=False)\n",
    "#     artifact_type: ArtifactType = field(default=ArtifactType.OutputArtifact)\n",
    "    \n",
    "#     def __post_init__(self):\n",
    "#         self.name = self.basename + \"_trained\"\n",
    "        \n",
    "#         super().__post_init__()\n",
    "#         self.model_dir = os.path.join(self.root_dir, \"trained\")\n",
    "#         self.model_path = os.path.join(self.model_dir, \"best_model.ckpt\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# #     output_model_dir: '/media/data/jacob/wandb_cache/artifacts/models/resnet50:v0/Extant_family_10_512_trained',\n",
    "# #     output_model_path: '/media/data/jacob/wandb_cache/artifacts/models/resnet50:v0/Extant_family_10_512_trained/best.ckpt',\n",
    "# #     output_name: 'resnet50_output'\n",
    "# #     output_type: 'Extant_trained_model',\n",
    "\n",
    "\n",
    "\n",
    "# input_model_artifact_cfg = InputModelArtifactConfig()\n",
    "\n",
    "# output_model_artifact_cfg = OutputModelArtifactConfig()\n",
    "\n",
    "# pp(input_model_artifact_cfg, output_model_artifact_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74971fec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config.wandb = {\n",
    "#                 \"init\":\n",
    "#                        {\n",
    "#                         \"entity\":\"jrose\",\n",
    "#                         \"project\":\"image_classification_datasets\",\n",
    "#                         \"job_type\":'create-dataset',\n",
    "#                         \"group\":None,\n",
    "#                         \"run_dir\":os.environ['WANDB_CACHE_DIR'],\n",
    "#                         \"tags\":[d.name for d in config.datasets]\n",
    "#                        },\n",
    "#                 \"artifacts\":\n",
    "#                         {\n",
    "#                         \"root_dir\":None\n",
    "#                         },\n",
    "#                 \"input_artifacts\":\n",
    "#                        [\n",
    "#                            {\n",
    "#                             \"entity\":\"jrose\",\n",
    "#                             \"project\":\"image_classification_datasets\",\n",
    "#                             \"name\": config.datasets[0].name,\n",
    "#                             \"version\": \"v6\",\n",
    "#                             \"type\": \"raw_data\",\n",
    "#                             \"root_dir\":None,\n",
    "#                             \"uri\":None\n",
    "#                            }\n",
    "#                        ]\n",
    "# }\n",
    "\n",
    "\n",
    "# config.dataset = {\n",
    "#                   \"name\":\"PNAS_family_100_1024\",\n",
    "#                   \"batch_size\":512,\n",
    "#                   \"val_split\":None, # TODO specify split explicitly in wandb report\n",
    "#                   \"num_workers\":4,\n",
    "#                   \"image_size\":1024,\n",
    "#                   \"channels\":3,\n",
    "#                   \"class_type\":\"family\",\n",
    "#                   \"debug\":False,\n",
    "#                   \"normalize\":True,\n",
    "#                   \"seed\":389,\n",
    "#                   \"dataset_dir\":None,\n",
    "#                   \"predict_on_split\":\"val\",\n",
    "#                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "830a8aa6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjrose\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.26<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">robust-fire-158</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/jrose/image_classification\" target=\"_blank\">https://wandb.ai/jrose/image_classification</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/jrose/image_classification/runs/lfz1o7vb\" target=\"_blank\">https://wandb.ai/jrose/image_classification/runs/lfz1o7vb</a><br/>\n",
       "                Run data is saved locally in <code>/media/data/jacob/wandb_cache/wandb/run-20210522_152100-lfz1o7vb</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# config.dataset = {\n",
    "#                   \"name\":\"Extant_family_10_512\",\n",
    "#                   \"batch_size\":64,\n",
    "#                   \"val_split\":None, # TODO specify split explicitly in wandb report\n",
    "#                   \"num_workers\":4,\n",
    "#                   \"image_size\":512,\n",
    "#                   \"channels\":3,\n",
    "#                   \"class_type\":\"family\",\n",
    "#                   \"debug\":False,\n",
    "#                   \"normalize\":True,\n",
    "#                   \"seed\":389,\n",
    "#                   \"dataset_dir\":None,\n",
    "#                   \"predict_on_split\":\"val\",\n",
    "#                   }\n",
    "\n",
    "\n",
    "# #####################################\n",
    "\n",
    "# config.model = {\n",
    "#                 \"name\":\"resnet50\",\n",
    "#                 \"num_classes\":None,\n",
    "#                 \"input_shape\": (config.dataset.channels, config.dataset.image_size, config.dataset.image_size),\n",
    "#                 \"optimizer\":{'name':\"Adam\", 'lr':0.01},\n",
    "#                 \"head\":[\"avg_pool\",\"flatten\",\"fc\"],\n",
    "#                 \"unfreeze\":[\"layer4\"],\n",
    "#                 \"checkpoint_dir\":None\n",
    "#                 }\n",
    "\n",
    "# config.trainer = {\n",
    "#                   \"gpus\":1,\n",
    "#                   \"max_epochs\":40,\n",
    "#                   \"log_every_n_steps\":50,\n",
    "#                   \"weights_summary\":\"top\",\n",
    "#                   \"profiler\":\"simple\",\n",
    "#                   \"auto_scale_batch_size\":\"power\", # \"binsearch\" # None\n",
    "#                   \"auto_lr_find\":True,\n",
    "#                   \"auto_lr_num_samples\":50\n",
    "#                   }\n",
    "\n",
    "\n",
    "\n",
    "# config.wandb = {\n",
    "#                 \"init\":\n",
    "#                        {\n",
    "#                         \"entity\":\"jrose\",\n",
    "#                         \"project\":\"image_classification\",\n",
    "#                         \"job_type\":\"train\",\n",
    "#                         \"group\":\"stage_0\",\n",
    "#                         \"run_dir\":os.environ['WANDB_CACHE_DIR'],\n",
    "#                         \"tags\":[config.dataset.name]\n",
    "#                        },\n",
    "#                 \"artifacts\":\n",
    "#                         {\n",
    "#                         \"root_dir\":None\n",
    "#                         },\n",
    "#                 \"input_artifact\":\n",
    "#                        {\n",
    "#                         \"entity\":\"jrose\",\n",
    "#                         \"project\":\"image_classification_datasets\",\n",
    "#                         \"name\": config.dataset.name,\n",
    "#                         \"version\": \"latest\", #\"v1\",\n",
    "#                         \"type\": \"raw_data\",\n",
    "#                         \"root_dir\":None\n",
    "#                        },\n",
    "#                 \"model_artifact\":\n",
    "#                         {\n",
    "#                          \"basename\": config.model.name,\n",
    "#                          \"version\":\"v0\",\n",
    "#                          \"input_type\":\"imagenet-init-model\",\n",
    "#                          \"output_type\":f'{config.dataset.name.split(\"_\")[0]}_trained_model',\n",
    "#                          \"description\":\"imagenet-pretrained ResNet50 base model -> avg_pool -> 1 fc output layer w/ size=num_classes\",\n",
    "#                          \"root_dir\":None\n",
    "#                         }\n",
    "#     }\n",
    "\n",
    "# config.wandb.input_artifact.uri = \"/\".join([config.wandb.input_artifact.entity,\n",
    "#                                             config.wandb.input_artifact.project,\n",
    "#                                             config.wandb.input_artifact.name]) \\\n",
    "#                                    + f':{config.wandb.input_artifact.version}'\n",
    "\n",
    "# config.wandb.artifacts.root_dir = os.path.join(*[config.wandb.init.run_dir,\n",
    "#                                                 \"artifacts\"])\n",
    "\n",
    "# config.wandb.input_artifact.root_dir = os.path.join(*[config.wandb.artifacts.root_dir,\n",
    "#                                                      \"datasets\",\n",
    "#                                                      config.wandb.input_artifact.name \\\n",
    "#                                                  + f':{config.wandb.input_artifact.version}'\n",
    "#                                                     ])\n",
    "\n",
    "\n",
    "# config.wandb.model_artifact.root_dir = os.path.join(*[config.wandb.artifacts.root_dir,\n",
    "#                                                      \"models\",\n",
    "#                                                      config.wandb.model_artifact.basename \\\n",
    "#                                                  + f':{config.wandb.model_artifact.version}'\n",
    "#                                                     ])\n",
    "# config.wandb.model_artifact.init_model_dir = os.path.join(*[config.wandb.model_artifact.root_dir,\n",
    "#                                                             'init'\n",
    "#                                                            ])\n",
    "# config.wandb.model_artifact.output_model_dir = str(Path(config.wandb.model_artifact.root_dir,\n",
    "#                                                     f\"{config.dataset.name}_trained\"))\n",
    "\n",
    "# config.wandb.model_artifact.output_model_path = os.path.join(config.wandb.model_artifact.output_model_dir, \"best.ckpt\")\n",
    "\n",
    "# config.wandb.model_artifact.init_name = config.wandb.model_artifact.basename + \"_init\"\n",
    "# config.wandb.model_artifact.output_name = config.wandb.model_artifact.basename + \"_output\"\n",
    "\n",
    "\n",
    "# config.dataset.dataset_dir = config.wandb.input_artifact.root_dir\n",
    "# config.model.checkpoint_dir = os.path.join(config.wandb.model_artifact.root_dir,'ckpts')\n",
    "\n",
    "# os.environ[\"WANDB_PROJECT\"] = config.wandb.init.project\n",
    "# run = wandb.init(entity='jrose',\n",
    "#                  project=config.wandb.init.project,\n",
    "#                  job_type=config.wandb.init.job_type,\n",
    "#                  group=config.wandb.init.group,\n",
    "#                  dir=config.wandb.init.run_dir,\n",
    "#                  config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aea18b73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class_type = \"family\"\n",
    "# pnas_threshold = 100\n",
    "# image_size = 512\n",
    "# seed = 257\n",
    "\n",
    "# config.datasets = [{\n",
    "#                   \"name\": f\"PNAS_{class_type}_{pnas_threshold}_{image_size}\",\n",
    "#                   \"batch_size\":32,\n",
    "#                   \"val_split\":None, # TODO specify split explicitly in wandb report\n",
    "#                   \"num_workers\":4,\n",
    "#                   \"image_size\":image_size,\n",
    "#                   \"channels\":3,\n",
    "#                   \"class_type\":class_type,\n",
    "#                   \"debug\":False,\n",
    "#                   \"normalize\":True,\n",
    "#                   \"seed\":seed,\n",
    "#                   \"dataset_dir\":None,\n",
    "#                   \"predict_on_split\":\"val\",\n",
    "#                   },\n",
    "#     {\n",
    "#                   \"name\":f\"Extant_{class_type}_{extant_threshold}_{image_size}\",  # f\"PNAS_{label_type}_{pnas_threshold}_{image_size}\"\n",
    "#                   \"batch_size\":32,\n",
    "#                   \"val_split\":None, # TODO specify split explicitly in wandb report\n",
    "#                   \"num_workers\":4,\n",
    "#                   \"image_size\":image_size,\n",
    "#                   \"channels\":3,\n",
    "#                   \"class_type\":class_type,\n",
    "#                   \"debug\":False,\n",
    "#                   \"normalize\":True,\n",
    "#                   \"seed\":seed,\n",
    "#                   \"dataset_dir\":None,\n",
    "#                   \"predict_on_split\":\"val\",\n",
    "#                   }]\n",
    "\n",
    "\n",
    "\n",
    "# config.wandb = {\n",
    "#                 \"init\":\n",
    "#                        {\n",
    "#                         \"entity\":\"jrose\",\n",
    "#                         \"project\":\"image_classification_datasets\",\n",
    "#                         \"job_type\":'create-dataset',\n",
    "#                         \"group\":None,\n",
    "#                         \"run_dir\":os.environ['WANDB_CACHE_DIR'],\n",
    "#                         \"tags\":[config.dataset.name]\n",
    "#                        },\n",
    "#                 \"artifacts\":\n",
    "#                         {\n",
    "#                         \"root_dir\":None\n",
    "#                         },\n",
    "#                 \"input_artifacts\":\n",
    "#                        [\n",
    "#                            {\n",
    "#                             \"entity\":\"jrose\",\n",
    "#                             \"project\":\"image_classification_datasets\",\n",
    "#                             \"name\": config.dataset.name,\n",
    "#                             \"version\": \"v0\",\n",
    "#                             \"type\": \"raw_data\",\n",
    "#                             \"root_dir\":None,\n",
    "#                             \"uri\":None\n",
    "#                            }\n",
    "#                        ]\n",
    "# }\n",
    "\n",
    "# def fetch_datamodule_from_dataset_artifact(config: Box) -> LeavesLightningDataModule:\n",
    "    \n",
    "#     artifact = run.use_artifact(config.wandb.input_artifact.uri,\n",
    "#                                 type=config.wandb.input_artifact.type) #'jrose/image_classification_datasets/PNAS_family_100_1024:v0', type='raw_data')\n",
    "#     dataset_artifact_dir = artifact.download(root=config.wandb.input_artifact.root_dir)\n",
    "\n",
    "\n",
    "#     datamodule = get_datamodule(config.dataset)\n",
    "#     datamodule.setup('fit')\n",
    "#     datamodule.setup('test')\n",
    "#     ########################\n",
    "#     config.model.num_classes = config.dataset.num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f831323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Step 3: Train w/ artifacts and save model\n",
    "\n",
    "########################################\n",
    "#### Instantiate dataset ####\n",
    "########################################\n",
    "\n",
    "# def fetch_datamodule_from_dataset_artifact(artifact_config: Box, run_or_api=None) -> LeavesLightningDataModule:\n",
    "#     run = run_or_api or wandb.Api()\n",
    "#     artifact = run.use_artifact(config.wandb.input_artifact.uri,\n",
    "#                                 type=config.wandb.input_artifact.type) #'jrose/image_classification_datasets/PNAS_family_100_1024:v0', type='raw_data')\n",
    "#     dataset_artifact_dir = artifact.download(root=config.dataset.dataset_dir) # config.wandb.input_artifact.root_dir)\n",
    "\n",
    "\n",
    "#     datamodule = get_datamodule(config.dataset)\n",
    "#     datamodule.setup('fit')\n",
    "#     datamodule.setup('test')\n",
    "#     ########################\n",
    "#     config.model.num_classes = config.dataset.num_classes\n",
    "\n",
    "#     return datamodule, artifact\n",
    "\n",
    "########################################\n",
    "#### Instantiate model ####\n",
    "########################################\n",
    "\n",
    "# datamodule, artifact = fetch_datamodule_from_dataset_artifact(config=config, run_or_api=run)\n",
    "\n",
    "# assert (num_classes == 19) | (num_classes == 179)\n",
    "\n",
    "########################################\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5d8e8fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = ResNet(model_name=config.model.name,\n",
    "#                num_classes=config.model.num_classes,\n",
    "#                input_shape=config.model.input_shape,\n",
    "#                optimizer=config.model.optimizer)\n",
    "# model.reset_classifier(config.model.num_classes,'avg')\n",
    "# model.unfreeze(getattr(model, config.model.unfreeze[0]))\n",
    "\n",
    "# config.wandb.model_artifact.init_model_path = Path(config.wandb.model_artifact.init_model_dir,\n",
    "#                                                    config.wandb.model_artifact.version,\n",
    "#                                                    \"imagenet\")\n",
    "# os.makedirs(os.path.dirname(config.wandb.model_artifact.init_model_path), exist_ok=True)\n",
    "# model.save_model(config.wandb.model_artifact.init_model_path)\n",
    "\n",
    "# model_artifact = wandb.Artifact(config.wandb.model_artifact.init_name,\n",
    "#                                 type=config.wandb.model_artifact.input_type,\n",
    "#                                 description=config.wandb.model_artifact.description,\n",
    "#                                 metadata=dict(**config.wandb.model_artifact,\n",
    "#                                               **config.model)\n",
    "# #                                 metadata=dict(version=config.wandb.model_artifact.version,\n",
    "# #                                               **config.model)\n",
    "#                                 )\n",
    "# model_artifact.add_dir(os.path.dirname(config.wandb.model_artifact.init_model_path))\n",
    "# run.log_artifact(model_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449cd08b",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db8822ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def build_or_restore_model_from_artifact(model_config: Box, \n",
    "#                                          artifact_config: Box,\n",
    "#                                          run_or_api=None) -> pl.LightningModule:\n",
    "#     \"\"\"\n",
    "    \n",
    "    \n",
    "#     \"\"\"\n",
    "#     model = ResNet(model_name=model_config.name,\n",
    "#                    num_classes=model_config.num_classes,\n",
    "#                    input_shape=model_config.input_shape,\n",
    "#                    optimizer=model_config.optimizer)\n",
    "#     model.reset_classifier(model_config.num_classes,'avg')\n",
    "#     model.unfreeze(getattr(model, model_config.unfreeze[0]))\n",
    "\n",
    "#     artifact_config.init_model_path = Path(artifact_config.init_model_dir,\n",
    "#                                            artifact_config.version,\n",
    "#                                            \"imagenet\")\n",
    "#     os.makedirs(os.path.dirname(artifact_config.init_model_path), exist_ok=True)\n",
    "#     model.save_model(artifact_config.init_model_path)\n",
    "\n",
    "#     artifact = wandb.Artifact(artifact_config.init_name,\n",
    "#                               type=artifact_config.input_type,\n",
    "#                               description=artifact_config.description,\n",
    "#                               metadata=dict({\n",
    "#                                             \"artifact\":{**artifact_config},\n",
    "#                                              \"model\":{**model_config}\n",
    "#                                            })\n",
    "#                              )\n",
    "#     artifact.add_dir(os.path.dirname(artifact_config.init_model_path))\n",
    "#     run.log_artifact(model_config)\n",
    "    \n",
    "\n",
    "#     return model, artifact\n",
    "\n",
    "# from typing import Dict\n",
    "# def get_labels_from_filepath(path: str, fix_catalog_number: bool = False) -> Dict[str,str]:\n",
    "#     \"\"\"\n",
    "#     Splits a precisely-formatted filename with the expectation that it is constructed with the following fields separated by '_':\n",
    "#     1. family\n",
    "#     2. genus\n",
    "#     3. species\n",
    "#     4. collection\n",
    "#     5. catalog_number\n",
    "    \n",
    "#     If fix_catalog_number is True, assume that the collection is not included and must separately be extracted from the first part of the catalog number.\n",
    "    \n",
    "#     \"\"\"\n",
    "#     family, genus, species, collection, catalog_number = Path(path).stem.split(\"_\", maxsplit=4)\n",
    "#     if fix_catalog_number:\n",
    "#         catalog_number = '_'.join([collection, catalog_number])\n",
    "#     return {\"family\":family,\n",
    "#             \"genus\":genus,\n",
    "#             \"species\":species,\n",
    "#             \"collection\":collection,\n",
    "#             \"catalog_number\":catalog_number}\n",
    "\n",
    "\n",
    "# from tqdm.auto import trange, tqdm\n",
    "\n",
    "# def log_predictions(trainer, model, datamodule, fix_catalog_number=True):\n",
    "#     subset = datamodule.predict_on_split\n",
    "#     datamodule.setup(stage=\"predict\")\n",
    "#     classes = datamodule.classes\n",
    "    \n",
    "#     columns = ['catalog_number',\n",
    "#                'image',\n",
    "#                'guess',\n",
    "#                'truth',\n",
    "#                'softmax_guess',\n",
    "#                'softmax_truth']\n",
    "#     for j, class_name in enumerate(classes):\n",
    "#         columns.append(f'score_{class_name}')\n",
    "\n",
    "    \n",
    "#     loss_func = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "#     batches = trainer.predict(model, datamodule=datamodule)\n",
    "# #         x, y_logit, y_true, img_paths = list(np.concatenate(i) for i in zip(*batches))    \n",
    "#     prediction_rows = []\n",
    "#     for x, y_logit, y_true, img_paths in tqdm(batches, position=0, desc=\"epoch->\", unit='batches'):\n",
    "\n",
    "#         y_true = torch.from_numpy(y_true).to(dtype=torch.long)\n",
    "#         y_logit = torch.from_numpy(y_logit)\n",
    "\n",
    "#         loss = loss_func(y_logit, y_true)\n",
    "\n",
    "#         y_pred = torch.argmax(y_logit, -1)\n",
    "#         y_score = y_logit.softmax(1)\n",
    "\n",
    "#         x = torch.from_numpy(x).permute(0,2,3,1)\n",
    "#         x = (255 * (x - x.min()) / (x.max() - x.min())).numpy().astype(np.uint8)\n",
    "# #         for i in tqdm(range(len(y_pred)), position=1, desc=\"batch->\", unit='sample'):\n",
    "#         for i in trange(len(y_pred), position=1, leave=False, desc=\"batch->\", unit='sample'):\n",
    "#             labels = get_labels_from_filepath(path=img_paths[i],\n",
    "#                                               fix_catalog_number=fix_catalog_number)\n",
    "#             row = [\n",
    "#                     labels['catalog_number'],\n",
    "#                     wandb.Image(x[i,...]),\n",
    "#                     classes[y_pred[i]],\n",
    "#                     classes[y_true[i]],\n",
    "#                     y_score[i,y_pred[i]],\n",
    "#                     y_score[i,y_true[i]]\n",
    "#             ]\n",
    "\n",
    "#             for j, score_j in enumerate(y_score[i,:].tolist()):\n",
    "#                 row.append(np.round(score_j, 4))\n",
    "#             prediction_rows.append(row)\n",
    "\n",
    "\n",
    "#     prediction_table = wandb.Table(data=prediction_rows, columns=columns)\n",
    "#     artifact_name = f\"{wandb.run.name}_{wandb.run.id}\"\n",
    "#     prediction_artifact = wandb.Artifact(artifact_name,\n",
    "#                                          type=f\"{subset}_predictions\")\n",
    "    \n",
    "#     prediction_artifact.add(prediction_table, f\"{subset}_predictions\")\n",
    "#     wandb.run.log_artifact(prediction_artifact)\n",
    "#     return prediction_artifact\n",
    "\n",
    "\n",
    "\n",
    "# ########################################\n",
    "# ########################################\n",
    "\n",
    "# def test_model(trainer,\n",
    "#                model,\n",
    "#                model_artifact,\n",
    "#                datamodule,\n",
    "#                config,\n",
    "#                fix_catalog_number=False, predictions_only=False):\n",
    "\n",
    "#     run = wandb.init(entity=config.wandb.init.entity,\n",
    "#                      project=config.wandb.init.project,\n",
    "#                      job_type=\"test\",\n",
    "#                      group=config.wandb.init.group,\n",
    "#                      dir=config.wandb.init.run_dir,\n",
    "#                      config=config)\n",
    "    \n",
    "# #     model_artifact.wait()\n",
    "# #     model_input_artifact = run.use_artifact(model_artifact)\n",
    "    \n",
    "#     model_input_artifact = run.use_artifact(config.wandb.model_artifact.output_name + \":latest\")\n",
    "#     model_input_dir = model_input_artifact.download(config.wandb.model_artifact.output_model_dir)\n",
    "    \n",
    "#     assert Path(config.wandb.model_artifact.output_model_path).name in os.listdir(model_input_dir)\n",
    "# #     model.load_from_checkpoint(config.wandb.model_artifact.output_model_path)\n",
    "#     model.load_model(config.wandb.model_artifact.output_model_path)\n",
    "    \n",
    "    \n",
    "#     if not predictions_only:\n",
    "#         test_results = trainer.test(model, datamodule=datamodule)\n",
    "#         if isinstance(test_results, list):\n",
    "#             assert len(test_results) == 1\n",
    "#             test_results = test_results[0]\n",
    "            \n",
    "#     prediction_artifact = log_predictions(trainer, model, datamodule, fix_catalog_number=fix_catalog_number)\n",
    "    \n",
    "#     run.finish()    \n",
    "#     return prediction_artifact, test_results\n",
    "\n",
    "#### Callbacks + Trainer\n",
    "\n",
    "\n",
    "# def configure_trainer(config, **kwargs):\n",
    "#     \"\"\"\n",
    "#     Example:\n",
    "#         trainer = configure_trainer(config)\n",
    "#     \"\"\"\n",
    "#     monitor = ModuleDataMonitor()#log_every_n_steps=25)\n",
    "#     per_class_metric_plots_cb = LogPerClassMetricsToWandb()\n",
    "#     early_stop_callback = EarlyStopping(monitor='val_loss',\n",
    "#                                         patience=3,\n",
    "#                                         verbose=False,\n",
    "#                                         mode='min')\n",
    "\n",
    "#     logger=pl.loggers.wandb.WandbLogger(name=f\"{config.dataset.name}-timm-{config.model.name}\",\n",
    "#                                         config=config)\n",
    "\n",
    "#     trainer = pl.Trainer(gpus=config.trainer.gpus,\n",
    "#                          logger=logger,\n",
    "#                          max_epochs=config.trainer.max_epochs,\n",
    "#                          weights_summary=config.trainer.weights_summary,\n",
    "#                          profiler=config.trainer.profiler,\n",
    "#                          log_every_n_steps=config.trainer.log_every_n_steps,\n",
    "#                          auto_scale_batch_size=config.trainer.auto_scale_batch_size,\n",
    "#                          callbacks=[per_class_metric_plots_cb,\n",
    "#                                     monitor,\n",
    "#                                     early_stop_callback],\n",
    "#                         **kwargs)\n",
    "#     return trainer\n",
    "\n",
    "# def log_model_checkpoint_2_artifact(model, \n",
    "#                                     ckpt_path: str,\n",
    "#                                     artifact_config,\n",
    "#                                     run=None):\n",
    "    \n",
    "#     run = run or wandb.run\n",
    "#     os.makedirs(os.path.dirname(artifact_config.output_model_path), exist_ok=True)\n",
    "    \n",
    "#     model = model.load_from_checkpoint(ckpt_path)\n",
    "#     model.save_model(artifact_config.output_model_path)\n",
    "\n",
    "#     output_model_artifact = wandb.Artifact(\n",
    "#                                     artifact_config.output_name,\n",
    "#                                     type=artifact_config.output_type,\n",
    "#                                     description=artifact_config.description,\n",
    "#                                     metadata=dict(**artifact_config,\n",
    "#                                                   **config.model)\n",
    "#                                     )\n",
    "#     output_model_artifact.add_dir(artifact_config.output_model_dir)\n",
    "#     run.log_artifact(output_model_artifact)\n",
    "\n",
    "#     print(\"Output Model Artifact Checkpoint path:\\n\", \n",
    "#           artifact_config.output_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804da54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# pipeline_stage = \"1\"\n",
    "# os.environ[\"WANDB_PROJECT\"] = config.wandb[pipeline_stage].init.project\n",
    "# run = wandb.init(entity=config.wandb[pipeline_stage].init.entity,\n",
    "#                  project=config.wandb[pipeline_stage].init.project,\n",
    "#                  job_type=config.wandb[pipeline_stage].init.job_type,\n",
    "#                  group=config.wandb[pipeline_stage].init.group,\n",
    "#                  dir=config.wandb[pipeline_stage].init.run_dir,\n",
    "#                  config=config)\n",
    "\n",
    "# if config.trainer.auto_lr_find:\n",
    "#     trainer = configure_trainer(config)\n",
    "#     lr_finder = trainer.tuner.lr_find(model, datamodule, num_training=config.trainer.auto_lr_num_samples)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# lr_finder.lr_min\n",
    "# lr_finder.lr_max\n",
    "# lr_finder.mode\n",
    "# lr_finder.num_training\n",
    "# lr_finder.results.keys()\n",
    "\n",
    "# # lr_finder.suggestion()\n",
    "# # fig = lr_finder.plot()\n",
    "# new_lr = lr_finder.suggestion()\n",
    "# print(f'Suggested starting learning rate: {new_lr:.2e}')\n",
    "\n",
    "# model.hparams.lr = new_lr\n",
    "# config.model.optimizer.init_lr = new_lr\n",
    "# fig = lr_finder.plot(suggest=True)\n",
    "# fig.show()\n",
    "# plt.suptitle(f\"lr_tuner loss landscape | {lr_finder.num_training} random samples | bsz = {config.model.batch_size}\")\n",
    "# plt.legend()\n",
    "# lr_tuner_results = pd.DataFrame(lr_finder.results)\n",
    "# # table = wandb.Table(dataframe=lr_tuner_results)\n",
    "# run.summary['lr_tuner_loss_landscape'] = table\n",
    "# run.summary['batch_size'] = config.model.batch_size\n",
    "# run.summary['image_size'] = config.datamodule.image_size\n",
    "# run.summary['lr_suggestion'] = lr_finder.suggestion()\n",
    "# run.summary['image_size'] = config.datamodule.image_size\n",
    "# run.log({\"lr_tuner-loss_landscape/plot\":wandb.Image(fig, caption=f\"{config.datamodule.name}-{config.model.name}\"),\n",
    "#          \"lr_tuner-loss_landscape/table\":wandb.Table(dataframe=lr_tuner_results)})\n",
    "\n",
    "# rows = table.iterrows()\n",
    "# rows_list = list(rows)\n",
    "# rows_list\n",
    "\n",
    "# def detailed_objective(trial):\n",
    "#     \"\"\"\n",
    "#     [source] https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/010_reuse_best_trial.html#sphx-glr-tutorial-20-recipes-010-reuse-best-trial-py\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Use same code objective to reproduce the best model\n",
    "#     X, y = make_classification(n_features=10, random_state=1)\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "#     C = trial.suggest_loguniform(\"C\", 1e-7, 10.0)\n",
    "\n",
    "#     clf = LogisticRegression(C=C)\n",
    "#     clf.fit(X_train, y_train)\n",
    "\n",
    "#     # calculate more evaluation metrics\n",
    "#     pred = clf.predict(X_test)\n",
    "\n",
    "#     acc = metrics.accuracy_score(pred, y_test)\n",
    "#     recall = metrics.recall_score(pred, y_test)\n",
    "#     precision = metrics.precision_score(pred, y_test)\n",
    "#     f1 = metrics.f1_score(pred, y_test)\n",
    "\n",
    "#     return acc, f1, recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8592fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19046f01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tqdm.auto import trange\n",
    "# def log_predictions(trainer, model, datamodule, subset=\"test\", fix_catalog_number=True):\n",
    "    \n",
    "#     datamodule.setup(stage=\"predict\")\n",
    "#     batches = trainer.predict(model, datamodule=datamodule)\n",
    "\n",
    "\n",
    "#     x, y_logit, y_true, img_paths = list(np.concatenate(i) for i in zip(*batches))\n",
    "#     loss_func = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "#     y_true = torch.from_numpy(y_true).to(dtype=torch.long)\n",
    "#     y_logit = torch.from_numpy(y_logit)\n",
    "\n",
    "#     loss = loss_func(y_logit, y_true)\n",
    "#     idx = np.argsort(loss)\n",
    "\n",
    "#     y_pred = torch.argmax(y_logit, -1)\n",
    "#     y_score = y_logit.softmax(1)\n",
    "\n",
    "#     x, y_logit, y_true, y_pred, img_paths, loss, y_score = (x[idx],\n",
    "#                                                             y_logit[idx],\n",
    "#                                                             y_true[idx],\n",
    "#                                                             y_pred[idx],\n",
    "#                                                             img_paths[idx],\n",
    "#                                                             loss[idx],\n",
    "#                                                             y_score[idx])\n",
    "\n",
    "#     columns = ['catalog_number',\n",
    "#                'image',\n",
    "#                'guess',\n",
    "#                'truth']\n",
    "#     for j, class_name in enumerate(datamodule.classes):\n",
    "#         columns.append(f'score_{class_name}')\n",
    "\n",
    "#     x = torch.from_numpy(x).permute(0,2,3,1)\n",
    "#     prediction_rows = []\n",
    "#     x = (255 * (x - x.min()) / (x.max() - x.min())).numpy().astype(np.uint8)\n",
    "#     for i in trange(len(y_pred)):\n",
    "#         labels = get_labels_from_filepath(path=img_paths[i],\n",
    "#                                           fix_catalog_number=fix_catalog_number)\n",
    "#         row = [\n",
    "#                 labels['catalog_number'],\n",
    "#                 wandb.Image(x[i,...]),\n",
    "#                 datamodule.classes[y_pred[i]],\n",
    "#                 datamodule.classes[y_true[i]]\n",
    "#         ]\n",
    "\n",
    "#         for j, score_j in enumerate(y_score[i,:].tolist()):\n",
    "#             row.append(np.round(score_j, 4))\n",
    "#         prediction_rows.append(row)\n",
    "\n",
    "\n",
    "#     prediction_table = wandb.Table(data=prediction_rows, columns=columns)\n",
    "#     prediction_artifact = wandb.Artifact(f\"{subset}_predictions-{wandb.run.id}\", \n",
    "#                                          type=\"predictions\")\n",
    "#     prediction_artifact.add(prediction_table, f\"{subset}_predictions\")\n",
    "#     wandb.run.log_artifact(prediction_artifact)\n",
    "\n",
    "# log_predictions(trainer, model, datamodule, subset = \"init\", fix_catalog_number=True)\n",
    "\n",
    "# y_logit, y_true, img_paths = [], [], []\n",
    "\n",
    "# for batch in batches:\n",
    "#     y_logit.append(batch[0])\n",
    "#     y_true.append(batch[1])\n",
    "#     img_paths.append(batch[2])\n",
    "\n",
    "\n",
    "\n",
    "# gathered = list(zip(*batches))\n",
    "# (gathered[0][0])\n",
    "\n",
    "# y_logit = np.concatenate(y_logit, axis=0)\n",
    "# y_true = np.concatenate(y_true, axis=0)\n",
    "# img_paths = np.concatenate(img_paths, axis=0)\n",
    "\n",
    "# print(y_logit.shape,y_true.shape,img_paths.shape)\n",
    "\n",
    "# i = 1200\n",
    "# print(y_logit[i:i+100,:],y_true[i:i+100],img_paths[i:i+100])\n",
    "\n",
    "# np.concatenate\n",
    "\n",
    "# # y_logit = torch.cat(y_logit, dim=0)\n",
    "# # y_true = torch.cat(y_true, dim=0)\n",
    "# # img_paths = torch.cat(img_paths, dim=0)\n",
    "\n",
    "# print(type(results_init),\n",
    "# len(results_init))\n",
    "\n",
    "\n",
    "\n",
    "# # pp(results_init[0][0].shape, results_init[0][1].shape, (results_init[0][2]))\n",
    "# i = 80\n",
    "\n",
    "# pp(results_init[i][0].shape, results_init[i][1], (results_init[i][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61fb966f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#         config.wandb.model_artifact.output_model_path = Path(config.wandb.model_artifact.output_model_dir,\n",
    "#                                                              config.wandb.model_artifact.version,\n",
    "#                                                              f\"epoch_{trainer.current_epoch}\" \\\n",
    "#                                                             + f\"test_macro_acc={test_results['test/acc/top1']}\")\n",
    "#         os.makedirs(os.path.dirname(config.wandb.model_artifact.output_model_path), exist_ok=True)\n",
    "#         model.save_model(config.wandb.model_artifact.output_model_path)\n",
    "\n",
    "\n",
    "#         model_artifact = wandb.Artifact(\n",
    "#                                         config.wandb.model_artifact.name+\"_output\", # ==config.model.name\n",
    "#                                         type=config.wandb.model_artifact.output_type,\n",
    "#                                         description=config.wandb.model_artifact.description,\n",
    "#                                         metadata=dict(version=config.wandb.model_artifact.version,\n",
    "#                                                       **config.model)\n",
    "#                                         )\n",
    "#         model_artifact.add_dir(os.path.dirname(config.wandb.model_artifact.output_model_path))\n",
    "#         run.log_artifact(model_artifact)\n",
    "\n",
    "\n",
    "#     log_predictions(trainer, model, datamodule, subset = \"test\", fix_catalog_number=fix_catalog_number)\n",
    "    \n",
    "#     run.finish()\n",
    "    \n",
    "#     return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77836bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # path = \"/media/data/jacob/wandb_cache/artifacts/models/resnet50:v0/trained/best.ckpt\"\n",
    "    \n",
    "# # loaded = model.load_state_dict(torch.load(path))\n",
    "\n",
    "\n",
    "\n",
    "# # for n, m in model.classifier.named_modules():\n",
    "# #     print(n, type(m))\n",
    "# #     for p in m.parameters():\n",
    "\n",
    "# for n, p in model.classifier.named_parameters():\n",
    "#     print(n, p.shape, p.norm().detach())\n",
    "    \n",
    "    \n",
    "    \n",
    "# model.reset_classifier(config.model.num_classes,'avg')\n",
    "\n",
    "# for n, p in model.classifier.named_parameters():\n",
    "#     print(n, p.shape, p.norm().detach())\n",
    "\n",
    "\n",
    "# loaded = torch.load(\"/media/data/jacob/wandb_cache/artifacts/models/resnet50:v0/trained/best.ckpt\")\n",
    "# loaded.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62c68c62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.load_model(Path(config.wandb.model_artifact.init_model_dir,\n",
    "#                       config.wandb.model_artifact.version))\n",
    "\n",
    "# # log initial model before training (for illustration purposes)\n",
    "# model_artifact.add_dir(config.wandb.model_artifact.init_model_dir)\n",
    "# run.log_artifact(model_artifact)\n",
    "\n",
    "# train_dataloader = datamodule.train_dataloader()\n",
    "# batch_x, batch_y = next(iter(train_dataloader))\n",
    "# print(batch_x.shape, batch_y.shape)\n",
    "\n",
    "# datamodule.batch_size = 48\n",
    "# train_dataloader = datamodule.train_dataloader()\n",
    "# batch_x, batch_y = next(iter(train_dataloader))\n",
    "# print(batch_x.shape, batch_y.shape)\n",
    "\n",
    "# batch_x.shape\n",
    "\n",
    "# batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc33e0af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # EXPERIMENT CONFIG\n",
    "# #------------------------\n",
    "# # Core globals to modify\n",
    "# # hyperparams set low for demo/training speed\n",
    "# # if you set these higher, be mindful of how many items are in\n",
    "# # the dataset artifacts you chose by setting the SIZE at the top\n",
    "# NUM_TRAIN = BALANCED_SPLITS[\"train\"]*10\n",
    "# NUM_VAL = BALANCED_SPLITS[\"val\"]*10\n",
    "# NUM_EPOCHS = 3 # set low for demo purposes, try 3, 5, or as many as you like\n",
    "# NUM_LOG_BATCHES = 16 # enforced max for this is ceil(NUM_VAL/batch_size)\n",
    "\n",
    "# # ARTIFACTS CONFIG\n",
    "# #------------------------\n",
    "# TRAIN_DATA_AT = PREFIX + \"_80-10-10_\" + str(TOTAL_IMAGES) # training data artifact to load\n",
    "\n",
    "# MODEL_NAME = \"iv3_trained\"\n",
    "# INIT_MODEL_DIR = \"init_model_keras_iv3\" # folder in which to save initial, untrained model\n",
    "# FINAL_MODEL_DIR = \"trained_keras_model_iv3\" # folder in which to save the final, trained model\n",
    "\n",
    "# CFG = {\n",
    "#   \"num_train\" : NUM_TRAIN,\n",
    "#   \"num_val\" : NUM_VAL,\n",
    "#   \"num_classes\" : 10,\n",
    "#   \"fc_size\" : 1024,\n",
    "#   \"epochs\" : NUM_EPOCHS,\n",
    "#   \"batch_size\" : 32,\n",
    "#   # inceptionV3 settings\n",
    "#   \"img_width\" : 299,\n",
    "#   \"img_height\": 299\n",
    "# }\n",
    "\n",
    "\n",
    "# max_log_batches = int(np.ceil(float(CFG[\"num_val\"])/float(CFG[\"batch_size\"]))) # number of validation data batches to log/use when computing metrics at the end of each epoch\n",
    "# CFG[\"num_log_batches\"] = min(max_log_batches, NUM_LOG_BATCHES)\n",
    "\n",
    "# from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "# from tensorflow.keras.callbacks import Callback\n",
    "# from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from wandb.keras import WandbCallback\n",
    "\n",
    "\n",
    "# def finetune_inception_model(fc_size, num_classes):\n",
    "#     \"\"\"Load InceptionV3 with ImageNet weights, freeze it,\n",
    "#     and attach a finetuning top for this classification task\"\"\"\n",
    "#     # load InceptionV3 as base\n",
    "#     base = InceptionV3(weights=\"imagenet\", include_top=\"False\")\n",
    "#     # freeze base layers\n",
    "#     for layer in base.layers:\n",
    "#         layer.trainable = False\n",
    "#     x = base.get_layer('mixed10').output \n",
    "\n",
    "#     # attach a fine-tuning layer\n",
    "#     x = GlobalAveragePooling2D()(x)\n",
    "#     x = Dense(fc_size, activation='relu')(x)\n",
    "#     guesses = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "#     model = Model(inputs=base.input, outputs=guesses)\n",
    "#     model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# def train():\n",
    "\n",
    "#     \"\"\" Main training loop which freezes the InceptionV3 layers of the model\n",
    "#     and only trains the new top layers on the new data. A subsequent training\n",
    "#     phase might unfreeze all the layers and finetune the whole model on the new data\"\"\" \n",
    "#     run = wandb.init(project=PROJECT_NAME, job_type=\"train\", config=CFG)\n",
    "#     cfg = wandb.config\n",
    "\n",
    "#     # locate and download training and validation data\n",
    "#     data_at = TRAIN_DATA_AT + \":latest\"\n",
    "#     data = run.use_artifact(data_at, type=\"balanced_data\")\n",
    "#     data_dir = data.download()\n",
    "#     train_dir = os.path.join(data_dir, \"train\")\n",
    "#     val_dir = os.path.join(data_dir, \"val\")\n",
    "\n",
    "#     # create train and validation data generators\n",
    "#     train_datagen = ImageDataGenerator(\n",
    "#                                        rescale=1. / 255,\n",
    "#                                        shear_range=0.2,\n",
    "#                                        zoom_range=0.2,\n",
    "#                                        horizontal_flip=True)\n",
    "#     val_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "#     train_generator = train_datagen.flow_from_directory(\n",
    "#                                                         train_dir,\n",
    "#                                                         target_size=(cfg.img_width, cfg.img_height),\n",
    "#                                                         batch_size=cfg.batch_size,\n",
    "#                                                         class_mode='categorical')\n",
    "\n",
    "#     val_generator = val_datagen.flow_from_directory(\n",
    "#                                                     val_dir,\n",
    "#                                                     target_size=(cfg.img_width, cfg.img_height),\n",
    "#                                                     batch_size=cfg.batch_size,\n",
    "#                                                     class_mode='categorical',\n",
    "#                                                     shuffle=False)\n",
    "\n",
    "#     # instantiate model and callbacks\n",
    "#     model = finetune_inception_model(cfg.fc_size, cfg.num_classes)\n",
    "\n",
    "#     # log initial model before training (for illustration purposes)\n",
    "#     model_artifact = wandb.Artifact(\n",
    "#                                     \"iv3\", type=\"model\",\n",
    "#                                     description=\"unmodified inception v3\",\n",
    "#                                     metadata=dict(cfg))\n",
    "#     model.save(INIT_MODEL_DIR)\n",
    "#     model_artifact.add_dir(INIT_MODEL_DIR)\n",
    "#     run.log_artifact(model_artifact)\n",
    "#     callbacks = [WandbCallback(), ValLog(val_generator, cfg.num_log_batches)]\n",
    "\n",
    "#     # train!\n",
    "#     model.fit(\n",
    "#               train_generator,\n",
    "#               steps_per_epoch = cfg.num_train // cfg.batch_size,\n",
    "#               epochs=cfg.epochs,\n",
    "#               validation_data=val_generator,\n",
    "#               callbacks = callbacks,\n",
    "#               validation_steps = cfg.num_val // cfg.batch_size)\n",
    "\n",
    "#     # save trained model as artifact\n",
    "#     trained_model_artifact = wandb.Artifact(\n",
    "#             MODEL_NAME, type=\"model\",\n",
    "#             description=\"trained inception v3\",\n",
    "#             metadata=dict(cfg))\n",
    "\n",
    "#     model.save(FINAL_MODEL_DIR)\n",
    "#     trained_model_artifact.add_dir(FINAL_MODEL_DIR)\n",
    "#     run.log_artifact(trained_model_artifact)\n",
    "#     run.finish()\n",
    "\n",
    "# class ValLog(Callback):\n",
    "#   \"\"\" Custom callback to log validation images\n",
    "#   at the end of each training epoch\"\"\"\n",
    "#   def __init__(self, generator=None, num_log_batches=1):\n",
    "#     self.generator = generator\n",
    "#     self.num_batches = num_log_batches\n",
    "#     # store full names of classes\n",
    "#     self.flat_class_names = [k for k, v in generator.class_indices.items()]\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         # collect validation data and ground truth labels from generator\n",
    "#         val_data, val_labels = zip(*(self.generator[i] for i in range(self.num_batches)))\n",
    "#         val_data, val_labels = np.vstack(val_data), np.vstack(val_labels)\n",
    "\n",
    "#         # use the trained model to generate predictions for the given number\n",
    "#         # of validation data batches (num_batches)\n",
    "#         val_preds = self.model.predict(val_data)\n",
    "#         true_ids = val_labels.argmax(axis=1)\n",
    "#         max_preds = val_preds.argmax(axis=1)\n",
    "\n",
    "#         # save validation predictions as an artifact\n",
    "#         val_res_at = wandb.Artifact(\"val_pred_\" + wandb.run.id, \"val_epoch_preds\")\n",
    "#         columns=[\"id\", \"image\", \"guess\", \"truth\"]\n",
    "#         for a in self.flat_class_names:\n",
    "#           columns.append(\"score_\" + a)\n",
    "#         val_dt = wandb.Table(columns = columns)\n",
    "\n",
    "#         # log image, predicted and actual labels, and all scores\n",
    "#         for filepath, img, top_guess, scores, truth in zip(self.generator.filenames,\n",
    "#                                                            val_data, \n",
    "#                                                            max_preds, \n",
    "#                                                            val_preds,\n",
    "#                                                            true_ids):\n",
    "#           img_id = filepath.split('/')[-1].split(\".\")[0]\n",
    "#           row = [img_id, wandb.Image(img), \n",
    "#                  self.flat_class_names[top_guess], self.flat_class_names[truth]]\n",
    "#           for s in scores.tolist():\n",
    "#             row.append(np.round(s, 4))\n",
    "#           val_dt.add_data(*row)\n",
    "#         val_res_at.add(val_dt, \"val_epoch_res\")\n",
    "#         wandb.run.log_artifact(val_res_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02269348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_lightning.loggers import TestTubeLogger\n",
    "# logger = TestTubeLogger('.', create_git_tag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83bd34e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92b32cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b467f8b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3a1e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f765f9b0",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Previous torchmetrics scratch code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a02a91f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification.stat_scores import StatScores\n",
    "from torchmetrics.classification.precision_recall import Precision, Recall\n",
    "from torch import Tensor, tensor\n",
    "\n",
    "_mc_k_target = tensor([0, 1, 2, 1])\n",
    "_mc_k_pred = tensor([[0.35, 0.4, 0.25], [0.1, 0.5, 0.4], [0.2, 0.1, 0.7], [0.2, 0.3, 0.5]])\n",
    "_mc_k_pred_2 = tensor([[0.2, 0.4, 0.4], [0.1, 0.5, 0.4], [0.2, 0.3, 0.5], [0.48, 0.27, 0.25]])\n",
    "\n",
    "pred, target = _mc_k_pred, _mc_k_target\n",
    "pred_2 = _mc_k_pred_2\n",
    "\n",
    "####################\n",
    "\n",
    "num_classes=3\n",
    "reduce=None#'macro'\n",
    "\n",
    "stat_metrics = StatScores(num_classes=num_classes, reduce='macro')#reduce)\n",
    "precision = Precision(num_classes=num_classes, top_k=1, average=reduce)\n",
    "recall = Recall(num_classes=num_classes, top_k=1, average=reduce)\n",
    "\n",
    "def update_metrics(pred, target):\n",
    "    return [stat_metrics(pred, target), precision(pred, target), recall(pred, target)]\n",
    "\n",
    "def compute_metrics():\n",
    "    return [stat_metrics.compute(), precision.compute(), recall.compute()]\n",
    "\n",
    "\n",
    "batch_stat_metrics, batch_precision, batch_recall = update_metrics(pred, target)\n",
    "\n",
    "batch_stat_metrics, batch_precision, batch_recall = update_metrics(pred, target)\n",
    "batch_stat_metrics, batch_precision, batch_recall = update_metrics(pred_2, target)\n",
    "\n",
    "print('OFFICIAL torchmetrics\\n')\n",
    "print('batch:')\n",
    "print(batch_stat_metrics, batch_precision, batch_recall,'\\n')\n",
    "epoch_metrics = compute_metrics()\n",
    "print('epoch:\\n',\n",
    "      f'stat_metrics:\\n{epoch_metrics[0]},\\n',\n",
    "      f'precision:\\n{epoch_metrics[1]},\\n',\n",
    "      f'recall:\\n{epoch_metrics[2]}')\n",
    "print('=='*10)\n",
    "print('TEST manual metrics\\n')\n",
    "print('epoch:')\n",
    "for i in range(len(epoch_metrics[0])):\n",
    "    tp, fp, tn, fn, total = epoch_metrics[0][i]\n",
    "    epoch_precision = tp / (tp + fp)\n",
    "    epoch_recall = tp / (tp + fn)\n",
    "    print(f'precision: {epoch_precision}')\n",
    "    print(f'recall: {epoch_recall}')\n",
    "\n",
    "# for class_i in zip(stat_metrics, precision, recall):\n",
    "#     print(class_i)\n",
    "\n",
    "def print_stats(tp, fp, tn, fn, total):\n",
    "    recall = tp / (total)\n",
    "    print(f\"tp:{tp}, fp:{fp}, tn:{tn}, fn:{fn}, total:{total}\")\n",
    "    print(f\"recall:{recall}\")\n",
    "\n",
    "print(target.shape, preds.shape)\n",
    "\n",
    "sum_preds = torch.zeros_like(preds[0,:])\n",
    "count_preds = torch.zeros_like(preds[0,:])\n",
    "print(sum_preds, count_preds)\n",
    "\n",
    "batch_results = stat_metrics(preds, target)\n",
    "print(batch_results)\n",
    "epoch_results = stat_metrics.compute()\n",
    "for tp, fp, tn, fn, total in epoch_results:\n",
    "    print_stats(tp, fp, tn, fn, total)\n",
    "    sum_preds += preds.sum(dim=0)\n",
    "    count_preds += preds.shape[0]\n",
    "    \n",
    "print(f'Avg preds: {sum_preds/count_preds}')\n",
    "    \n",
    "\n",
    "batch_results = stat_metrics(preds_2, target)\n",
    "print(batch_results)\n",
    "epoch_results = stat_metrics.compute()\n",
    "for tp, fp, tn, fn, total in epoch_results:\n",
    "    print_stats(tp, fp, tn, fn, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fe4097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ca099e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9ebafd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# batch_results = stat_metrics(preds, target)\n",
    "\n",
    "# for tp, fp, tn, fn, totals in batch_results:\n",
    "#     recall = tp / (totals)\n",
    "    \n",
    "#     print(f\"tp:{tp}, fp:{fp}, tn:{tn}, fn:{fn}, totals:{totals}\")\n",
    "#     print(f\"recall:{recall}\")\n",
    "\n",
    "# print(stat_metrics.compute())\n",
    "\n",
    "# datamodule.train_dataset\n",
    "\n",
    "# datamodule.train_dataset[0][0].shape\n",
    "\n",
    "# from rich import inspect\n",
    "\n",
    "# train_dataloader = datamodule.train_dataloader()\n",
    "# train_dataset = datamodule.train_dataset\n",
    "\n",
    "# classes = train_dataset.classes\n",
    "# # train_dataset.sample_params\n",
    "# train_dataset.samples[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb812b1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if SIZE == \"TINY\":\n",
    "#     src_url = \"https://storage.googleapis.com/wandb_datasets/nature_100.zip\"\n",
    "#     src_zip = \"nature_100.zip\"\n",
    "#     DATA_SRC = \"nature_100\"\n",
    "#     IMAGES_PER_LABEL = 10\n",
    "#     BALANCED_SPLITS = {\"train\" : 8, \"val\" : 1, \"test\": 1}\n",
    "# elif SIZE == \"MEDIUM\":\n",
    "#     src_url = \"https://storage.googleapis.com/wandb_datasets/nature_1K.zip\"\n",
    "#     src_zip = \"nature_1K.zip\"\n",
    "#     DATA_SRC = \"nature_1K\"\n",
    "#     IMAGES_PER_LABEL = 100\n",
    "#     BALANCED_SPLITS = {\"train\" : 80, \"val\" : 10, \"test\": 10}\n",
    "# elif SIZE == \"LARGE\":\n",
    "#     src_url = \"https://storage.googleapis.com/wandb_datasets/nature_12K.zip\"\n",
    "#     src_zip = \"nature_12K.zip\"\n",
    "#     DATA_SRC = \"inaturalist_12K/train\" # (technically a subset of only 10K images)\n",
    "#     IMAGES_PER_LABEL = 1000\n",
    "#     BALANCED_SPLITS = {\"train\" : 800, \"val\" : 100, \"test\": 100}\n",
    "\n",
    "# %%capture\n",
    "# !curl -SL $src_url > $src_zip\n",
    "# !unzip $src_zip\n",
    "\n",
    "\n",
    "\n",
    "# import os\n",
    "# from random import shuffle\n",
    "# import numpy as np\n",
    "# import wandb\n",
    "\n",
    "# # source directory for all raw data\n",
    "# SRC = DATA_SRC\n",
    "# PREFIX = \"inat\" # convenient for tracking local data\n",
    "# PROJECT_NAME = \"dsviz_demo\"\n",
    "\n",
    "# # number of images per class label\n",
    "# # the total number of images is 10X this (10 classes)\n",
    "# TOTAL_IMAGES = IMAGES_PER_LABEL * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47560c44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# os.path.join(*[config.wandb.artifact_root_dir,\"models\",config.model.name])\n",
    "\n",
    "# import wandb\n",
    "# api = wandb.Api()\n",
    "# artifact = api.artifact(config.wandb.input_artifact.uri)\n",
    "# artifact\n",
    "\n",
    "# print(config.wandb.input_artifact.uri)\n",
    "\n",
    "# run = wandb.init(**config.wandb.init)\n",
    "\n",
    "# artifact = run.use_artifact(artifact)#, type=config.wandb.input_artifact.type)\n",
    "\n",
    "# artifact_dir = artifact.download()\n",
    "# os.path.abspath(artifact_dir)\n",
    "# # files = artifact.file()\n",
    "\n",
    "# print(config.wandb.input_artifact.uri)\n",
    "\n",
    "# wandb.finish()\n",
    "\n",
    "# os.environ[\"WANDB_PROJECT\"] = config.wandb.init.project\n",
    "\n",
    "# import wandb\n",
    "# run = wandb.init(entity='jrose', project=config.wandb.init.project, job_type=config.wandb.init.job_type, config=config)\n",
    "# artifact = run.use_artifact('jrose/image_classification_datasets/PNAS_family_100_1024:v0', type='raw_data')\n",
    "# artifact_dir = artifact.download()\n",
    "# artifact_dir = os.path.abspath(artifact_dir)\n",
    "\n",
    "\n",
    "# config.model_name = 'resnet50'\n",
    "# config.dataset_name = 'PNAS_family_100_1024'\n",
    "# # config.dataset_name = 'Extant_family_10_512'\n",
    "# config.normalize = True\n",
    "# config.num_workers = 4\n",
    "# config.batch_size = 48\n",
    "# config.debug=False\n",
    "\n",
    "# from box import Box\n",
    "\n",
    "\n",
    "# data_config = Box(dict(name=\"PNAS_family_100_1024\",\n",
    "#                    batch_size=32,\n",
    "# #                    val_split=0.2,\n",
    "#                    num_workers=4,\n",
    "#                    seed=389,\n",
    "#                    debug=False,\n",
    "#                    normalize=True,\n",
    "#                    image_size=1024,\n",
    "#                    channels=3,\n",
    "#                    dataset_dir=artifact_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a9627b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # [source] https://wandb.ai/stacey/evalserver_answers_2/runs/2nrnmyk8/code\n",
    "\n",
    "\n",
    "# # Given the per-class metrics, compute the per-category metrics\n",
    "# # (aggregate over the component classes, e.g. category human = class person and class rider)\n",
    "# def per_category_metrics(ious, fps, fns):\n",
    "#   metrics = {}\n",
    "#   for metric_name, metric_type in zip([\"iou\", \"fps\", \"fns\"], [ious, fps, fns]):\n",
    "#     for category_name, ids in util.CITYSCAPE_IDS.items():\n",
    "#       category_metric = np.mean([metric_type[class_id] for class_id in ids])\n",
    "#       metrics[\"category/\" + metric_name + \"_\" + category_name] = category_metric\n",
    "#   # average across categories, per metric\n",
    "#   for metric in [\"category/iou\", \"category/fps\", \"category/fns\"]:\n",
    "#     metric_vals = [v for k, v in metrics.items() if k.startswith(metric)]\n",
    "#     metrics[\"mean_category/\" + metric.split(\"/\")[1]] = np.mean(metric_vals)\n",
    "#   return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb841f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # if this is a substantially different dataset, give it a new name\n",
    "# # this will create a whole new placeholder (Artifact) for this split\n",
    "# # instead of just incrementing a version of the old data split\n",
    "# run = wandb.init(project=PROJECT_NAME, job_type=\"data_split\")\n",
    "\n",
    "# SPLIT_DATA_AT = \"_\".join([PREFIX, \"80-10-10\", str(TOTAL_IMAGES)])\n",
    "\n",
    "# # create balanced train, val, test splits\n",
    "# # each count is the number of images per label\n",
    "# SPLIT_COUNTS = BALANCED_SPLITS\n",
    "\n",
    "# # find the most recent (\"latest\") version of the full raw data\n",
    "# # you can of course pass around programmatic aliases and not string literals\n",
    "# # note: RAW_DATA_AT is defined in the previous cellif you're running\n",
    "# # just this step, you may need to hardcode it\n",
    "# data_at = run.use_artifact(RAW_DATA_AT + \":latest\")\n",
    "# # download it locally (for illustration purposes/across hardware; you can\n",
    "# # also sync/version artifacts by reference)\n",
    "# data_dir = data_at.download()\n",
    "\n",
    "# # Create new dataset to use in training\n",
    "\n",
    "# data_split_at = wandb.Artifact(SPLIT_DATA_AT, type=\"balanced_data\")\n",
    "# # create a table with columns we want to track/compare\n",
    "# preview_dt = wandb.Table(columns=[\"id\", \"image\", \"label\", \"split\"])\n",
    "\n",
    "# labels = os.listdir(data_dir)\n",
    "# for l in labels:\n",
    "#   if l.startswith(\".\"): # skip non-label file\n",
    "#     continue\n",
    "#   imgs_per_label = os.listdir(os.path.join(data_dir, l))\n",
    "#   shuffle(imgs_per_label)\n",
    "#   start_id = 0\n",
    "#   for split, count in SPLIT_COUNTS.items():\n",
    "#     # take a subset\n",
    "#     split_imgs = imgs_per_label[start_id:start_id+count]\n",
    "#     for img_file in split_imgs:\n",
    "#       f_id = img_file.split(\".\")[0]\n",
    "#       full_path = os.path.join(data_dir, l, img_file)\n",
    "#       # add file to artifact by full path\n",
    "#       # note: pass the label to the name parameter to retain it in\n",
    "#       # the data structure \n",
    "#       data_split_at.add_file(full_path, name = os.path.join(split, l, img_file))\n",
    "#       # add a preview of the image\n",
    "#       if split != \"test\":\n",
    "#         preview_dt.add_data(f_id, wandb.Image(full_path), l, split)\n",
    "#     start_id += count\n",
    "\n",
    "# # log artifact to W&B\n",
    "# data_split_at.add(preview_dt, \"data_split\")\n",
    "# run.log_artifact(data_split_at)\n",
    "# run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a29667",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Dataset creation workflow outsourced to script in prj_contrastive_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23a72a34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Register PNAS dataset to WandB\n",
    "\n",
    "# data_config = dict(name=\"PNAS_family_100_1024\",\n",
    "#                    batch_size=32,\n",
    "#                    val_split=0.2,\n",
    "#                    num_workers=4,\n",
    "#                    seed=389,\n",
    "#                    debug=False,\n",
    "#                    normalize=True,\n",
    "#                    image_size=1024,\n",
    "#                    channels=3)\n",
    "\n",
    "# wandb.init(project=\"image_classification_datasets\",\n",
    "#            job_type='create-dataset',\n",
    "#            config=data_config)\n",
    "\n",
    "# dataset_artifact = wandb.Artifact(data_config[\"name\"],\n",
    "#                                   type='raw_data')\n",
    "\n",
    "# datamodule = PNASLightningDataModule(**data_config)\n",
    "# datamodule.setup(stage='fit')\n",
    "# datamodule.setup(stage='test')\n",
    "\n",
    "# # classes = datamodule.classes\n",
    "\n",
    "# register_raw_dataset(dataset=datamodule.train_dataset,\n",
    "#                      artifact=dataset_artifact,\n",
    "#                      subset='train',\n",
    "#                      fix_catalog_number=True)\n",
    "\n",
    "\n",
    "# register_raw_dataset(dataset=datamodule.val_dataset,\n",
    "#                      artifact=dataset_artifact,\n",
    "#                      subset='val',\n",
    "#                      fix_catalog_number=True)\n",
    "\n",
    "\n",
    "# register_raw_dataset(dataset=datamodule.test_dataset,\n",
    "#                      artifact=dataset_artifact,\n",
    "#                      subset='test',\n",
    "#                      fix_catalog_number=True)\n",
    "\n",
    "# wandb.log_artifact(dataset_artifact)\n",
    "\n",
    "# wandb.finish()\n",
    "\n",
    "########################################\n",
    "########################################\n",
    "\n",
    "## Register Extant dataset to WandB\n",
    "\n",
    "# %%time\n",
    "\n",
    "# data_config = dict(name=\"Extant_family_10_512\",\n",
    "#                    batch_size=32,\n",
    "#                    val_split=0.2,\n",
    "#                    num_workers=4,\n",
    "#                    seed=5363,\n",
    "#                    debug=False,\n",
    "#                    normalize=True,\n",
    "#                    image_size=512,\n",
    "#                    channels=3)\n",
    "\n",
    "# wandb.init(project=\"image_classification_datasets\",job_type='create-dataset', config=data_config)\n",
    "# dataset_artifact = wandb.Artifact(data_config[\"name\"], type='raw_data')\n",
    "\n",
    "# datamodule = ExtantLightningDataModule(**data_config)\n",
    "# datamodule.setup(stage='fit')\n",
    "# datamodule.setup(stage='test')\n",
    "\n",
    "# train_dataloader = datamodule.train_dataloader()\n",
    "# train_dataset = datamodule.train_dataset\n",
    "# val_dataset = datamodule.val_dataset\n",
    "# test_dataset = datamodule.test_dataset\n",
    "# classes = datamodule.classes\n",
    "\n",
    "# register_raw_dataset(dataset=train_dataset,\n",
    "#                      artifact=dataset_artifact,\n",
    "#                      subset='train')\n",
    "\n",
    "\n",
    "# register_raw_dataset(dataset=val_dataset,\n",
    "#                      artifact=dataset_artifact,\n",
    "#                      subset='val')\n",
    "\n",
    "\n",
    "# register_raw_dataset(dataset=test_dataset,\n",
    "#                      artifact=dataset_artifact,\n",
    "#                      subset='test')\n",
    "\n",
    "# wandb.log_artifact(dataset_artifact)\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012c119d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# # if this is a substantially new dataset, give it a new name\n",
    "# # this will create a whole new placeholder (Artifact) for this dataset\n",
    "# # instead of just incrementing a version of the old dataset\n",
    "# RAW_DATA_AT = \"_\".join([PREFIX, \"raw_data\", str(TOTAL_IMAGES)])\n",
    "# run = wandb.init(project=PROJECT_NAME, job_type=\"upload\")\n",
    "# # create an artifact for all the raw data\n",
    "# raw_data_at = wandb.Artifact(RAW_DATA_AT, type=\"raw_data\")\n",
    "\n",
    "# # SRC_DIR contains 10 folders, one for each of 10 class labels\n",
    "# # each folder contains images of the corresponding class\n",
    "# labels = os.listdir(SRC)\n",
    "# for l in labels:\n",
    "#   imgs_per_label = os.path.join(SRC, l)\n",
    "#   if os.path.isdir(imgs_per_label):\n",
    "#     # filter out \"DS_Store\"\n",
    "#     imgs = [i for i in os.listdir(imgs_per_label) if not i.startswith(\".DS\")]\n",
    "#     # randomize the order\n",
    "#     shuffle(imgs)\n",
    "#     img_file_ids = imgs[:IMAGES_PER_LABEL]\n",
    "#     for f in img_file_ids:\n",
    "#       file_path = os.path.join(SRC, l, f)\n",
    "#       # add file to artifact by full path\n",
    "#       raw_data_at.add_file(file_path, name=l + \"/\" + f)\n",
    "\n",
    "# # save artifact to W&B\n",
    "# run.log_artifact(raw_data_at)\n",
    "# run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
