{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be77af4c",
   "metadata": {},
   "source": [
    "# pytorch metrics playground\n",
    "\n",
    "Author: Jacob A Rose  \n",
    "Created: Monday, April 19th, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf4a4a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# InteractiveShell.ast_node_interactivity = \"last_expr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86682315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2b4c72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'image.png'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'fam/image.png'\n",
    "\n",
    "Path(path).name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f08db38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 9\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import os\n",
    "import types\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "import torch\n",
    "import timm\n",
    "from rich import print\n",
    "import matplotlib.pyplot as plt\n",
    "from contrastive_learning.data.pytorch.pnas import PNASLightningDataModule\n",
    "from contrastive_learning.data.pytorch.extant import ExtantLightningDataModule\n",
    "from contrastive_learning.data.pytorch.common import DataStageError, colorbar\n",
    "\n",
    "from lightning_hydra_classifiers.callbacks.wandb_callbacks import WatchModelWithWandb, LogPerClassMetricsToWandb, WandbClassificationCallback # LogConfusionMatrixToWandb\n",
    "from lightning_hydra_classifiers.models.resnet import ResNet, get_scalar_metrics\n",
    "import lightning_hydra_classifiers\n",
    "from torch import nn\n",
    "import inspect\n",
    "\n",
    "import wandb\n",
    "pl.trainer.seed_everything(seed=9)\n",
    "\n",
    "    \n",
    "class Config:\n",
    "    pass\n",
    "\n",
    "\n",
    "config = Config()\n",
    "\n",
    "config.model_name = 'resnet50'\n",
    "# config.dataset_name = 'PNAS_family_100_512'\n",
    "config.dataset_name = 'Extant_family_10_512'\n",
    "config.normalize = True\n",
    "config.num_workers = 4\n",
    "config.batch_size = 48\n",
    "config.debug=False\n",
    "########################################\n",
    "def get_datamodule(config):\n",
    "    if 'Extant' in config.dataset_name:\n",
    "        datamodule = ExtantLightningDataModule(name=config.dataset_name,\n",
    "                                               batch_size=config.batch_size,\n",
    "                                               debug=config.debug,\n",
    "                                               normalize=config.normalize,\n",
    "                                               num_workers=config.num_workers)\n",
    "    elif 'PNAS' in config.dataset_name:\n",
    "        datamodule = PNASLightningDataModule(name=config.dataset_name,\n",
    "                                             batch_size=config.batch_size,\n",
    "                                             debug=config.debug,\n",
    "                                             normalize=config.normalize,\n",
    "                                             num_workers=config.num_workers)\n",
    "    \n",
    "    return datamodule\n",
    "    \n",
    "datamodule = get_datamodule(config)\n",
    "        \n",
    "datamodule.setup('fit')\n",
    "datamodule.setup('test')\n",
    "########################################\n",
    "num_classes = len(datamodule.classes)\n",
    "config.num_classes = num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff4e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = datamodule.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d0a6a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048 100352\n"
     ]
    }
   ],
   "source": [
    "assert (num_classes == 19) | (num_classes == 179)\n",
    "\n",
    "########################################\n",
    "model = ResNet(model_name=config.model_name, num_classes=config.num_classes)\n",
    "model.reset_classifier(config.num_classes,'avg')\n",
    "model.unfreeze(model.layer4)\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d094c83d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ckpt_path = \"/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/wandb/run-20210508_041808-2dj05hbk/files/default/2dj05hbk/checkpoints/epoch=19-step=11259.ckpt\"\n",
    "# ckpt = torch.load(ckpt_path)\n",
    "# loaded_model = model.load_from_checkpoint(ckpt_path)\n",
    "\n",
    "# test_dataloader = datamodule.test_dataloader()\n",
    "# num_batches = len(test_dataloader)\n",
    "\n",
    "# print(num_batches, \"batches\")\n",
    "# batch = next(iter(test_dataloader))\n",
    "# print(f'batch_size = {batch[0].shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f9992d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImagePredictionLogger(pl.Callback):\n",
    "    def __init__(self, val_samples, num_samples=32):\n",
    "        super().__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.val_imgs, self.val_labels = val_samples\n",
    "        \n",
    "        \n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        # Bring the tensors to CPU\n",
    "        val_imgs = self.val_imgs.to(device=pl_module.device)\n",
    "        val_labels = self.val_labels.to(device=pl_module.device)\n",
    "        # Get model prediction\n",
    "        if pl_module.training:\n",
    "            subset='train'\n",
    "        else:\n",
    "            subset='val'\n",
    "        \n",
    "        \n",
    "        logits = pl_module(val_imgs)\n",
    "        preds = torch.argmax(logits, -1)\n",
    "        # Log the images as wandb Image\n",
    "        trainer.logger.experiment.log({\n",
    "            f\"{subset}/examples\":[wandb.Image(x, caption=f\"Pred:{pred}, Label:{y}\") \n",
    "                           for x, pred, y in zip(val_imgs[:self.num_samples], \n",
    "                                                 preds[:self.num_samples], \n",
    "                                                 val_labels[:self.num_samples])]\n",
    "            })\n",
    "\n",
    "\n",
    "datamodule.batch_size = 32\n",
    "val_dataloader = datamodule.val_dataloader()\n",
    "        \n",
    "# val_dataloader.batch_size = 32\n",
    "image_pred_logger_cb = ImagePredictionLogger(next(iter(val_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a40b843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjrose\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.26<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">Extant_family_10_512-timm-resnet50</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/jrose/default\" target=\"_blank\">https://wandb.ai/jrose/default</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/jrose/default/runs/g2rqiicx\" target=\"_blank\">https://wandb.ai/jrose/default/runs/g2rqiicx</a><br/>\n",
       "                Run data is saved locally in <code>/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/wandb/run-20210514_200721-g2rqiicx</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pl_bolts.callbacks import ModuleDataMonitor, BatchGradientVerificationCallback\n",
    "# verification = BatchGradientVerificationCallback()\n",
    "\n",
    "\n",
    "wandb.init(name=f\"{config.dataset_name}-timm-{config.model_name}\",\n",
    "           group='baselines',\n",
    "           config=config)\n",
    "\n",
    "monitor = ModuleDataMonitor()#log_every_n_steps=25)\n",
    "per_class_metric_plots_cb = LogPerClassMetricsToWandb()\n",
    "early_stop_callback = EarlyStopping(\n",
    "                                    monitor='val_loss',\n",
    "                                    patience=3,\n",
    "                                    verbose=False,\n",
    "                                    mode='min'\n",
    "                                    )\n",
    "\n",
    "logger=pl.loggers.wandb.WandbLogger(name=f\"{config.dataset_name}-timm-{config.model_name}\", config=config)\n",
    "filepath = wandb.run.dir + \"/epoch={epoch:02d}-val_acc={val/acc/top1:.2f}.ckpt\"\n",
    "\n",
    "trainer = pl.Trainer(gpus=1,\n",
    "                     logger=logger,\n",
    "                     max_epochs=40,\n",
    "                     weights_summary='top',#,\n",
    "                     profiler=\"simple\", #\"advanced\", #\n",
    "                     callbacks=[per_class_metric_plots_cb,\n",
    "                                image_pred_logger_cb,\n",
    "                                monitor,\n",
    "                                early_stop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "429b2a2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name          | Type              | Params | In sizes          | Out sizes        \n",
      "---------------------------------------------------------------------------------------------\n",
      "0  | relu          | ReLU              | 0      | [2, 64, 112, 112] | [2, 64, 112, 112]\n",
      "1  | conv1         | Conv2d            | 9.4 K  | [2, 3, 224, 224]  | [2, 64, 112, 112]\n",
      "2  | bn1           | BatchNorm2d       | 128    | [2, 64, 112, 112] | [2, 64, 112, 112]\n",
      "3  | maxpool       | MaxPool2d         | 0      | [2, 64, 112, 112] | [2, 64, 56, 56]  \n",
      "4  | layer1        | Sequential        | 215 K  | [2, 64, 56, 56]   | [2, 256, 56, 56] \n",
      "5  | layer2        | Sequential        | 1.2 M  | [2, 256, 56, 56]  | [2, 512, 28, 28] \n",
      "6  | layer3        | Sequential        | 7.1 M  | [2, 512, 28, 28]  | [2, 1024, 14, 14]\n",
      "7  | layer4        | Sequential        | 15.0 M | [2, 1024, 14, 14] | [2, 2048, 7, 7]  \n",
      "8  | stem          | Sequential        | 9.5 K  | [2, 3, 224, 224]  | [2, 64, 56, 56]  \n",
      "9  | features      | Sequential        | 23.5 M | [2, 64, 56, 56]   | [2, 2048, 7, 7]  \n",
      "10 | global_pool   | AdaptiveAvgPool2d | 0      | [2, 2048, 7, 7]   | [2, 2048, 1, 1]  \n",
      "11 | flatten       | Flatten           | 0      | [2, 2048, 1, 1]   | [2, 2048]        \n",
      "12 | fc            | Linear            | 366 K  | [2, 2048]         | [2, 179]         \n",
      "13 | classifier    | Sequential        | 366 K  | [2, 2048, 7, 7]   | [2, 179]         \n",
      "14 | criterion     | CrossEntropyLoss  | 0      | ?                 | ?                \n",
      "15 | train_metrics | MetricCollection  | 0      | ?                 | ?                \n",
      "16 | val_metrics   | MetricCollection  | 0      | ?                 | ?                \n",
      "17 | test_metrics  | MetricCollection  | 0      | ?                 | ?                \n",
      "---------------------------------------------------------------------------------------------\n",
      "15.3 M    Trainable params\n",
      "8.5 M     Non-trainable params\n",
      "23.9 M    Total params\n",
      "95.499    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf84f70a5ac4d74b0629197d45b03ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  3497.6         \t|  100 %          \t|\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_epoch                 \t|  436.09         \t|8              \t|  3488.8         \t|  99.746         \t|\n",
      "get_train_batch                    \t|  0.70161        \t|3376           \t|  2368.6         \t|  67.721         \t|\n",
      "run_training_batch                 \t|  0.11024        \t|3376           \t|  372.17         \t|  10.641         \t|\n",
      "model_forward                      \t|  0.10999        \t|3376           \t|  371.34         \t|  10.617         \t|\n",
      "training_step_end                  \t|  0.070577       \t|3376           \t|  238.27         \t|  6.8123         \t|\n",
      "training_step                      \t|  0.038791       \t|3376           \t|  130.96         \t|  3.7443         \t|\n",
      "evaluation_step_and_end            \t|  0.10975        \t|850            \t|  93.29          \t|  2.6672         \t|\n",
      "validation_step_end                \t|  0.092546       \t|850            \t|  78.664         \t|  2.2491         \t|\n",
      "optimizer_step_and_closure_0       \t|  0.014997       \t|3376           \t|  50.631         \t|  1.4476         \t|\n",
      "validation_step                    \t|  0.016602       \t|850            \t|  14.111         \t|  0.40346        \t|\n",
      "on_epoch_end                       \t|  0.80303        \t|17             \t|  13.652         \t|  0.39031        \t|\n",
      "on_train_batch_end                 \t|  0.0026877      \t|3376           \t|  9.0738         \t|  0.25943        \t|\n",
      "on_validation_end                  \t|  0.496          \t|9              \t|  4.464          \t|  0.12763        \t|\n",
      "on_validation_batch_end            \t|  0.0036459      \t|850            \t|  3.099          \t|  0.088603       \t|\n",
      "cache_result                       \t|  0.00010827     \t|16160          \t|  1.7496         \t|  0.050023       \t|\n",
      "on_validation_start                \t|  0.033521       \t|9              \t|  0.30169        \t|  0.0086255      \t|\n",
      "on_train_end                       \t|  0.1954         \t|1              \t|  0.1954         \t|  0.0055865      \t|\n",
      "on_batch_start                     \t|  5.6138e-05     \t|3376           \t|  0.18952        \t|  0.0054186      \t|\n",
      "on_train_batch_start               \t|  3.6466e-05     \t|3376           \t|  0.12311        \t|  0.0035198      \t|\n",
      "on_batch_end                       \t|  3.4658e-05     \t|3376           \t|  0.117          \t|  0.0033453      \t|\n",
      "on_train_start                     \t|  0.070889       \t|1              \t|  0.070889       \t|  0.0020268      \t|\n",
      "on_validation_batch_start          \t|  4.1581e-05     \t|850            \t|  0.035343       \t|  0.0010105      \t|\n",
      "on_train_epoch_start               \t|  0.0032636      \t|8              \t|  0.026109       \t|  0.00074648     \t|\n",
      "on_epoch_start                     \t|  4.3273e-05     \t|17             \t|  0.00073564     \t|  2.1033e-05     \t|\n",
      "on_train_epoch_end                 \t|  8.9357e-05     \t|8              \t|  0.00071486     \t|  2.0438e-05     \t|\n",
      "on_validation_epoch_end            \t|  7.4354e-05     \t|9              \t|  0.00066919     \t|  1.9133e-05     \t|\n",
      "on_validation_epoch_start          \t|  2.9196e-05     \t|9              \t|  0.00026277     \t|  7.5128e-06     \t|\n",
      "on_fit_start                       \t|  4.2623e-05     \t|1              \t|  4.2623e-05     \t|  1.2186e-06     \t|\n",
      "on_before_accelerator_backend_setup\t|  2.1233e-05     \t|1              \t|  2.1233e-05     \t|  6.0707e-07     \t|\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd08bae738bd44fc92f00fcd4be87c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test/acc/top1': 0.5878283977508545,\n",
      " 'test/acc/top3': 0.7602733969688416,\n",
      " 'test/precision/top1': 0.4063551723957062,\n",
      " 'test/recall/top1': 0.3104079067707062,\n",
      " 'test_loss': 2.1013941764831543}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/wandb/run-20210514_200721-g2rqiicx</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/files/default/g2rqiicx/checkpoints/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">epoch</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>-<span style=\"color: #808000; text-decoration-color: #808000\">step</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2109.</span>ckpt <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6964671</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[35m/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/wandb/run-20210514_200721-g2rqiicx\u001b[0m\n",
       "\u001b[35m/files/default/g2rqiicx/checkpoints/\u001b[0m\u001b[95mepoch\u001b[0m=\u001b[1;36m4\u001b[0m-\u001b[33mstep\u001b[0m=\u001b[1;36m2109\u001b[0m\u001b[1;36m.\u001b[0mckpt \u001b[1;36m0.6964671\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6530<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 174.04MB of 174.04MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/wandb/run-20210514_200721-g2rqiicx/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/wandb/run-20210514_200721-g2rqiicx/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>val/batch_idx</td><td>105</td></tr><tr><td>val/acc/top1</td><td>0.85498</td></tr><tr><td>val/acc/top3</td><td>0.94632</td></tr><tr><td>val/precision/top1</td><td>0.78202</td></tr><tr><td>val/recall/top1</td><td>0.74961</td></tr><tr><td>_runtime</td><td>3814</td></tr><tr><td>_timestamp</td><td>1621041055</td></tr><tr><td>_step</td><td>5572</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>train/batch_idx</td><td>421</td></tr><tr><td>train_loss_step</td><td>0.59332</td></tr><tr><td>trainer/global_step</td><td>3376</td></tr><tr><td>train/acc/top1</td><td>0.83648</td></tr><tr><td>train/acc/top3</td><td>0.95315</td></tr><tr><td>train/precision/top1</td><td>0.82445</td></tr><tr><td>train/recall/top1</td><td>0.78351</td></tr><tr><td>train_loss_epoch</td><td>0.53764</td></tr><tr><td>val_loss_step/epoch_0</td><td>2.23843</td></tr><tr><td>val_loss_epoch</td><td>0.60457</td></tr><tr><td>val_loss_step/epoch_1</td><td>1.97988</td></tr><tr><td>val_loss_step/epoch_2</td><td>1.73948</td></tr><tr><td>val_loss_step/epoch_3</td><td>1.24613</td></tr><tr><td>val_loss_step/epoch_4</td><td>0.69647</td></tr><tr><td>val_loss_step/epoch_5</td><td>1.02477</td></tr><tr><td>val_loss_step/epoch_6</td><td>0.89532</td></tr><tr><td>val_loss_step/epoch_7</td><td>1.02273</td></tr><tr><td>test_loss</td><td>2.10139</td></tr><tr><td>test/acc/top1</td><td>0.58783</td></tr><tr><td>test/acc/top3</td><td>0.76027</td></tr><tr><td>test/precision/top1</td><td>0.40636</td></tr><tr><td>test/recall/top1</td><td>0.31041</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>val/batch_idx</td><td>▁▂▄▆▇▁▃▄▆▇▁▃▄▆▇▁▃▄▆▇▁▃▅▆▇▂▃▄▆█▂▃▅▆█▂▃▅▆█</td></tr><tr><td>val/acc/top1</td><td>▁▂▂▁▃▃▄▃▃▂▆▅▅▆▃▆▅▆▄▅▃▆▆▆▆▆▆▆▆▆▆▅▆█▇▅▆▆▆█</td></tr><tr><td>val/acc/top3</td><td>▂▁▂▁▄▄▄▃▃▃▇▆▅▇▆▅▇▇▆▆▅▆▇█▇▆▆▇▆▆▆▇▆▇▇▆▅▇▇▇</td></tr><tr><td>val/precision/top1</td><td>▁▁▁▂▂▃▂▃▂▁▅▅▄▁▃▆▅▆▄▄▄▅▄█▆▆▄▄▅▆▅▅▇▆▇▅▆▅▇▇</td></tr><tr><td>val/recall/top1</td><td>▁▁▁▂▂▃▃▃▂▂▆▅▅▁▄▇▅▅▄▅▄▅▄█▆▆▅▄▅▆▆▆▇▆▇▅▇▅▇▇</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/batch_idx</td><td>▁▂▄▆▇▁▃▄▆▇▁▃▄▆▇▁▃▄▆▇▁▃▄▆█▁▃▅▆█▂▃▅▆█▂▃▅▆█</td></tr><tr><td>train_loss_step</td><td>███▆▆▅▆▆▅▄▅▃▃▅▃▃▄▃▃▄▂▃▃▃▃▂▃▂▄▄▂▁▂▃▂▁▂▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▁▁▂▂▃▁▃▃▃▁▁▄▄▄▁▅▅▅▅▁▅▆▆▁▁▆▇▇▁▁▇█▁▁███</td></tr><tr><td>train/acc/top1</td><td>▁▃▅▆▆▇██</td></tr><tr><td>train/acc/top3</td><td>▁▄▅▆▇▇██</td></tr><tr><td>train/precision/top1</td><td>▁▂▄▅▆▇██</td></tr><tr><td>train/recall/top1</td><td>▁▂▃▄▆▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▅▄▃▂▂▁▁</td></tr><tr><td>val_loss_step/epoch_0</td><td>▂▇▆▂▇▅▇▁▆▆▅▅█▇▅▇▃▃▂▇▄▃▅▇▅▅▇▃▂▇▅▄▅▇▆▇▆▅▅▆</td></tr><tr><td>val_loss_epoch</td><td>█▆▄▃▂▂▁▁</td></tr><tr><td>val_loss_step/epoch_1</td><td>▄▅▇▃▇▄▇▁▆▅▅▄▇▅▆▆▃▃▃▆▄▃▆█▆▄▅▆▂▅▄▅▄▆▇▅▅▄▅▆</td></tr><tr><td>val_loss_step/epoch_2</td><td>▃▅▆▄▇▅▇▃▅▅▄▆▇▅▃▆▄▃▃▅▆▄▆█▅▅▄▄▁▅▅▄▃▅▆▆▄▄▆▇</td></tr><tr><td>val_loss_step/epoch_3</td><td>▃▅▆▂▆▃▆▂▄▄▄▄█▅▄▅▄▂▂▄▅▃▅▆▅▅▃▄▁▄▃▄▂▅▆▅▅▃▅▅</td></tr><tr><td>val_loss_step/epoch_4</td><td>▃▄▄▃▇▂▇▁▅▃▃▂█▅▄▅▂▂▁▅▄▂▄▇▄▅▄▄▁▅▂▃▂▄▇▆▂▂▄▃</td></tr><tr><td>val_loss_step/epoch_5</td><td>▃▃▇▄▆▂▅▂▆▅▄▄▇▇▃▅▂▁▂▅▇▃▃█▅▇▆▄▁▅▂▂▄▅▇█▂▂▆▆</td></tr><tr><td>val_loss_step/epoch_6</td><td>▄▃▆▃▅▂▄▃▆▄▄▄█▇▂▇▃▁▁▅▆▂▅▇▄▅▃▅▁▄▂▃▂▅▆▆▃▂▄▅</td></tr><tr><td>val_loss_step/epoch_7</td><td>▄▄▅▂▃▂▄▂▅▄▁▄█▆▂▅▂▁▂▄▃▂▃▅▄▄▂▄▁▂▂▃▂▄▅▅▂▁▃▅</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>test/acc/top1</td><td>▁</td></tr><tr><td>test/acc/top3</td><td>▁</td></tr><tr><td>test/precision/top1</td><td>▁</td></tr><tr><td>test/recall/top1</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 956 media file(s), 0 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">Extant_family_10_512-timm-resnet50</strong>: <a href=\"https://wandb.ai/jrose/default/runs/g2rqiicx\" target=\"_blank\">https://wandb.ai/jrose/default/runs/g2rqiicx</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%wandb\n",
    "trainer.fit(model, datamodule)\n",
    "results = trainer.test(model, datamodule=datamodule)\n",
    "# trainer.callbacks[-1].best_model_path\n",
    "print(trainer.callbacks[-1].best_model_path,\n",
    "trainer.callbacks[-1].best_model_score.cpu().numpy())\n",
    "# ckpt_path = \"/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/wandb/run-20210508_041808-2dj05hbk/files/default/2dj05hbk/checkpoints/epoch=19-step=11259.ckpt\"\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134244a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0f4b00c",
   "metadata": {},
   "source": [
    "## Extra: top k metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa6209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_k_highest_loss_images(batch, model, topk=5):\n",
    "    \n",
    "    x, y = batch\n",
    "    model.eval()\n",
    "\n",
    "    logits = model(x)\n",
    "    preds = torch.argmax(logits, -1)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "    loss = loss_fn(logits, y.long())\n",
    "\n",
    "    highest_losses, highest_losses_idx = torch.sort(loss, descending=True)\n",
    "    highest_losses = highest_losses.detach().numpy()\n",
    "    highest_losses_idx = highest_losses_idx.numpy()\n",
    "\n",
    "    \n",
    "    topk_losses  = highest_losses[:topk]\n",
    "    select_idx = highest_losses_idx[:topk]\n",
    "    \n",
    "    y_true = y[select_idx].numpy()\n",
    "    y_prob = logits.softmax(dim=-1)\n",
    "    topk_logits, topk_y_pred = logits.topk(topk)\n",
    "#     topk_y_prob = y_prob[topk_y_pred].detach().numpy()\n",
    "\n",
    "    topk_logits = topk_logits.detach().numpy()\n",
    "    topk_y_pred = topk_y_pred.numpy()\n",
    "    topk_y_prob = [y_prob[i,topk_y_pred[i,:]].detach().numpy() for i in range(len(topk_y_pred))]\n",
    "\n",
    "    topk_pred_labels = [(classes[int(k)] for k in y_pred_i) \\\n",
    "                        for y_pred_i in topk_y_pred]\n",
    "    true_labels = [classes[int(k)] for k in y_true]\n",
    "\n",
    "    \n",
    "    return (topk_losses,\n",
    "            topk_pred_labels,\n",
    "            topk_logits,\n",
    "            topk_y_pred,\n",
    "            topk_y_prob,\n",
    "            true_labels,\n",
    "            y_true,\n",
    "            x,\n",
    "            select_idx)\n",
    "    \n",
    "    \n",
    "topk = 3\n",
    "    \n",
    "topk_losses, topk_pred_labels, topk_logits, topk_y_pred, topk_y_prob, true_labels, y_true, x, select_idx = \\\n",
    "                        select_k_highest_loss_images(batch=batch, model=loaded_model, topk=topk)\n",
    "\n",
    "print(len(topk_pred_labels),\n",
    "      len(topk_logits),\n",
    "      topk_y_pred.shape,\n",
    "      len(topk_y_prob),\n",
    "      len(true_labels),\n",
    "      y_true.shape,\n",
    "      x.shape,\n",
    "     len(select_idx))\n",
    "    \n",
    "    \n",
    "# for i, batch in enumerate(iter(test_dataloader)):\n",
    "#     print(i)\n",
    "#     print(batch[0].shape)\n",
    "#     if i >= num_batches:\n",
    "#         break\n",
    "\n",
    "# logit = logits[i,:]\n",
    "\n",
    "def plot_example_topk_preds(x, logit, y_true, topk, classes, cmap: str='cividis', grayscale=True):\n",
    "    y_true = int(y_true.item())\n",
    "    y_prob = logit.softmax(dim=-1)\n",
    "    topk_logits, topk_y_pred = logit.topk(topk)\n",
    "    topk_y_prob = y_prob[topk_y_pred].detach().numpy()\n",
    "\n",
    "    topk_logits = topk_logits.detach().numpy()\n",
    "    topk_y_pred = topk_y_pred.numpy()\n",
    "\n",
    "    topk_pred_labels = [classes[k] for k in topk_y_pred]\n",
    "    true_label = classes[y_true]\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=(15,10))\n",
    "\n",
    "    img = x.permute(1,2,0)\n",
    "    img_min, img_max = img.min(), img.max()\n",
    "    \n",
    "    if grayscale and len(img.shape)==3:\n",
    "        img = img[:,:,0]    \n",
    "    \n",
    "    img_ax = ax[0].imshow(img, cmap=cmap, vmin = img_min, vmax = img_max)\n",
    "#     colorbar(img_ax)\n",
    "    \n",
    "    bar_x = [*list(topk_pred_labels), \"other\"][::-1]\n",
    "    bar_y = [*list(topk_y_prob), (1 - sum(topk_y_prob))][::-1]\n",
    "    ax[1].barh(bar_x, bar_y)\n",
    "    ax[1].set_xlim([0,1])\n",
    "    plt.suptitle(f'True label: {true_label}')\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "topk_y_pred\n",
    "\n",
    "i = 0\n",
    "topk = 3\n",
    "num_predictions = 5\n",
    "seed = 387\n",
    "batch_size = x.shape[0]\n",
    "\n",
    "\n",
    "topk_losses, topk_pred_labels, topk_logits, topk_y_pred, topk_y_prob, true_labels, y_true, x, select_idx = select_k_highest_loss_images(batch=batch, model=loaded_model, topk=topk)\n",
    "\n",
    "# plt.style.available\n",
    "# plt.style.use('tableau-colorblind10')\n",
    "plt.style.use('seaborn-pastel')\n",
    "\n",
    "class_idx, class_counts = np.unique(datamodule.test_dataset.targets, return_counts=True)\n",
    "sorted_class_counts = np.argsort(class_counts)#, reverse=True)\n",
    "# sorted_class_counts[class_counts[[10,11]]]\n",
    "\n",
    "plt.bar(class_idx, sorted(class_counts, reverse=True), log=True, color='blue')\n",
    "plt.bar([23,56],sorted(class_counts[[23,56]], reverse=True), log=True, color='red')\n",
    "\n",
    "for i in range(len(select_idx)):\n",
    "    plot_example_topk_preds(x[i,...], logit=torch.Tensor(topk_logits[i,:]), y_true=torch.Tensor(y_true)[i], topk=topk, classes=classes)\n",
    "\n",
    "# print(x[i,...].shape, topk_logits[i,:].shape, y_true[i].shape, topk,len(classes))\n",
    "# print(type(x[i,...]), topk_logits[i,:], y_true[i], topk,len(classes))\n",
    "i = 0\n",
    "topk = 3\n",
    "num_predictions = 3\n",
    "seed = 387\n",
    "batch_size = x.shape[0]\n",
    "rng = np.random.default_rng(seed)\n",
    "batch_idx = rng.choice(batch_size, num_predictions, replace=False)\n",
    "print(f'Plotting top {topk} predictions for randomly chosen images at batch indices: {batch_idx}')\n",
    "# for i in select_idx:\n",
    "#     plot_example_topk_preds(x[i,...], logits[i,:], y_true=y[i], topk=topk, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9168c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # _, argsorted_logit = torch.sort(logit, descending=True)\n",
    "# # false_negative_rank = argsorted_logit[y[0]]\n",
    "# print(topk_logits.shape, topk_y_pred.shape)\n",
    "# print(topk_logits, topk_y_pred)\n",
    "\n",
    "# print(f\"True: {true_labels}\")\n",
    "# print(f\"Misclassified by ranking true label {false_negative_rank} out of {num_classes} classes\")\n",
    "# print(f\"Pred Top {topk}: {topk_pred_labels}\")\n",
    "# print(f\"Prob Top {topk}: {topk_y_prob}\")\n",
    "\n",
    "\n",
    "# print(f\"True: {true_labels}\")\n",
    "# print(f\"Misclassified by ranking true label {false_negative_rank} out of {num_classes} classes\")\n",
    "# print(f\"Pred Top {topk}: {topk_pred_labels}\")\n",
    "# print(f\"Prob Top {topk}: {topk_y_prob}\")\n",
    "\n",
    "# num_predictions = 10\n",
    "# print(d[0].shape, d[1].shape)\n",
    "# print(d[0][:num_predictions,:], d[1][:num_predictions,:])\n",
    "\n",
    "# y.view(-1,1).dtype#.shape\n",
    "\n",
    "# preds.float()\n",
    "\n",
    "# x, y = batch\n",
    "# # x, y = x[:1], y[:1]\n",
    "# print(x.shape, y.shape)\n",
    "# logits = loaded_model(x)\n",
    "# preds = torch.argmax(logits, -1)\n",
    "# loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "# print(logits.shape, preds.shape, y.shape, loss.shape)\n",
    "# loss = loss_fn(logits, y.long())#view(-1,1).float())\n",
    "# print(logits.shape, preds.shape, y.shape, loss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba43d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "452ce3d2",
   "metadata": {},
   "source": [
    "## Plot confused images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f66938",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = datamodule.get_batch()\n",
    "plt.imshow(x[0,...].permute(1,2,0))\n",
    "plt.suptitle(f'True label: {datamodule.classes[y[0]]}')\n",
    "\n",
    "y_pred_loaded = loaded(x)\n",
    "\n",
    "num_predictions = 5\n",
    "topk = 3\n",
    "y_prob_topk, y_idx_topk = y_pred_loaded.softmax(dim=1).topk(topk)\n",
    "\n",
    "fig, ax = plt.subplots(3,3, figsize=(15,15))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i in range(num_predictions):\n",
    "    x_i = x[i,...].permute(1,2,0)\n",
    "    x_i = (x_i-x_i.min())/(x_i.max()-x_i.min())\n",
    "    ax[i].imshow(x_i)\n",
    "    title = f\"True: {classes[y[i]]}\\n\"\n",
    "    title += f'Prob: {[classes[idx] for idx in y_idx_topk[i,:]]}'\n",
    "    ax[i].set_title(title)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74295a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_idx_topk.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f8c6b4",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ffa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pl.optim.LightningOptimizer\n",
    "\n",
    "%debug\n",
    "\n",
    "# x, y = datamodule.get_batch()\n",
    "# opt = model.configure_optimizers()\n",
    "\n",
    "# def get_stats(y: torch.Tensor):\n",
    "#     try:\n",
    "#         y = y.detach().numpy()\n",
    "#     except AttributeError:\n",
    "#         pass\n",
    "#     print('shape:', y.shape,\n",
    "#           f'mean: {np.mean(y):.2f}',\n",
    "#           f'std: {np.std(y):.2f}',\n",
    "#           f'min: {np.min(y):.2f}',\n",
    "#           f'max: {np.max(y):.2f}')\n",
    "import json\n",
    "\n",
    "\n",
    "class StatTensor:\n",
    "\n",
    "    def __init__(self, data, decimals=3, *args, **kwargs):\n",
    "#         super().__init__(**kwargs)\n",
    "        self._data = getattr(data, 'data', data)\n",
    "        self.decimals = decimals\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        return self._data\n",
    "\n",
    "    @property\n",
    "    def min(self):\n",
    "        return torch.min(self.data).item()\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return torch.max(self.data).item()\n",
    "\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        self.data += other.data\n",
    "        self.decimals = max([self.decimals, other.decimals])\n",
    "        return self\n",
    "    \n",
    "    def get_stats(self):\n",
    "        data = self.data\n",
    "        stats = {'shape':data.shape,\n",
    "                 'mean':torch.mean(data).item(),\n",
    "                 'std':torch.std(data).item(),\n",
    "                 'min':torch.min(data).item(),\n",
    "                 'max':torch.max(data).item()}\n",
    "\n",
    "        return self._format_stats(stats)\n",
    "    \n",
    "    def _format_stats(self, stats):\n",
    "        for k in list(stats.keys()):\n",
    "            if k == 'shape':\n",
    "                continue\n",
    "            stats[k] = np.round(stats[k], decimals=self.decimals)\n",
    "        return stats\n",
    "\n",
    "    def __repr__(self):\n",
    "        return json.dumps(self.get_stats())\n",
    "    \n",
    "import torchmetrics as metrics\n",
    "    \n",
    "class AverageStatTensor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._data = []\n",
    "        self.mean = metrics.AverageMeter()\n",
    "        self.std = metrics.AverageMeter()\n",
    "        self.min = np.inf\n",
    "        self.max = -np.inf\n",
    "        self.decimals = 3\n",
    "\n",
    "        \n",
    "    @property\n",
    "    def data(self):\n",
    "        return self._data\n",
    "        \n",
    "    def _update_metrics(self, data):\n",
    "        stats = data.get_stats()\n",
    "        self.mean.update(stats['mean'])\n",
    "        self.std.update(stats['std'])\n",
    "        self.min = np.min([self.min, data.min])\n",
    "        self.max = np.max([self.max, data.max])\n",
    "\n",
    "        \n",
    "    def update(self, data):\n",
    "        self._data.append(StatTensor(data, 3))\n",
    "        self._update_metrics(self.data[-1])\n",
    "        return self.compute()\n",
    "    \n",
    "    def _format_stats(self, stats):\n",
    "        for k in list(stats.keys()):\n",
    "            if k == 'shape':\n",
    "                continue\n",
    "            stats[k] = np.round(stats[k], decimals=self.decimals)\n",
    "        return stats\n",
    "    \n",
    "    def compute(self):\n",
    "        return self._format_stats(\n",
    "                {'mean':self.mean.compute().item(),\n",
    "                 'std':self.std.compute().item(),\n",
    "                 'min':self.min,\n",
    "                 'max':self.max})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f58002",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datamodule.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2614ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.rand((3,3))\n",
    "# print(x)\n",
    "# x_avg = AverageStatTensor()\n",
    "# x, y = data[0]\n",
    "# x_avg.update(x)\n",
    "# print(x_avg.compute())\n",
    "# print(x_avg.update(x+1))\n",
    "# # print(x_avg.compute())\n",
    "# print(x_avg.update(x-1-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331c9f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = datamodule.get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d8e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_avg = AverageStatTensor()\n",
    "x, y = data[0]\n",
    "print(x_avg.update(x))\n",
    "\n",
    "\n",
    "x_avg1 = AverageStatTensor()\n",
    "x1, y1 = data[1]\n",
    "print(x_avg1.update(x1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5049f9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_running_stats_from_dataset(data: torch.utils.data.Dataset):\n",
    "    x_avg = AverageStatTensor()\n",
    "\n",
    "    history = {'batch_idx':[],\n",
    "               'mean':[],\n",
    "               'std':[],\n",
    "               'max':[],\n",
    "               'min':[]}\n",
    "\n",
    "    for i, (x, y) in enumerate(data):\n",
    "        stats = x_avg.update(x)\n",
    "\n",
    "        history['batch_idx'].append(i)\n",
    "        history['mean'].append(stats['mean'])\n",
    "        history['std'].append(stats['std'])\n",
    "        history['max'].append(stats['max'])\n",
    "        history['min'].append(stats['min'])\n",
    "\n",
    "    return history\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_history(history, label: str=None, title: str=None, ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1,1)\n",
    "    else:\n",
    "        fig = plt.gcf()\n",
    "        \n",
    "#     if 'c' in kwargs:\n",
    "    c = kwargs['c']\n",
    "#     else:\n",
    "#         c = 'm'\n",
    "\n",
    "        \n",
    "    idx = history['batch_idx']\n",
    "    ax.plot(idx, history['mean'], c+'-', label=label)\n",
    "    ax.plot(idx, [m+std for m, std in  zip(history['mean'], history['std'])], c+':')#, label=label)\n",
    "    ax.plot(idx, [m-std for m, std in  zip(history['mean'], history['std'])], c+':')#, label=label)\n",
    "#     ax.plot(idx, history['max'], 'k'+'.', label=label)\n",
    "#     ax.plot(idx, history['min'], 'k'+'.', label=label)\n",
    "    plt.suptitle(title)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3315cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_data = datamodule.train_dataset\n",
    "history = collect_running_stats_from_dataset(train_data)\n",
    "plot_history(history, title='train images -- mean pixel intensities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a70f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "val_data = datamodule.val_dataset\n",
    "val_history = collect_running_stats_from_dataset(val_data)\n",
    "plot_history(val_history, title='val images -- mean pixel intensities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83eb8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "datamodule.setup('test')\n",
    "test_data = datamodule.test_dataset\n",
    "test_history = collect_running_stats_from_dataset(test_data)\n",
    "plot_history(test_history, title='test images -- mean pixel intensities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e2a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649cc2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-darkgrid')\n",
    "# plt.style.use('seaborn-talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c17097",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(15,10))\n",
    "\n",
    "plot_history(history, label='train images', ax=ax, c='r')#kwargs = {'c':'r'})\n",
    "plot_history(val_history, label='val images', ax=ax, c='b')#kwargs = {'c':'b'})\n",
    "plot_history(test_history, label='test images', ax=ax, c='g')#kwargs = {'c':'g'})\n",
    "plt.legend()\n",
    "plt.suptitle(f'Running mean of batch pixel intensities in PNAS', size='xx-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a641f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetadataTensor(object):\n",
    "    def __init__(self, data, metadata=None, **kwargs):\n",
    "        self._t = torch.as_tensor(data, **kwargs)\n",
    "        self._metadata = metadata\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Metadata:\\n{}\\n\\ndata:\\n{}\".format(self._metadata, self._t)\n",
    "\n",
    "    def __torch_function__(self, func, types, args=(), kwargs=None):\n",
    "        if kwargs is None:\n",
    "            kwargs = {}\n",
    "        args = [a._t if hasattr(a, '_t') else a for a in args]\n",
    "        ret = func(*args, **kwargs)\n",
    "        return MetadataTensor(ret, metadata=self._metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdb26bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(y: torch.Tensor):\n",
    "    try:\n",
    "        y = y.detach().numpy()\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    return {'shape':y.shape,\n",
    "            'mean':np.mean(y),\n",
    "            'std':np.std(y),\n",
    "            'min':np.min(y),\n",
    "            'max':np.max(y)}\n",
    "\n",
    "\n",
    "def fit_one_batch():\n",
    "    y_hat = model(x)\n",
    "\n",
    "    loss = model.loss(y_hat, y)\n",
    "    y_prob = model.probs(y_hat)\n",
    "\n",
    "    y_hat_int, y_pred = torch.max(y_prob, dim=1)\n",
    "    loss.register_hook(get_stats)\n",
    "    \n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "   \n",
    "    # norms = model.grad_norm(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f34d6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, p in model.named_parameters():\n",
    "    print(name);print(p.requires_grad)\n",
    "#     if isinstance(p, (nn.Conv2d, nn.BatchNorm2d, nn.ReLU, nn.MaxPool2d)):\n",
    "#         continue\n",
    "#     else:\n",
    "#         print(name);print(p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276dd0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "\n",
    "class VerboseExecution(nn.Module):\n",
    "    def __init__(self, model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "        # Register a hook for each layer\n",
    "        for name, layer in self.model.named_children():\n",
    "            layer.__name__ = name\n",
    "            layer.register_forward_hook(self.module_hook)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def module_hook(module: nn.Module, input: Tensor, output: Tensor):\n",
    "        print(f\"{module.__name__}: {output.shape}\")\n",
    "        \n",
    "        for p in module.parameters():\n",
    "            if p.requires_grad:\n",
    "                print(f\"requires_grad: {p.requires_grad}\")\n",
    "                p.register_hook(VerboseExecution.tensor_hook)\n",
    "\n",
    "        if not isinstance(input, (tuple, list)):\n",
    "            input = (input,)\n",
    "        if not isinstance(output, (tuple, list)):\n",
    "            output = (output,)\n",
    "            \n",
    "        for i in input:\n",
    "            get_stats(i)\n",
    "        for o in output:\n",
    "            get_stats(o)\n",
    "        \n",
    "    @staticmethod    \n",
    "    def tensor_hook(grad: Tensor):\n",
    "        # For Tensor objects only.\n",
    "        # Only executed during the *backward* pass!\n",
    "        print(\"Gradient stats:\")\n",
    "        get_stats(grad)\n",
    "        print('='*20)\n",
    "        \n",
    "\n",
    "\n",
    "# import torch\n",
    "# from torchvision.models import resnet50\n",
    "\n",
    "verbose_resnet = VerboseExecution(model) #resnet50())\n",
    "dummy_input = torch.ones(10, 3, 224, 224)\n",
    "result = verbose_resnet(dummy_input)\n",
    "\n",
    "print(result.shape)\n",
    "# conv1: torch.Size([10, 64, 112, 112])\n",
    "# bn1: torch.Size([10, 64, 112, 112])\n",
    "# relu: torch.Size([10, 64, 112, 112])\n",
    "# maxpool: torch.Size([10, 64, 56, 56])\n",
    "# layer1: torch.Size([10, 256, 56, 56])\n",
    "# layer2: torch.Size([10, 512, 28, 28])\n",
    "# layer3: torch.Size([10, 1024, 14, 14])\n",
    "# layer4: torch.Size([10, 2048, 7, 7])\n",
    "# avgpool: torch.Size([10, 2048, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c91d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d601a90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Dict, Iterable, Callable\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, model: nn.Module, layers: Iterable[str]):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.layers = layers\n",
    "        self._features = {layer: torch.empty(0) for layer in layers}\n",
    "\n",
    "        for layer_id in layers:\n",
    "            layer = dict([*self.model.named_modules()])[layer_id]\n",
    "            layer.register_forward_hook(self.save_outputs_hook(layer_id))\n",
    "\n",
    "    def save_outputs_hook(self, layer_id: str) -> Callable:\n",
    "        def fn(_, __, output):\n",
    "            self._features[layer_id] = output\n",
    "        return fn\n",
    "\n",
    "    def forward(self, x: Tensor) -> Dict[str, Tensor]:\n",
    "        _ = self.model(x)\n",
    "        return self._features\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "resnet_features = FeatureExtractor(resnet50(), layers=[\"layer4\", \"avgpool\"])\n",
    "features = resnet_features(dummy_input)\n",
    "\n",
    "print({name: output.shape for name, output in features.items()})\n",
    "# {'layer4': torch.Size([10, 2048, 7, 7]), 'avgpool': torch.Size([10, 2048, 1, 1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0878fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_clipper(model: nn.Module, val: float) -> nn.Module:\n",
    "    for parameter in model.parameters():\n",
    "        parameter.register_hook(lambda grad: grad.clamp_(-val, val))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d53a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clipped_resnet = gradient_clipper(resnet50(), 0.01)\n",
    "pred = clipped_resnet(dummy_input)\n",
    "loss = pred.log().mean()\n",
    "loss.backward()\n",
    "\n",
    "print(clipped_resnet.fc.bias.grad[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5258d4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2634fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, p in enumerate(model.parameters()):\n",
    "#     print(i, type(p))\n",
    "\n",
    "\n",
    "for l in model.forward_features:\n",
    "    print(type(l), l.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e3db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cac542",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3320ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, p in enumerate(model.children()):\n",
    "    print(i, type(p))\n",
    "\n",
    "for name, p in model.named_parameters():\n",
    "    print(name, type(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242fb070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "cbb24d51",
   "metadata": {},
   "source": [
    "trainer.fit(model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de29e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(trainer)\n",
    "\n",
    "dir(model)\n",
    "\n",
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39698672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6c8647c",
   "metadata": {},
   "source": [
    "## Inspect single backward pass gradients manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946ab54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = model.configure_optimizers()\n",
    "\n",
    "\n",
    "x, y = datamodule.get_batch(stage='train', batch_idx=0)\n",
    "y_hat = model(x)\n",
    "\n",
    "loss = model.loss(y_hat, y)\n",
    "y_prob = model.probs(y_hat)\n",
    "y_hat_int, y_pred = torch.max(y_prob, dim=1)\n",
    "loss.backward()\n",
    "opt.step()\n",
    "norms = model.grad_norm(2)\n",
    "print(norms)\n",
    "\n",
    "datamodule.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b306c95",
   "metadata": {},
   "source": [
    "### Model testing utils playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4213f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_model_params_requires_grad(model, prefix=''):\n",
    "    for name, p in model.named_parameters():\n",
    "        print(prefix + name, type(p), p.shape, f'requires_grad=={p.requires_grad}')\n",
    "\n",
    "from torch.nn.modules.container import Sequential\n",
    "def recursive_print_model_children(model, prefix='', lw=15):\n",
    "    for l in model.children():\n",
    "        print(prefix + '='*lw)\n",
    "        print(prefix + str(type(l)))\n",
    "        if isinstance(l, Sequential):\n",
    "            recursive_print_model_children(l, prefix='\\t'+prefix)\n",
    "        elif hasattr(l, 'named_parameters'):\n",
    "            display_model_params_requires_grad(l, prefix='\\t'+prefix)\n",
    "#             for param in l.parameters():\n",
    "#                 if hasattr(l, 'requires_grad'):\n",
    "#                     print(f'requires_grad={l.requires_grad}')\n",
    "\n",
    "# from torchsummary import summary\n",
    "# summary(model.cuda(), input_size=(3, 224, 224))\n",
    "                    \n",
    "# display_model_params_requires_grad(model)\n",
    "# recursive_print_model_children(model, prefix='\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "backbone_name = 'resnet18'\n",
    "model = timm.create_model(backbone_name, pretrained=True)\n",
    "\n",
    "display_model_params_requires_grad(model)\n",
    "\n",
    "from pl_bolts.utils import BatchGradientVerification\n",
    "\n",
    "def perform_batch_gradient_verification(model, input_size=(3, 224, 224), input_array=None):\n",
    "    \"\"\"\n",
    "    Checks if a model mixes data across the batch dimension.\n",
    "    \n",
    "    This can happen if reshape- and/or permutation operations are carried out in the wrong order or\n",
    "    on the wrong tensor dimensions.\n",
    "    \n",
    "    Examples:\n",
    "    ========\n",
    "    perform_batch_gradient_verification(model)\n",
    "\n",
    "    perform_batch_gradient_verification(model, input_size=(3, 224, 224))\n",
    "\n",
    "    perform_batch_gradient_verification(model, input_array=torch.rand(2,3,224,224))\n",
    "    \n",
    "    \"\"\"\n",
    "    if input_array is None:\n",
    "        input_array = model.example_input_array\n",
    "    if input_array is None:\n",
    "        input_array = torch.rand(2,*input_size)\n",
    "    verification = BatchGradientVerification(model)\n",
    "    valid = verification.check(input_array=input_array, sample_idx=1)\n",
    "    \n",
    "    if valid:\n",
    "        print('Test: [PASSED]',\n",
    "              '\\n==============\\n',\n",
    "              'Model passed batch gradient verification test!\\n',\n",
    "              'Confirmed no data mixing occurs across batch dimension')\n",
    "    \n",
    "    return valid\n",
    "\n",
    "\n",
    "valid = perform_batch_gradient_verification(model)\n",
    "\n",
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da120e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c47fcab",
   "metadata": {},
   "source": [
    "## Miscellaneous -- End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78601761",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(datamodule.train_dataloader()))\n",
    "y = y.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d2e4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26f11ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_hat.argmax(dim=1).detach().numpy()\n",
    "y_hat_max = torch.max(y_hat, dim=1).values.detach().numpy()\n",
    "\n",
    "# list(y_hat_max.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb8f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.mean\n",
    "datamodule.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806b0ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_stats(y: torch.Tensor):\n",
    "    try:\n",
    "        y = y.detach().numpy()\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    print('shape:', y.shape,\n",
    "          f'mean: {np.mean(y):.2f}',\n",
    "          f'std: {np.std(y):.2f}',\n",
    "          f'min: {np.min(y):.2f}',\n",
    "          f'max: {np.max(y):.2f}')\n",
    "    \n",
    "def unnormalize(img, mean=None, std=None):\n",
    "    if mean is None:\n",
    "        img = img / 2 + 0.5 # unnormalise\n",
    "    else:\n",
    "        img = img * std + mean\n",
    "    img = img.numpy()\n",
    "    img = np.transpose(img, (1,2,0))\n",
    "    return img\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "print('y_hat:')\n",
    "get_stats(y_hat)\n",
    "\n",
    "print('y_pred:')\n",
    "get_stats(y_pred)\n",
    "\n",
    "print('y_true:')\n",
    "get_stats(y)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,10))\n",
    "grid_img = torchvision.utils.make_grid(x, nrow=7)\n",
    "c, h, w = grid_img.shape\n",
    "grid_img = unnormalize(grid_img, torch.Tensor(datamodule.mean).view(c,1,1), torch.Tensor(datamodule.std).view(c,1,1))\n",
    "# plt.style('presentation')\n",
    "# pos = ax.imshow(grid_img.permute(1,2,0), cmap='gray')\n",
    "# fig.colorbar(pos, ax=ax)\n",
    "get_stats(grid_img)\n",
    "plt.imshow(grid_img, cmap='gray')\n",
    "plt.colorbar()\n",
    "\n",
    "# plt.style.use('seaborn-talk')\n",
    "x.shape\n",
    "\n",
    "plt.imshow(x[-1,...].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6d66e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.imshow?\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "plt.colorbar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45031bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project the digits into 2 dimensions using IsoMap\n",
    "from sklearn.manifold import Isomap\n",
    "iso = Isomap(n_components=2)\n",
    "# x = x.numpy()\n",
    "projection = iso.fit_transform([x[i,0,:,:].ravel() for i in range(x.shape[0])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a8cdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats(projection[:,0])\n",
    "get_stats(projection[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0668f324",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats(projection[:,0])\n",
    "get_stats(projection[:,1])\n",
    "\n",
    "stats = np.min(projection[:,0]), np.max(projection[:,0])\n",
    "projection[:,0] = (projection[:,0]-stats[0])/(stats[1]-stats[0])\n",
    "\n",
    "stats = np.min(projection[:,1]), np.max(projection[:,1])\n",
    "projection[:,1] = (projection[:,1]-stats[0])/(stats[1]-stats[0])\n",
    "\n",
    "# plot the results\n",
    "plt.scatter(projection[:, 0], projection[:, 1],# lw=0.1,\n",
    "            c=y, cmap=plt.cm.get_cmap('cubehelix', 19))\n",
    "plt.colorbar(ticks=range(19), label='leaf family')\n",
    "# plt.clim(0.0, 1.0)\n",
    "# plt.clim(-0.5, 5.5)\n",
    "\n",
    "# plot the results\n",
    "plt.scatter(projection[:, 0], projection[:, 1],# lw=0.1,\n",
    "            c=y_pred, cmap=plt.cm.get_cmap('cubehelix', 19))\n",
    "plt.colorbar(ticks=range(19), label='leaf family')\n",
    "# plt.clim(0.0, 1.0)\n",
    "# plt.clim(-0.5, 5.5)\n",
    "\n",
    "projection\n",
    "\n",
    "for i in range(48):\n",
    "    get_stats(x[i,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4325c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print([('y_hat','y_pred','y_true'), *list(zip(list(y_hat_max), list(y_pred), list(y)))])\n",
    "\n",
    "\n",
    "\n",
    "list(a.values.detach().numpy())\n",
    "\n",
    "print(y_hat.shape)\n",
    "\n",
    "%debug\n",
    "\n",
    "child_counter = 0\n",
    "for child in model.children():\n",
    "    print(\" child\", child_counter, \"is -\")\n",
    "    print(child)\n",
    "    child_counter += 1\n",
    "\n",
    "Log histogram of losses\n",
    "\n",
    "fig = plt.figure()\n",
    "losses = np.stack([x['val_loss'].numpy() for x in outputs])\n",
    "plt.hist(losses)\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "torchmetrics.__version__\n",
    "\n",
    "torchmetrics.__version__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390b3dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.log(1/19)\n",
    "\n",
    "\n",
    "\n",
    "datamodule.setup('fit')\n",
    "train_dataloader = datamodule.train_dataloader()    \n",
    "    \n",
    "classes = datamodule.classes\n",
    "num_classes = len(classes)\n",
    "# Get training images\n",
    "dataiter = iter(train_dataloader)\n",
    "images, labels = next(dataiter)#.next()\n",
    "\n",
    "\n",
    "# def imshow(img):\n",
    "#     img = img / 2 + 0.5 # unnormalise\n",
    "#     npimg = img.numpy()\n",
    "#     plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# imshow(torchvision.utils.make_grid(images))\n",
    "# print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "\n",
    "train_metrics = get_metrics(num_classes=num_classes, prefix='train')\n",
    "\n",
    "train_metrics\n",
    "\n",
    "def eval_one_batch(model, batch):\n",
    "\n",
    "    x, y = batch\n",
    "    y_hat = model(x)\n",
    "    loss = model.loss(y_hat, y)\n",
    "    y_prob = model.probs(y_hat)\n",
    "    y_pred = y_prob.argmax(dim=1)\n",
    "\n",
    "\n",
    "    results = {'loss':loss,\n",
    "               'y_hat':y_hat,\n",
    "               'y_prob':y_prob,\n",
    "               'y_pred':y_pred,\n",
    "               'y_true':y}\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "results = eval_one_batch(model, batch=(images, labels))\n",
    "\n",
    "metric_result = train_metrics(results['y_prob'], results['y_true'])\n",
    "\n",
    "def TP_topk(y_prob, y_true, k: int=1):\n",
    "    _, topk = torch.topk(y_prob, k=k) #.numpy()\n",
    "#     return [y_true[i].detach().numpy() in topk[i] for i in range(len(y_true))]\n",
    "    return [y_true[i] in topk[i] for i in range(len(y_true))]\n",
    "    \n",
    "\n",
    "y_prob, y_true = results['y_prob'], results['y_true']\n",
    "\n",
    "\n",
    "tp_top1 = TP_topk(y_prob, y_true, k=1)\n",
    "tp_top3 = TP_topk(y_prob, y_true, k=3)\n",
    "tp_top5 = TP_topk(y_prob, y_true, k=5)\n",
    "\n",
    "print(list(zip(tp_top1, tp_top3, tp_top5)))\n",
    "\n",
    "# %debug\n",
    "\n",
    "# %debug\n",
    "# torch.topk(results['y_prob'], k=5)[1]\n",
    "\n",
    "# for k, v in results.items():\n",
    "#     if len(v.size())==0:\n",
    "#         print(k, v.size(), v.detach().numpy())\n",
    "#     elif len(v.size())==1:\n",
    "#         print(k, v.size(), v.detach().numpy())\n",
    "#     else:\n",
    "#         print(k, v.size(), f'Min: {v.min().detach():.2f}, Max: {v.max().detach():.2f}')\n",
    "#         print(\"per-sample argmax:\", v.argmax(dim=1).detach().numpy())\n",
    "        \n",
    "# print(list(zip(y_pred.numpy(), y.numpy())))\n",
    "\n",
    "# plt.imshow(x.cpu()[0,...].permute(1,2,0))\n",
    "\n",
    "backbone_name = 'resnet50' #'xception41'\n",
    "# backbone_name = 'resnet101'\n",
    "# backbone_name = 'resnet152'\n",
    "# model = timm.create_model(backbone_name, pretrained=True)\n",
    "model.reset_classifier(19,'avg')\n",
    "# model = Classifier(backbone,\n",
    "#            head_source=None,\n",
    "#            head_target=None,\n",
    "#            num_classes=19,\n",
    "#            finetune=True)\n",
    "\n",
    "# helper function to show an image\n",
    "# (used in the `plot_classes_preds` function below)\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "# show images\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "\n",
    "\n",
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "\n",
    "modules = list(model.modules())\n",
    "len(modules)    \n",
    "#         def _initialize_weights(self) -> None:\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, torch.nn.Conv2d):\n",
    "#                 torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "#                 if m.bias is not None:\n",
    "#                     torch.nn.init.constant_(m.bias, 0)\n",
    "#             elif isinstance(m, torch.nn.BatchNorm2d):\n",
    "#                 torch.nn.init.constant_(m.weight, 1)\n",
    "#                 torch.nn.init.constant_(m.bias, 0)\n",
    "#             elif isinstance(m, torch.nn.Linear):\n",
    "#                 torch.nn.init.normal_(m.weight, 0, 0.01)\n",
    "#                 torch.nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7a31d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "# print(inspect.getsource(model.reset_classifier))\n",
    "# inspect.__dir__()\n",
    "model.__dir__()\n",
    "\n",
    "import inspect\n",
    "# print(inspect.getsource(model.reset_classifier))\n",
    "# inspect.__dir__()\n",
    "model.num_features\n",
    "\n",
    "state = model.bn1.state_dict()\n",
    "\n",
    "state['bias'].size()\n",
    "\n",
    "model.bn1.state_dict()\n",
    "\n",
    "model.__dict__\n",
    "\n",
    "import inspect\n",
    "print(inspect.getsource(model.num_features))\n",
    "\n",
    "## ResNet50\n",
    "for i, child in enumerate(model.children()):\n",
    "    if isinstance(child, torch.nn.modules.container.Sequential):\n",
    "#         break\n",
    "        print(f'Found conv block at module child index: {i}')\n",
    "        print('='*20)\n",
    "    else:\n",
    "        print(i, type(child))\n",
    "\n",
    "for i, (name,child) in enumerate(model.named_children()):\n",
    "    if isinstance(child, torch.nn.modules.container.Sequential):\n",
    "#         break\n",
    "        print(f'Found conv block at module child index: {i}')\n",
    "        print(f'name is: {name}')\n",
    "        print('='*20)\n",
    "    else:\n",
    "        print(i, type(child))\n",
    "        print(f'name is: {name}')\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "named_layers = OrderedDict(model.named_children())\n",
    "layers = list(named_layers.values())\n",
    "layer_names = list(named_layers.keys())\n",
    "layer_names\n",
    "\n",
    "stem = torch.nn.modules.container.Sequential(\n",
    "    OrderedDict({l:named_layers[l] for l in ['conv1','bn1','act1', 'maxpool']}\n",
    "))\n",
    "stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f35c192",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_group_names = layer_names[4:8]\n",
    "print(residual_group_names)\n",
    "\n",
    "learner = torch.nn.modules.container.Sequential(\n",
    "    OrderedDict({f\"res_group{i}\":named_layers[l] for i,l in enumerate(residual_group_names)}\n",
    "))\n",
    "learner\n",
    "\n",
    "classifier_layer_names = layer_names[8:]\n",
    "print(classifier_layer_names)\n",
    "\n",
    "classifier = torch.nn.modules.container.Sequential(\n",
    "    OrderedDict({l:named_layers[l] for l in classifier_layer_names}\n",
    "))\n",
    "classifier\n",
    "\n",
    "from torch.nn.modules.container import Sequential\n",
    "\n",
    "macro_model = Sequential(\n",
    "    OrderedDict({'stem':stem,\n",
    "     'learner':learner,\n",
    "     'classifier':classifier})\n",
    ")\n",
    "\n",
    "for n, mod in macro_model.named_children():\n",
    "    print(n, len(mod))\n",
    "\n",
    "torch.nn.modules.container.Sequential*(layers[:4])\n",
    "\n",
    "from rich import print\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68848315",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inspect.getsource(backbone.reset_classifier))\n",
    "\n",
    "# list(backbone.children())\n",
    "# backbone.__dir__()\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, logger=pl.loggers.wandb.WandbLogger(name=\"default-timm-resnet50\"))\n",
    "trainer.fit(model, datamodule)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_name = 'resnet50' #'xception41'\n",
    "model = timm.create_model(model_name, pretrained=True)\n",
    "\n",
    "model.forward_features\n",
    "\n",
    "model.__dir__()\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.chdir(\"/media/data/jacob/GitHub/lightning-hydra-classifiers/src/models\") #/pnas_model.py\")\n",
    "\n",
    "\n",
    "from .pnas_model import Classifier\n",
    "# from contrastive_learning.data.pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dace0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimmLightningModel(LightningModule):\n",
    "    \"\"\"\n",
    "    Example of LightningModule for MNIST classification.\n",
    "\n",
    "    A LightningModule organizes your PyTorch code into 5 sections:\n",
    "        - Computations (init).\n",
    "        - Train loop (training_step)\n",
    "        - Validation loop (validation_step)\n",
    "        - Test loop (test_step)\n",
    "        - Optimizers (configure_optimizers)\n",
    "\n",
    "    Read the docs:\n",
    "        https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: Union[str,nn.Module] = \"resnet50\"\n",
    "        input_size: int = 784,\n",
    "        lin1_size: int = 256,\n",
    "        lin2_size: int = 256,\n",
    "        lin3_size: int = 256,\n",
    "        output_size: int = 10,\n",
    "        lr: float = 0.001,\n",
    "        weight_decay: float = 0.0005,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # this line ensures params passed to LightningModule will be saved to ckpt\n",
    "        # it also allows to access params with 'self.hparams' attribute\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        model_name = 'resnet50' #'xception41'\n",
    "        self.backbone = timm.create_model(backbone, pretrained=True)\n",
    "\n",
    "        self.model = SimpleDenseNet(hparams=self.hparams)\n",
    "\n",
    "        # loss function\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        # use separate metric instance for train, val and test step\n",
    "        # to ensure a proper reduction over the epoch\n",
    "        self.train_accuracy = Accuracy()\n",
    "        self.val_accuracy = Accuracy()\n",
    "        self.test_accuracy = Accuracy()\n",
    "\n",
    "        self.metric_hist = {\n",
    "            \"train/acc\": [],\n",
    "            \"val/acc\": [],\n",
    "            \"train/loss\": [],\n",
    "            \"val/loss\": [],\n",
    "        }\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.model(x)\n",
    "\n",
    "    def step(self, batch: Any):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        return loss, preds, y\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int):\n",
    "        loss, preds, targets = self.step(batch)\n",
    "\n",
    "        # log train metrics\n",
    "        acc = self.train_accuracy(preds, targets)\n",
    "        self.log(\"train/loss\", loss, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"train/acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        # we can return here dict with any tensors\n",
    "        # and then read it in some callback or in training_epoch_end() below\n",
    "        # remember to always return loss from training_step, or else backpropagation will fail!\n",
    "        return {\"loss\": loss, \"preds\": preds, \"targets\": targets}\n",
    "\n",
    "    def training_epoch_end(self, outputs: List[Any]):\n",
    "        # log best so far train acc and train loss\n",
    "        self.metric_hist[\"train/acc\"].append(self.trainer.callback_metrics[\"train/acc\"])\n",
    "        self.metric_hist[\"train/loss\"].append(self.trainer.callback_metrics[\"train/loss\"])\n",
    "        self.log(\"train/acc_best\", max(self.metric_hist[\"train/acc\"]), prog_bar=False)\n",
    "        self.log(\"train/loss_best\", min(self.metric_hist[\"train/loss\"]), prog_bar=False)\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int):\n",
    "        loss, preds, targets = self.step(batch)\n",
    "\n",
    "        # log val metrics\n",
    "        acc = self.val_accuracy(preds, targets)\n",
    "        self.log(\"val/loss\", loss, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"val/acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return {\"loss\": loss, \"preds\": preds, \"targets\": targets}\n",
    "\n",
    "    def validation_epoch_end(self, outputs: List[Any]):\n",
    "        # log best so far val acc and val loss\n",
    "        self.metric_hist[\"val/acc\"].append(self.trainer.callback_metrics[\"val/acc\"])\n",
    "        self.metric_hist[\"val/loss\"].append(self.trainer.callback_metrics[\"val/loss\"])\n",
    "        self.log(\"val/acc_best\", max(self.metric_hist[\"val/acc\"]), prog_bar=False)\n",
    "        self.log(\"val/loss_best\", min(self.metric_hist[\"val/loss\"]), prog_bar=False)\n",
    "\n",
    "    def test_step(self, batch: Any, batch_idx: int):\n",
    "        loss, preds, targets = self.step(batch)\n",
    "\n",
    "        # log test metrics\n",
    "        acc = self.test_accuracy(preds, targets)\n",
    "        self.log(\"test/loss\", loss, on_step=False, on_epoch=True)\n",
    "        self.log(\"test/acc\", acc, on_step=False, on_epoch=True)\n",
    "\n",
    "        return {\"loss\": loss, \"preds\": preds, \"targets\": targets}\n",
    "\n",
    "    def test_epoch_end(self, outputs: List[Any]):\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Choose what optimizers and learning-rate schedulers to use in your optimization.\n",
    "        Normally you'd need one. But in the case of GANs or similar you might have multiple.\n",
    "\n",
    "        See examples here:\n",
    "            https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#configure-optimizers\n",
    "        \"\"\"\n",
    "        return torch.optim.Adam(\n",
    "            params=self.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c391ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained=True\n",
    "model = models.resnet50(pretrained=pretrained)\n",
    "num_classes = 19\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "print(model.fc)\n",
    "\n",
    "\n",
    "model.conv1\n",
    "\n",
    "model.layer1\n",
    "\n",
    "for name, child in model.named_children():\n",
    "    print(name)\n",
    "# if \"layer1\" in ct:\n",
    "#     for params in child.parameters():\n",
    "#         params.requires_grad = True\n",
    "# ct.append(name)\n",
    "\n",
    "from rich import print\n",
    "\n",
    "\n",
    "print(dir(datamodule))\n",
    "\n",
    "\n",
    "\n",
    "# name=\n",
    "#                  batch_size: int=32,\n",
    "#                  val_split: float=0.2,\n",
    "#                  num_workers=0,\n",
    "#                  seed: int=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec263977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from contrastive_learning.data.pytorch.pnas import PNASLightningDataModule\n",
    "import inspect\n",
    "from pprint import pprint as pp\n",
    "\n",
    "\n",
    "data = PNASLightningDataModule()\n",
    "\n",
    "data.setup(stage='fit')\n",
    "train_dataset = ImageFolder(root=data.train_dir)\n",
    "\n",
    "dir(val_data)\n",
    "\n",
    "len(val_data)\n",
    "\n",
    "inspect.getsource(val_data.__len__)\n",
    "# getmembers(val_data.__len__)\n",
    "\n",
    "# data.train_dataset\n",
    "\n",
    "pp(inspect.getmembers(data, inspect.isfunction))\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch import Generator\n",
    "import numpy as np\n",
    "# from .common import SubsetImageDataset, seed_worker\n",
    "\n",
    "\n",
    "self = data\n",
    "\n",
    "\n",
    "self.train_dataset = ImageFolder(root=self.train_dir)#, transform=train_transform, target_transform=target_transform)\n",
    "self.classes = self.train_dataset.classes\n",
    "\n",
    "val_split = self.val_split\n",
    "num_train = len(self.train_dataset)\n",
    "split_idx = (int(np.floor((1-val_split) * num_train)), int(np.floor(val_split * num_train)))\n",
    "if self.seed is None:\n",
    "    generator = None\n",
    "else:\n",
    "    generator = Generator().manual_seed(self.seed)\n",
    "\n",
    "train_data, val_data = random_split(self.train_dataset, \n",
    "                                    [split_idx[0], split_idx[1]], \n",
    "                                    generator=generator)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e296d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch import Generator\n",
    "import numpy as np\n",
    "from typing import List, Callable, Tuple\n",
    "from torchvision.datasets import VisionDataset\n",
    "\n",
    "# class TrainValSplitDataset(VisionDataset):\n",
    "class TrainValSplitDataset(ImageFolder):\n",
    "    \n",
    "    all_params: List[str]= [\n",
    "                            'class_to_idx',\n",
    "                            'classes',\n",
    "                            'extensions',\n",
    "                            'imgs',\n",
    "                            'loader',\n",
    "                            'root',\n",
    "                            'samples',\n",
    "                            'target_transform',\n",
    "                            'targets',\n",
    "                            'transform',\n",
    "                            'transforms'\n",
    "                            ]\n",
    "    sample_params: List[str] = ['imgs',\n",
    "                                'targets',\n",
    "                                'samples']\n",
    "    \n",
    "    @classmethod\n",
    "    def train_val_split(cls, full_dataset, val_split: float=0.2, seed: float=None) -> Tuple[ImageFolder]:\n",
    "        \n",
    "        num_samples = len(full_dataset)\n",
    "        split_idx = (int(np.floor((1-val_split) * num_samples)),\n",
    "                     int(np.floor(val_split * num_samples)))\n",
    "        if seed is None:\n",
    "            generator = None\n",
    "        else:\n",
    "            generator = Generator().manual_seed(seed)\n",
    "\n",
    "        train_data, val_data = random_split(full_dataset, \n",
    "                                            split_idx,\n",
    "                                            generator=generator)\n",
    "        \n",
    "        train_dataset = cls.select_from_dataset(full_dataset, indices=train_data.indices)\n",
    "        val_dataset = cls.select_from_dataset(full_dataset, indices=val_data.indices)\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "        \n",
    "    @classmethod\n",
    "    def from_dataset(cls, dataset):\n",
    "        new_dataset = cls(root=dataset.root)\n",
    "        \n",
    "        for key in cls.all_params:\n",
    "            if hasattr(dataset, key):\n",
    "                setattr(new_dataset, key, getattr(dataset, key))\n",
    "                         \n",
    "        return new_dataset\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def select_from_dataset(cls, dataset, indices=None):\n",
    "        upgraded_dataset = cls.from_dataset(dataset)\n",
    "        return upgraded_dataset.select(indices)\n",
    "    \n",
    "    \n",
    "    def select(self, indices):\n",
    "        new_subset = self.from_dataset(self) #, indices)\n",
    "        \n",
    "        for key in self.sample_params:\n",
    "            old_attr = getattr(self, key)\n",
    "            new_attr = [old_attr[idx] for idx in indices]\n",
    "            setattr(new_subset, key, new_attr)\n",
    "        return new_subset\n",
    "\n",
    "                \n",
    "    def __repr__(self) -> str:\n",
    "        head = \"Dataset \" + self.__class__.__name__\n",
    "        body = [\"Number of datapoints: {}\".format(self.__len__())]\n",
    "        if self.root is not None:\n",
    "            body.append(\"Root location: {}\".format(self.root))\n",
    "        body += self.extra_repr().splitlines()\n",
    "        if hasattr(self, \"transforms\") and self.transforms is not None:\n",
    "            body += [repr(self.transforms)]\n",
    "        lines = [head] + [\" \" * self._repr_indent + line for line in body]\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "    def _format_transform_repr(self, transform: Callable, head: str) -> List[str]:\n",
    "        lines = transform.__repr__().splitlines()\n",
    "        return ([\"{}{}\".format(head, lines[0])] +\n",
    "                [\"{}{}\".format(\" \" * len(head), line) for line in lines[1:]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce627a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from contrastive_learning.data.pytorch.pnas import PNASLightningDataModule\n",
    "import inspect\n",
    "from pprint import pprint as pp\n",
    "\n",
    "\n",
    "data = PNASLightningDataModule()\n",
    "\n",
    "data.setup(stage='fit')\n",
    "train_dataset = ImageFolder(root=data.train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa1ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = TrainValSplitDataset.train_val_split(train_dataset, val_split=0.2, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617bcb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data), len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3fafd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainValSplitDataset.select_from_dataset(train_data.dataset, train_data.indices)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922b823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6f9688",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = TrainValSplitDataset.select_from_dataset(val_data.dataset, val_data.indices)\n",
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f386e99d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d278105",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainValSplitDataset.from_dataset(train_data.dataset, train_data.indices)\n",
    "train_dataset = train_dataset.select(train_data.indices)\n",
    "train_dataset\n",
    "\n",
    "\n",
    "\n",
    "# len(train_dataset)\n",
    "len(val_dataset.samples)\n",
    "\n",
    "val_dataset = TrainValSplitDataset.from_dataset(val_data.dataset, val_data.indices)\n",
    "val_dataset\n",
    "\n",
    "val_data.dataset\n",
    "\n",
    "type(train_dataset)\n",
    "\n",
    "getattr?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d0e49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasattr?\n",
    "\n",
    "from torchvision import get_image_backend\n",
    "get_image_backend()\n",
    "\n",
    "inspect.getsource(train_data.dataset.loader)\n",
    "\n",
    "(train_data.dataset.imgs[0])\n",
    "\n",
    "dir(train_data.dataset)\n",
    "#, val_data\n",
    "\n",
    "self.train_dataset = SubsetImageDataset(train_data.dataset, train_data.indices)\n",
    "self.val_dataset = SubsetImageDataset(val_data.dataset, val_data.indices)\n",
    "\n",
    "class IndexDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset, indices):\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc6440",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RAMDataset(Dataset):\n",
    "    def __init__(image_fnames, targets):\n",
    "        self.targets = targets\n",
    "        self.images = []\n",
    "        for fname in tqdm(image_fnames, desc=\"Loading files in RAM\"):\n",
    "            with open(fname, \"rb\") as f:\n",
    "                self.images.append(f.read())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        target = self.targets[index]\n",
    "        image, retval = cv2.imdecode(self.images[index], cv2.IMREAD_COLOR)\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0c0501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Given a dataset, creates a dataset which applies a mapping function\n",
    "    to its items (lazily, only when an item is called).\n",
    "\n",
    "    Note that data is not cloned/copied from the initial dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: torch.utils.data.Dataset, transform =None, target_transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.dataset[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        if self.target_transform:\n",
    "            y = self.target_transform(y)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8ae014",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(data.train_dataset)\n",
    "\n",
    "type(data)\n",
    "\n",
    "# data.val_dataset.class_to_idx\n",
    "len(data.val_dataset.indices)\n",
    "\n",
    "data.val_dataset.transforms\n",
    "\n",
    "len(data.val_dataset.targets)\n",
    "\n",
    "len(data.val_dataset.samples)\n",
    "\n",
    "dir(data.val_dataset)\n",
    "\n",
    "data.train_dataset\n",
    "\n",
    "pretrained=True\n",
    "model = models.resnet50(pretrained=pretrained)\n",
    "\n",
    "# dir(model.fc)\n",
    "\n",
    "in_features = model.fc.in_features\n",
    "out_features = model.fc.out_features\n",
    "print(in_features, out_features)\n",
    "\n",
    "model.__dir__()\n",
    "\n",
    "pretrained=True\n",
    "model = models.resnet50(pretrained=pretrained)\n",
    "\n",
    "num_classes = 10\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "print(model.fc)\n",
    "\n",
    "dir(model.avgpool)\n",
    "\n",
    "model.avgpool.T_destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda90215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_lightning.metrics.classification import StatScores\n",
    "preds  = torch.tensor([1, 0, 2, 1])\n",
    "target = torch.tensor([1, 1, 2, 0])\n",
    "stat_scores = StatScores(reduce='macro', num_classes=3)\n",
    "stat_scores(preds, target)\n",
    "# tensor([[0, 1, 2, 1, 1],\n",
    "#         [1, 1, 1, 1, 2],\n",
    "#         [1, 0, 3, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f10273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_scores = StatScores(reduce='micro')\n",
    "stat_scores(preds, target)\n",
    "# tensor([2, 2, 6, 2, 4])\n",
    "\n",
    "stat_scores = StatScores(reduce='samples')\n",
    "stat_scores(preds, target)\n",
    "# tensor([2, 2, 6, 2, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde09621",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 5\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "print(torch.randint(0,9, size=(4,)))\n",
    "print(torch.randint(0,9, size=(4,)))\n",
    "\n",
    "seed = 6\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "print(torch.randint(0,9, size=(4,)))\n",
    "print(torch.randint(0,9, size=(4,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d3266b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c3833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.metrics.classification import StatScores\n",
    "from pprint import pprint as pp\n",
    "\n",
    "\n",
    "def generate_preds_target_pairs(num_classes=19, num_samples=100, min_percent_true=0.5):\n",
    "    \"\"\"\n",
    "    Generate 2 random torch tensors of shape (num_samples,) each,\n",
    "    where a fraction at least as large as min_percent_true are identical between the two.\n",
    "    \"\"\"\n",
    "    target = torch.randint(0, num_classes, size=(num_samples,))\n",
    "    preds = torch.clone(target.detach())\n",
    "\n",
    "    lock_true_pairs = int(num_samples*min_percent_true)\n",
    "    preds[lock_true_pairs:] = torch.randint(0, num_classes, size=(lock_true_pairs,))\n",
    "    \n",
    "    assert preds.size() == target.size()\n",
    "    assert preds.size() == (num_samples,)\n",
    "    assert (preds==target).sum() >= lock_true_pairs\n",
    "    \n",
    "    return preds, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c42fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=19\n",
    "num_samples=100\n",
    "min_percent_true=0.5\n",
    "\n",
    "preds, target = generate_preds_target_pairs(num_classes=num_classes, num_samples=num_samples, min_percent_true=min_percent_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5c7bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_stats = StatScores(reduce='macro', num_classes=num_classes)\n",
    "micro_stats = StatScores(reduce='micro', num_classes=num_classes)\n",
    "sample_stats = StatScores(reduce='samples', num_classes=num_classes)\n",
    "\n",
    "# print(macro_stats.compute(), micro_stats.compute(), sample_stats.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b263914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "import pdbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e96272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "# dir(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69572364",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique([0,4, 3,2,2,2,3,5])#, return_counts=True)\n",
    "\n",
    "macro_stats.update(preds, target)\n",
    "micro_stats.update(preds, target)\n",
    "sample_stats.update(preds, target)\n",
    "\n",
    "print(\"macro: \", macro_stats.compute())\n",
    "print(\"micro: \", micro_stats.compute())\n",
    "print(\"Samples: \", sample_stats.compute())\n",
    "# print(macro_stats, micro_stats, sample_stats)\n",
    "\n",
    "print(f'preds.size = {preds.size()}', f'targets.size = {target.size()}')\n",
    "print(f'# correct = {(preds==target).sum()}')\n",
    "# target = torch.randint(0,9, size=(100,))\n",
    "\n",
    "pp(list(zip(preds.tolist(), target.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94581e33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
